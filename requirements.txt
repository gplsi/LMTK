transformers
datasets
evaluate
accelerate
tokenizers
pyyaml
jsonschema
python-box
types-pyyaml
scikit-learn
lightning
pytorch-lightning
wandb==0.17
<<<<<<< HEAD
pydantic==1.10.11
# PyTorch - CI will override this with CPU-specific installation
torch>=2.0.0
# Optional - flash attention (GPU only)
# flash-attn
# Testing dependencies
pytest
pytest-cov
pytest-mock
pytest-xdist
pytest-timeout
hypothesis
=======
flash-attn
pydantic==1.10.11
deepspeed
>>>>>>> 0a487b3 (Add DeepSpeed support and enhance configuration for pretraining experiments. Currently tested in local until got out of memory for zero stage 1,2,3.)
