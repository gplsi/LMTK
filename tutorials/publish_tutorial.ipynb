{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Model Publishing Tutorial\n",
    "\n",
    "This tutorial demonstrates how to publish a trained model to the Hugging Face Hub using the Continual Pretraining Framework. We'll cover the following topics:\n",
    "\n",
    "1. Understanding the model publishing workflow\n",
    "2. Setting up the publishing configuration\n",
    "3. Converting FSDP checkpoints to HuggingFace format\n",
    "4. Uploading models to the Hugging Face Hub\n",
    "5. Validating the published model\n",
    "6. Best practices for model publishing\n",
    "\n",
    "This tutorial assumes you have already completed the CLM training tutorial and have a trained model checkpoint available."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 1. Understanding the Model Publishing Workflow\n",
    "\n",
    "The publish module in the Continual Pretraining Framework provides a streamlined way to convert your trained model checkpoints (especially those trained with FSDP - Fully Sharded Data Parallel) into a format that can be easily shared with the community via the Hugging Face Hub.\n",
    "\n",
    "The publishing workflow consists of two main steps:\n",
    "\n",
    "1. **Format Conversion**: Converting the model checkpoint from the training format (e.g., FSDP) to a standard HuggingFace format.\n",
    "2. **Model Upload**: Uploading the converted model and its tokenizer to the Hugging Face Hub.\n",
    "\n",
    "The `PublishOrchestrator` class orchestrates this entire workflow, making it easy to publish your models with minimal effort."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 2. Setting Up the Environment\n",
    "\n",
    "First, let's import the necessary modules and set up our environment:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "import torch\n",
    "import yaml\n",
    "from box import Box\n",
    "from pathlib import Path\n",
    "from transformers import AutoModelForCausalLM, AutoTokenizer\n",
    "from huggingface_hub import login\n",
    "\n",
    "# Import the publish components\n",
    "from src.tasks.publish import execute\n",
    "from src.tasks.publish.orchestrator import PublishOrchestrator\n",
    "from src.tasks.publish.format.fsdp import ConvertFSDPCheckpoint\n",
    "from src.tasks.publish.upload.huggingface import UploadHuggingface\n",
    "from src.config.config_loader import ConfigValidator\n",
    "\n",
    "# Check if CUDA is available\n",
    "print(f\"CUDA available: {torch.cuda.is_available()}\")\n",
    "if torch.cuda.is_available():\n",
    "    print(f\"CUDA device count: {torch.cuda.device_count()}\")\n",
    "    print(f\"CUDA device name: {torch.cuda.get_device_name(0)}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 3. Creating a Sample Checkpoint\n",
    "\n",
    "For this tutorial, we'll create a small sample checkpoint to demonstrate the publishing process. In a real-world scenario, you would use a checkpoint from your trained model."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Create a sample checkpoint directory\n",
    "sample_checkpoint_dir = Path(\"sample_checkpoint\")\n",
    "sample_checkpoint_dir.mkdir(exist_ok=True)\n",
    "\n",
    "# Function to create a sample FSDP checkpoint\n",
    "def create_sample_checkpoint():\n",
    "    # Load a small model for demonstration\n",
    "    model_name = \"gpt2\"\n",
    "    model = AutoModelForCausalLM.from_pretrained(model_name)\n",
    "    \n",
    "    # Create a state dict that mimics an FSDP checkpoint structure\n",
    "    fsdp_state_dict = {}\n",
    "    \n",
    "    # Convert regular state dict to FSDP-like format\n",
    "    for key, value in model.state_dict().items():\n",
    "        # Mimic FSDP key structure\n",
    "        if key.startswith(\"transformer.\"):\n",
    "            # Transform transformer.h.0.attn.c_attn.weight to model.model.layers.0.attn.c_attn.weight\n",
    "            new_key = key.replace(\"transformer.\", \"model.model.\")\n",
    "            new_key = new_key.replace(\".h.\", \".layers.\")\n",
    "            fsdp_state_dict[new_key] = value\n",
    "        elif key == \"lm_head.weight\":\n",
    "            # Keep lm_head as is\n",
    "            fsdp_state_dict[\"model.lm_head.weight\"] = value\n",
    "        else:\n",
    "            # Other parameters\n",
    "            fsdp_state_dict[f\"model.{key}\"] = value\n",
    "    \n",
    "    # Create a checkpoint dictionary with model_state_dict\n",
    "    checkpoint = {\n",
    "        \"model_state_dict\": fsdp_state_dict,\n",
    "        \"step_count\": 1000,\n",
    "        \"current_epoch\": 1\n",
    "    }\n",
    "    \n",
    "    # Save the checkpoint\n",
    "    checkpoint_path = sample_checkpoint_dir / \"checkpoint.pt\"\n",
    "    torch.save(checkpoint, checkpoint_path)\n",
    "    \n",
    "    return str(checkpoint_path)\n",
    "\n",
    "# Create the sample checkpoint\n",
    "checkpoint_path = create_sample_checkpoint()\n",
    "print(f\"Sample checkpoint created at: {checkpoint_path}\")\n",
    "\n",
    "# Load and verify the checkpoint\n",
    "checkpoint = torch.load(checkpoint_path, map_location=\"cpu\")\n",
    "print(f\"Checkpoint keys: {list(checkpoint.keys())}\")\n",
    "print(f\"Number of parameters in checkpoint: {len(checkpoint['model_state_dict'])}\")\n",
    "print(f\"Sample parameter key: {list(checkpoint['model_state_dict'].keys())[0]}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 4. Setting Up the Publishing Configuration\n",
    "\n",
    "To publish a model, we need to create a configuration that specifies the model, checkpoint path, and publishing details. The Continual Pretraining Framework uses YAML configuration files, but we can also create a configuration programmatically:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Create a configuration for model publishing\n",
    "def create_publish_config(checkpoint_path, base_model=\"gpt2\", repo_id=\"your-username/your-model-name\"):\n",
    "    \"\"\"\n",
    "    Create a configuration for model publishing.\n",
    "    \n",
    "    Args:\n",
    "        checkpoint_path: Path to the model checkpoint\n",
    "        base_model: Name or path of the base model used for training\n",
    "        repo_id: HuggingFace repository ID where the model will be published\n",
    "        \n",
    "    Returns:\n",
    "        Box: Configuration object\n",
    "    \"\"\"\n",
    "    config = {\n",
    "        \"task\": \"publish\",\n",
    "        \"experiment_name\": \"tutorial_publish\",\n",
    "        \"verbose_level\": 4,\n",
    "        \n",
    "        # Publish configuration\n",
    "        \"publish\": {\n",
    "            # Format conversion\n",
    "            \"format\": \"fsdp\",\n",
    "            \"base_model\": base_model,\n",
    "            \"checkpoint_path\": checkpoint_path,\n",
    "            \n",
    "            # Upload configuration\n",
    "            \"host\": \"huggingface\",\n",
    "            \"repo_id\": repo_id,\n",
    "            \"commit_message\": f\"Add {base_model} model trained with Continual Pretraining Framework\",\n",
    "            \n",
    "            # Advanced options\n",
    "            \"max_shard_size\": \"5GB\",\n",
    "            \"safe_serialization\": True,\n",
    "            \"create_pr\": False\n",
    "        }\n",
    "    }\n",
    "    \n",
    "    # Convert to Box object for dot notation access\n",
    "    return Box(config)\n",
    "\n",
    "# Create the configuration\n",
    "# Note: Replace 'your-username/your-model-name' with your actual HuggingFace username and desired model name\n",
    "publish_config = create_publish_config(\n",
    "    checkpoint_path=checkpoint_path,\n",
    "    base_model=\"gpt2\",\n",
    "    repo_id=\"your-username/your-model-name\"\n",
    ")\n",
    "\n",
    "# Print the configuration\n",
    "print(\"Publish Configuration:\")\n",
    "for key, value in publish_config.items():\n",
    "    if isinstance(value, dict):\n",
    "        print(f\"{key}:\")\n",
    "        for k, v in value.items():\n",
    "            print(f\"  {k}: {v}\")\n",
    "    else:\n",
    "        print(f\"{key}: {value}\")\n",
    "\n",
    "# Save the configuration to a YAML file\n",
    "config_path = \"publish_config.yaml\"\n",
    "with open(config_path, \"w\") as f:\n",
    "    yaml.dump(publish_config.to_dict(), f)\n",
    "\n",
    "print(f\"\\nConfiguration saved to {config_path}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 5. Understanding the Configuration Parameters\n",
    "\n",
    "Let's go through the key configuration parameters for model publishing:\n",
    "\n",
    "### Format Conversion Parameters\n",
    "\n",
    "- **format**: The format of the checkpoint to convert. Currently, only \"fsdp\" is supported.\n",
    "- **base_model**: The name or path of the base model used for training. This is used to get the model architecture and configuration.\n",
    "- **checkpoint_path**: The path to the checkpoint file to convert.\n",
    "\n",
    "### Upload Parameters\n",
    "\n",
    "- **host**: The host where the model will be published. Currently, only \"huggingface\" is supported.\n",
    "- **repo_id**: The HuggingFace repository ID where the model will be published, in the format \"username/model-name\".\n",
    "- **commit_message**: The commit message for the model upload.\n",
    "\n",
    "### Advanced Parameters\n",
    "\n",
    "- **max_shard_size**: The maximum size of each model shard when uploading to HuggingFace. Default is \"5GB\".\n",
    "- **safe_serialization**: Whether to use safe serialization when uploading the model. Default is True.\n",
    "- **create_pr**: Whether to create a pull request instead of pushing directly to the repository. Default is False."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 6. Converting FSDP Checkpoints\n",
    "\n",
    "Let's convert our FSDP checkpoint to HuggingFace format using the `ConvertFSDPCheckpoint` class:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Create an instance of the FSDP conversion handler\n",
    "converter = ConvertFSDPCheckpoint(\n",
    "    host=publish_config.publish.host,\n",
    "    base_model=publish_config.publish.base_model,\n",
    "    checkpoint_path=publish_config.publish.checkpoint_path\n",
    ")\n",
    "\n",
    "# Convert the checkpoint\n",
    "try:\n",
    "    model = converter.execute()\n",
    "    print(f\"Model successfully converted from FSDP checkpoint\")\n",
    "    print(f\"Model type: {type(model).__name__}\")\n",
    "    print(f\"Number of parameters: {sum(p.numel() for p in model.parameters()):,}\")\n",
    "except Exception as e:\n",
    "    print(f\"Error converting checkpoint: {str(e)}\")\n",
    "    # For the tutorial, we'll load a pretrained model as a fallback\n",
    "    print(\"Loading pretrained model as fallback...\")\n",
    "    model = AutoModelForCausalLM.from_pretrained(publish_config.publish.base_model)\n",
    "\n",
    "# Load the tokenizer\n",
    "tokenizer = AutoTokenizer.from_pretrained(publish_config.publish.base_model)\n",
    "print(f\"Tokenizer loaded: {tokenizer.__class__.__name__}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 7. Uploading Models to HuggingFace\n",
    "\n",
    "Now that we have converted our model, let's upload it to the HuggingFace Hub. First, we need to log in to HuggingFace:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Log in to HuggingFace\n",
    "# Uncomment the following line and replace with your token to log in\n",
    "# login(token=\"your_huggingface_token\")\n",
    "\n",
    "# For the tutorial, we'll skip the actual upload to avoid modifying HuggingFace repositories\n",
    "# In a real scenario, you would uncomment the following code and run it\n",
    "\n",
    "'''\n",
    "# Create an instance of the HuggingFace upload handler\n",
    "uploader = UploadHuggingface(\n",
    "    base_model=publish_config.publish.base_model,\n",
    "    model=model,\n",
    "    tokenizer=tokenizer,\n",
    "    repo_id=publish_config.publish.repo_id\n",
    ")\n",
    "\n",
    "# Upload the model and tokenizer\n",
    "try:\n",
    "    uploader.execute(\n",
    "        message=publish_config.publish.commit_message,\n",
    "        max_shard_size=publish_config.publish.max_shard_size,\n",
    "        safe_serialization=publish_config.publish.safe_serialization,\n",
    "        create_pr=publish_config.publish.create_pr\n",
    "    )\n",
    "    print(f\"Model and tokenizer successfully uploaded to {publish_config.publish.repo_id}\")\n",
    "    print(f\"Model available at: https://huggingface.co/{publish_config.publish.repo_id}\")\n",
    "except Exception as e:\n",
    "    print(f\"Error uploading model: {str(e)}\")\n",
    "'''\n",
    "\n",
    "print(\"To upload the model to HuggingFace:\")\n",
    "print(\"1. Uncomment the login line and replace with your HuggingFace token\")\n",
    "print(\"2. Uncomment the upload code block\")\n",
    "print(\"3. Run the cell\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 8. Publishing a Model with PublishOrchestrator\n",
    "\n",
    "Instead of using the individual components, we can use the `PublishOrchestrator` to handle the entire publishing workflow:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Create an instance of the PublishOrchestrator\n",
    "orchestrator = PublishOrchestrator(publish_config)\n",
    "\n",
    "# For the tutorial, we'll skip the actual execution to avoid modifying HuggingFace repositories\n",
    "# In a real scenario, you would uncomment the following code and run it\n",
    "\n",
    "'''\n",
    "# Execute the publish workflow\n",
    "try:\n",
    "    orchestrator.execute()\n",
    "    print(f\"Model successfully published to {publish_config.publish.repo_id}\")\n",
    "    print(f\"Model available at: https://huggingface.co/{publish_config.publish.repo_id}\")\n",
    "except Exception as e:\n",
    "    print(f\"Error publishing model: {str(e)}\")\n",
    "'''\n",
    "\n",
    "# Alternatively, you can use the execute function directly with the config file\n",
    "'''\n",
    "from src.tasks.publish import execute\n",
    "execute(config_path)\n",
    "'''\n",
    "\n",
    "print(\"To publish the model using the orchestrator:\")\n",
    "print(\"1. Ensure you are logged in to HuggingFace\")\n",
    "print(\"2. Uncomment the execution code block\")\n",
    "print(\"3. Run the cell\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 9. Validating the Published Model\n",
    "\n",
    "After publishing a model, it's important to validate that it works correctly:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def validate_published_model(repo_id):\n",
    "    \"\"\"\n",
    "    Validate a published model by loading it and running a test inference.\n",
    "    \n",
    "    Args:\n",
    "        repo_id: HuggingFace repository ID of the published model\n",
    "    \"\"\"\n",
    "    print(f\"Validating model from {repo_id}...\")\n",
    "    \n",
    "    try:\n",
    "        # Load model and tokenizer from repo\n",
    "        model = AutoModelForCausalLM.from_pretrained(repo_id)\n",
    "        tokenizer = AutoTokenizer.from_pretrained(repo_id)\n",
    "        \n",
    "        # Print model information\n",
    "        print(f\"Model type: {type(model).__name__}\")\n",
    "        print(f\"Number of parameters: {sum(p.numel() for p in model.parameters()):,}\")\n",
    "        \n",
    "        # Test tokenization\n",
    "        test_text = \"Hello, world!\"\n",
    "        tokens = tokenizer(test_text, return_tensors=\"pt\")\n",
    "        print(f\"Tokenized '{test_text}' to {tokens['input_ids'].shape[1]} tokens\")\n",
    "        \n",
    "        # Test generation\n",
    "        with torch.no_grad():\n",
    "            outputs = model.generate(\n",
    "                tokens[\"input_ids\"],\n",
    "                max_length=20,\n",
    "                num_return_sequences=1,\n",
    "                pad_token_id=tokenizer.eos_token_id\n",
    "            )\n",
    "        \n",
    "        # Decode and print the generated text\n",
    "        generated_text = tokenizer.decode(outputs[0], skip_special_tokens=True)\n",
    "        print(f\"Generated text: {generated_text}\")\n",
    "        \n",
    "        print(\"✅ Model validation successful!\")\n",
    "        return True\n",
    "    except Exception as e:\n",
    "        print(f\"❌ Model validation failed: {str(e)}\")\n",
    "        return False\n",
    "\n",
    "# For the tutorial, we'll validate the base model instead of the published model\n",
    "# In a real scenario, you would validate the published model\n",
    "print(\"Validating the base model...\")\n",
    "validate_published_model(publish_config.publish.base_model)\n",
    "\n",
    "# To validate the published model, uncomment the following line\n",
    "# validate_published_model(publish_config.publish.repo_id)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 10. Best Practices for Model Publishing\n",
    "\n",
    "Here are some best practices to follow when publishing models:\n",
    "\n",
    "### Model Preparation\n",
    "\n",
    "- **Clean Checkpoints**: Ensure your checkpoint is clean and contains only the necessary model weights.\n",
    "- **Weight Tying**: Make sure weight tying is properly applied, especially for language models where the embedding and output layers often share weights.\n",
    "- **Parameter Validation**: Verify that all parameters are loaded correctly by checking the percentage of loaded parameters.\n",
    "\n",
    "### Repository Setup\n",
    "\n",
    "- **Clear Repository Name**: Choose a clear and descriptive repository name that reflects the model's purpose and architecture.\n",
    "- **Model Card**: Create a comprehensive model card that describes the model, its training data, performance, limitations, and intended use cases.\n",
    "- **License**: Include a clear license that specifies how the model can be used.\n",
    "\n",
    "### Upload Configuration\n",
    "\n",
    "- **Shard Size**: For large models, use an appropriate shard size to avoid timeout issues during upload.\n",
    "- **Safe Serialization**: Enable safe serialization to ensure the model can be loaded reliably.\n",
    "- **Pull Requests**: Consider using pull requests for collaborative model development.\n",
    "\n",
    "### Validation\n",
    "\n",
    "- **Functional Testing**: Verify that the model can generate text correctly after uploading.\n",
    "- **Performance Comparison**: Compare the performance of the uploaded model with the original checkpoint to ensure no degradation.\n",
    "- **Integration Testing**: Test the model in the intended application context to ensure it meets requirements."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 11. Integration with CLM Training\n",
    "\n",
    "The publish module is designed to work seamlessly with the CLM training module. Here's a typical workflow that integrates CLM training and model publishing:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Example workflow integrating CLM training and model publishing\n",
    "\n",
    "# 1. CLM training configuration\n",
    "clm_training_config = {\n",
    "    \"task\": \"pretraining\",\n",
    "    \"model_name\": \"gpt2\",\n",
    "    \"dataset\": {\n",
    "        \"source\": \"local\",\n",
    "        \"nameOrPath\": \"tokenized_dataset\"\n",
    "    },\n",
    "    \"output_dir\": \"trained_model\",\n",
    "    # Other training parameters...\n",
    "}\n",
    "\n",
    "# 2. Publish configuration using the trained model checkpoint\n",
    "publish_config = {\n",
    "    \"task\": \"publish\",\n",
    "    \"publish\": {\n",
    "        \"format\": \"fsdp\",\n",
    "        \"base_model\": \"gpt2\",\n",
    "        \"checkpoint_path\": \"trained_model/checkpoint.pt\",  # Output from CLM training\n",
    "        \"host\": \"huggingface\",\n",
    "        \"repo_id\": \"your-username/your-model-name\"\n",
    "    }\n",
    "}\n",
    "\n",
    "# In a real scenario, you would run:\n",
    "'''\n",
    "# 1. Execute CLM training\n",
    "from src.tasks.clm_training import execute as train\n",
    "train(\"path/to/clm_training_config.yaml\")\n",
    "\n",
    "# 2. Execute model publishing\n",
    "from src.tasks.publish import execute as publish\n",
    "publish(\"path/to/publish_config.yaml\")\n",
    "'''\n",
    "\n",
    "print(\"This example shows how to integrate CLM training and model publishing tasks.\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 12. Conclusion\n",
    "\n",
    "In this tutorial, we've covered the basics of model publishing using the Continual Pretraining Framework. We've learned how to:\n",
    "\n",
    "1. Set up a publishing configuration\n",
    "2. Convert FSDP checkpoints to HuggingFace format\n",
    "3. Upload models to the HuggingFace Hub\n",
    "4. Validate published models\n",
    "5. Follow best practices for model publishing\n",
    "6. Integrate model publishing with CLM training\n",
    "\n",
    "The publish module provides a simple and efficient way to share your trained models with the community, making it easy to collaborate and build upon your work.\n",
    "\n",
    "For more advanced usage, refer to the framework documentation and experiment with different configurations to find what works best for your specific use case."
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.12"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
