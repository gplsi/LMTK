task: tokenization
experiment_name: tutorial_tokenization
verbose_level: 4

# Tokenization configuration
tokenization:
  # Tokenizer configuration
  tokenizer:
    name: gpt2
    use_fast: true
    
  # Input data configuration
  input:
    source: local
    path: /path/to/your/raw_text_data
    format: text
    
  # Output configuration
  output:
    path: /path/to/output_tokenized_dataset
    save_format: arrow
    
  # Processing configuration
  processing:
    chunk_size: 1000
    max_length: 1024
    num_proc: 4
    batch_size: 1000
    
  # Text preprocessing configuration
  preprocessing:
    lowercase: false
    strip_accents: false
    add_special_tokens: true
