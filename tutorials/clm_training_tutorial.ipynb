{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Causal Language Model (CLM) Training Tutorial\n",
    "\n",
    "This tutorial demonstrates how to train a causal language model using the Continual Pretraining Framework. We'll cover the following topics:\n",
    "\n",
    "1. Understanding CLM training concepts\n",
    "2. Setting up the training configuration\n",
    "3. Loading a tokenized dataset\n",
    "4. Selecting a training strategy\n",
    "5. Training a model using the ContinualOrchestrator\n",
    "6. Monitoring training progress\n",
    "7. Evaluating the trained model\n",
    "8. Best practices and optimization tips\n",
    "\n",
    "This tutorial assumes you have already completed the tokenization tutorial and have a tokenized dataset available."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# ============================================================================="
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Understanding CLM Training Concepts\n",
    "\n",
    "Causal Language Model (CLM) training is a fundamental technique for training large language models. In CLM training, the model learns to predict the next token in a sequence given all previous tokens. This is also known as autoregressive language modeling.\n",
    "\n",
    "Key concepts in CLM training include:\n",
    "\n",
    "- **Autoregressive Prediction**: The model predicts one token at a time, with each prediction conditioned on all previous tokens.\n",
    "- **Causal Attention Mask**: Ensures that the model can only attend to previous tokens in the sequence, not future ones.\n",
    "- **Next Token Prediction Loss**: The training objective is to minimize the negative log-likelihood of predicting the correct next token.\n",
    "- **Distributed Training**: Large models often require training across multiple GPUs or nodes using strategies like FSDP, DDP, or DeepSpeed.\n",
    "- **Gradient Accumulation**: Accumulating gradients across multiple batches to simulate larger batch sizes.\n",
    "- **Learning Rate Scheduling**: Adjusting the learning rate during training to improve convergence.\n",
    "\n",
    "The Continual Pretraining Framework provides a comprehensive implementation for CLM training with various distributed training strategies, making it easy to train large language models efficiently."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# ============================================================================="
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Creating a correct yaml file for the clm_training task"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# ===========================\n",
    "# CLM Training Configuration\n",
    "# ===========================\n",
    "\n",
    "# --- Task Info ---\n",
    "task: \"clm_training\"                  # Causal Language Modeling training\n",
    "experiment_name: \"tutorial_clm_training\"\n",
    "verbose_level: 4\n",
    "\n",
    "# --- Dataset ---\n",
    "dataset:\n",
    "  source: \"local\"                    # or \"hf\" for Hugging Face datasets\n",
    "  nameOrPath: \"tutorials/sample_tokenized_dataset\"\n",
    "  format: \"hf\"\n",
    "\n",
    "# --- Model ---\n",
    "model_name: \"openai-community/gpt2\"  # Pretrained model or custom path\n",
    "precision: \"bf16-true\"\n",
    "\n",
    "# --- Training Parameters ---\n",
    "number_epochs: 1\n",
    "batch_size: 8\n",
    "gradient_accumulation: true          # Accumulate n steps before backprop\n",
    "gradient_accumulation_steps: 2       # Effective batch size = batch_size * steps\n",
    "grad_clip: 1.0\n",
    "lr: 0.00002\n",
    "lr_decay: true\n",
    "weight_decay: 0.01\n",
    "beta1: 0.9\n",
    "beta2: 0.95\n",
    "lr_scheduler: \"warmup_linear\"        # Several schedules supported\n",
    "warmup_proportion: 0.06              # Proportion of steps for warmup\n",
    "\n",
    "# --- Validation ---\n",
    "validate_after_epoch: false           # Validate after each epoch\n",
    "validate_on_end: false                # Validate at end of training\n",
    "validate_after_k_steps: 1000          # Validate every k steps\n",
    "\n",
    "# --- Checkpointing ---\n",
    "save_on_validate: false\n",
    "save_on_end: true\n",
    "output_dir: \"tutorials/output\"\n",
    "\n",
    "# --- Parallelization ---\n",
    "parallelization_strategy: \"fsdp\"      # Supported: \"fsdp\", \"ddp\"\n",
    "auto_wrap_policy: \"gpt2\"\n",
    "sharding_strategy: \"FULL_SHARD\"\n",
    "state_dict_type: \"sharded\"\n",
    "limit_all_gathers: true\n",
    "cpu_offload: false\n",
    "num_workers: 4\n",
    "gradient_checkpointing: true\n",
    "\n",
    "# --- Logging ---\n",
    "logging_config: \"wandb\"\n",
    "wandb_project: \"your_wandb_project\"   # Replace with your WandB project name\n",
    "wandb_entity: \"your_wandb_entity\"     # Replace with your WandB entity\n",
    "log_model: true\n",
    "log_iter_interval: 10\n",
    "\n",
    "# --- Usage ---\n",
    "# Run with:\n",
    "# python src/main.py --config path/to/clm_training_tutorial.yaml"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# ====================================================================="
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Let's load the configuration file"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Loaded config keys: dict_keys(['auto_wrap_policy', 'batch_size', 'beta1', 'beta2', 'cpu_offload', 'dataset', 'experiment_name', 'grad_clip', 'gradient_accumulation', 'gradient_accumulation_steps', 'gradient_checkpointing', 'limit_all_gathers', 'log_iter_interval', 'log_model', 'logging_config', 'lr', 'lr_decay', 'lr_scheduler', 'model_name', 'num_workers', 'number_epochs', 'output_dir', 'parallelization_strategy', 'precision', 'save_on_end', 'save_on_validate', 'sharding_strategy', 'state_dict_type', 'task', 'validate_after_epoch', 'validate_after_k_steps', 'validate_on_end', 'verbose_level', 'wandb_entity', 'wandb_project', 'warmup_proportion', 'weight_decay'])\n"
     ]
    }
   ],
   "source": [
    "import yaml\n",
    "from box import Box\n",
    "\n",
    "# Load the config from the YAML file\n",
    "with open(\"/workspace/tutorials/configs/clm_training_tutorial.yaml\", \"r\") as f:\n",
    "    clm_config = Box(yaml.safe_load(f), default_box=True)\n",
    "\n",
    "print(\"Loaded config keys:\", clm_config.keys())"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# ============================================================================="
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Creating a Sample Dataset\n",
    "\n",
    "For this tutorial, we'll use a small sample dataset. In a real-world scenario, you would typically use the tokenized dataset from the tokenization step. Let's first create a small sample dataset for demonstration purposes:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 45,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Saving the dataset (1/1 shards): 100%|██████████| 1/1 [00:00<00:00, 430.72 examples/s]\n",
      "Saving the dataset (1/1 shards): 100%|██████████| 1/1 [00:00<00:00, 503.82 examples/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Sample dataset created at: sample_tokenized_dataset\n",
      "Dataset path in config set to: sample_tokenized_dataset\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n"
     ]
    }
   ],
   "source": [
    "from datasets import Dataset, DatasetDict, load_from_disk\n",
    "from pathlib import Path\n",
    "\n",
    "# If you already have a tokenized dataset, skip this cell.\n",
    "# This creates a small dummy dataset for demonstration.\n",
    "sample_dataset_dir = Path(\"sample_tokenized_dataset\")\n",
    "if not sample_dataset_dir.exists():\n",
    "    train_data = {\n",
    "        \"input_ids\": [[101, 2023, 2003, 1037, 4937, 2361, 1012, 102] + [0]*8],\n",
    "        \"attention_mask\": [[1]*8 + [0]*8],\n",
    "        \"labels\": [[101, 2023, 2003, 1037, 4937, 2361, 1012, 102] + [0]*8],\n",
    "    }\n",
    "    valid_data = {\n",
    "        \"input_ids\": [[101, 2023, 2003, 1037, 3231, 2361, 1012, 102] + [0]*8],\n",
    "        \"attention_mask\": [[1]*8 + [0]*8],\n",
    "        \"labels\": [[101, 2023, 2003, 1037, 3231, 2361, 1012, 102] + [0]*8],\n",
    "    }\n",
    "    ds = DatasetDict({\n",
    "        \"train\": Dataset.from_dict(train_data),\n",
    "        \"valid\": Dataset.from_dict(valid_data),\n",
    "    })\n",
    "    ds.save_to_disk(str(sample_dataset_dir))\n",
    "    print(\"Sample dataset created at:\", sample_dataset_dir)\n",
    "else:\n",
    "    print(\"Sample dataset already exists at:\", sample_dataset_dir)\n",
    "    \n",
    "    \n",
    "# If you want to use the sample dataset, update the config in-memory\n",
    "clm_config.dataset.nameOrPath = str(sample_dataset_dir)\n",
    "print(\"Dataset path in config set to:\", clm_config.dataset.nameOrPath)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# ============================================================================="
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Loading a Tokenized Dataset\n",
    "\n",
    "In a real-world scenario, you would load the tokenized dataset created in the tokenization step. Let's see how to load a tokenized dataset from disk:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 46,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Loaded dataset splits: ['train', 'valid']\n",
      "Number of examples in train split: 1\n",
      "Number of examples in valid split: 1\n",
      "First example from train split: {'input_ids': [101, 2023, 2003, 1037, 4937, 2361, 1012, 102, 0, 0, 0, 0, 0, 0, 0, 0], 'attention_mask': [1, 1, 1, 1, 1, 1, 1, 1, 0, 0, 0, 0, 0, 0, 0, 0], 'labels': [101, 2023, 2003, 1037, 4937, 2361, 1012, 102, 0, 0, 0, 0, 0, 0, 0, 0]}\n"
     ]
    }
   ],
   "source": [
    "loaded_dataset = load_from_disk(clm_config.dataset.nameOrPath)\n",
    "print(\"Loaded dataset splits:\", list(loaded_dataset.keys()))\n",
    "print(\"Number of examples in train split:\", len(loaded_dataset[\"train\"]))\n",
    "print(\"Number of examples in valid split:\", len(loaded_dataset[\"valid\"]))\n",
    "print(\"First example from train split:\", loaded_dataset[\"train\"][0])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# ============================================================================="
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Validate Dataset Columns"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 50,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "All required columns present: True\n",
      "Updated config saved to /workspace/tutorials/configs/clm_training_tutorial.yaml\n"
     ]
    }
   ],
   "source": [
    "required_columns = [\"input_ids\", \"attention_mask\", \"labels\"]\n",
    "all_columns_present = all(col in loaded_dataset[\"train\"].column_names for col in required_columns)\n",
    "print(\"All required columns present:\", all_columns_present)\n",
    "assert all_columns_present, \"Dataset is missing required columns for CLM training!\"\n",
    "\n",
    "\n",
    "import yaml\n",
    "\n",
    "with open(\"/workspace/tutorials/configs/clm_training_tutorial.yaml\", \"w\") as f:\n",
    "    yaml.dump(clm_config.to_dict(), f)\n",
    "print(\"Updated config saved to /workspace/tutorials/configs/clm_training_tutorial.yaml\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# ============================================================================="
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Run Training "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 51,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "/usr/local/lib/python3.12/dist-packages/transformers/utils/hub.py:124: FutureWarning: Using `TRANSFORMERS_CACHE` is deprecated and will be removed in v5 of Transformers. Use `HF_HOME` instead.\n",
      "  warnings.warn(\n",
      "2025-06-16 11:11:06 - utils.orchestrator - \u001b[0;32mINFO\u001b[0m - \u001b[0;32mFound 1 CUDA devices available for training\u001b[0m\n",
      "[\u001b[0;32mINFO\u001b[0m | utils.orchestrator ]: \u001b[0;32mFound 1 CUDA devices available for training\u001b[0m\n",
      "2025-06-16 11:11:06 - utils.orchestrator - \u001b[0;32mINFO\u001b[0m - \u001b[0;32mFound 1 CUDA devices available for training\u001b[0m\n",
      "[\u001b[0;32mINFO\u001b[0m | utils.orchestrator ]: \u001b[0;32mFound 1 CUDA devices available for training\u001b[0m\n",
      "2025-06-16 11:11:06 - utils.orchestrator - \u001b[0;32mINFO\u001b[0m - \u001b[0;32mStarting training pipeline\u001b[0m\n",
      "[\u001b[0;32mINFO\u001b[0m | utils.orchestrator ]: \u001b[0;32mStarting training pipeline\u001b[0m\n",
      "2025-06-16 11:11:06 - utils.orchestrator - \u001b[0;36mDEBUG\u001b[0m - \u001b[0;36mOrchestrator config gradient_accumulation_steps: value=2, type=<class 'int'>\u001b[0m\n",
      "[\u001b[0;36mDEBUG\u001b[0m | utils.orchestrator ]: \u001b[0;36mOrchestrator config gradient_accumulation_steps: value=2, type=<class 'int'>\u001b[0m\n",
      "2025-06-16 11:11:06 - utils.orchestrator - \u001b[0;36mDEBUG\u001b[0m - \u001b[0;36mOrchestrator config validate_after_k_steps: value=1000, type=<class 'int'>\u001b[0m\n",
      "[\u001b[0;36mDEBUG\u001b[0m | utils.orchestrator ]: \u001b[0;36mOrchestrator config validate_after_k_steps: value=1000, type=<class 'int'>\u001b[0m\n",
      "2025-06-16 11:11:06 - utils.orchestrator - \u001b[0;36mDEBUG\u001b[0m - \u001b[0;36mOrchestrator config batch_size: value=8, type=<class 'int'>\u001b[0m\n",
      "[\u001b[0;36mDEBUG\u001b[0m | utils.orchestrator ]: \u001b[0;36mOrchestrator config batch_size: value=8, type=<class 'int'>\u001b[0m\n",
      "2025-06-16 11:11:06 - utils.orchestrator - \u001b[0;32mINFO\u001b[0m - \u001b[0;32mLoading dataset from path 'sample_tokenized_dataset'\u001b[0m\n",
      "[\u001b[0;32mINFO\u001b[0m | utils.orchestrator ]: \u001b[0;32mLoading dataset from path 'sample_tokenized_dataset'\u001b[0m\n",
      "2025-06-16 11:11:06 - utils.orchestrator - \u001b[0;32mINFO\u001b[0m - \u001b[0;32mStarting FSDP continual pretraining task\u001b[0m\n",
      "[\u001b[0;32mINFO\u001b[0m | utils.orchestrator ]: \u001b[0;32mStarting FSDP continual pretraining task\u001b[0m\n",
      "2025-06-16 11:11:06 - src.tasks.clm_training.fabric.base - \u001b[0;32mINFO\u001b[0m - \u001b[0;32mSetting up FSDP strategy.\u001b[0m\n",
      "[\u001b[0;32mINFO\u001b[0m | src.tasks.clm_training.fabric.base]: \u001b[0;32mSetting up FSDP strategy.\u001b[0m\n",
      "2025-06-16 11:11:06 - src.tasks.clm_training.fabric.base - \u001b[0;36mDEBUG\u001b[0m - \u001b[0;36mConfig gradient_accumulation_steps: value=2, type=<class 'int'>\u001b[0m\n",
      "[\u001b[0;36mDEBUG\u001b[0m | src.tasks.clm_training.fabric.base]: \u001b[0;36mConfig gradient_accumulation_steps: value=2, type=<class 'int'>\u001b[0m\n",
      "2025-06-16 11:11:06 - src.tasks.clm_training.fabric.base - \u001b[0;36mDEBUG\u001b[0m - \u001b[0;36mConfig validate_after_k_steps: value=1000, type=<class 'int'>\u001b[0m\n",
      "[\u001b[0;36mDEBUG\u001b[0m | src.tasks.clm_training.fabric.base]: \u001b[0;36mConfig validate_after_k_steps: value=1000, type=<class 'int'>\u001b[0m\n",
      "2025-06-16 11:11:06 - src.tasks.clm_training.fabric.base - \u001b[0;36mDEBUG\u001b[0m - \u001b[0;36mConfig batch_size: value=8, type=<class 'int'>\u001b[0m\n",
      "[\u001b[0;36mDEBUG\u001b[0m | src.tasks.clm_training.fabric.base]: \u001b[0;36mConfig batch_size: value=8, type=<class 'int'>\u001b[0m\n",
      "2025-06-16 11:11:06 - src.tasks.clm_training.fabric.base - \u001b[0;32mINFO\u001b[0m - \u001b[0;32mSetting up FSDP strategy.\u001b[0m\n",
      "[\u001b[0;32mINFO\u001b[0m | src.tasks.clm_training.fabric.base]: \u001b[0;32mSetting up FSDP strategy.\u001b[0m\n",
      "2025-06-16 11:11:06 - src.tasks.clm_training.fabric.base - \u001b[0;32mINFO\u001b[0m - \u001b[0;32mUsing Devices: 1\u001b[0m\n",
      "[\u001b[0;32mINFO\u001b[0m | src.tasks.clm_training.fabric.base]: \u001b[0;32mUsing Devices: 1\u001b[0m\n",
      "2025-06-16 11:11:06 - src.tasks.clm_training.fabric.base - \u001b[0;33mWARNING\u001b[0m - \u001b[0;33mUsing automatic strategy for 1 device.\u001b[0m\n",
      "[\u001b[0;33mWARNING\u001b[0m | src.tasks.clm_training.fabric.base]: \u001b[0;33mUsing automatic strategy for 1 device.\u001b[0m\n",
      "2025-06-16 11:11:06 - src.tasks.clm_training.fabric.base - \u001b[0;36mDEBUG\u001b[0m - \u001b[0;36m{'key': 'eval_batch_size', 'value': 8, 'strategy': 'auto'}\u001b[0m\n",
      "[\u001b[0;36mDEBUG\u001b[0m | src.tasks.clm_training.fabric.base]: \u001b[0;36m{'key': 'eval_batch_size', 'value': 8, 'strategy': 'auto'}\u001b[0m\n",
      "/usr/local/lib/python3.12/dist-packages/huggingface_hub/file_download.py:943: FutureWarning: `resume_download` is deprecated and will be removed in version 1.0.0. Downloads always resume when possible. If you want to force a new download, use `force_download=True`.\n",
      "  warnings.warn(\n",
      "2025-06-16 11:11:07 - src.tasks.clm_training.fabric.base - \u001b[0;32mINFO\u001b[0m - \u001b[0;32mTime to instantiate model: 0.58 seconds.\u001b[0m\n",
      "[\u001b[0;32mINFO\u001b[0m | src.tasks.clm_training.fabric.base]: \u001b[0;32mTime to instantiate model: 0.58 seconds.\u001b[0m\n",
      "2025-06-16 11:11:07 - src.tasks.clm_training.fabric.base - \u001b[0;36mDEBUG\u001b[0m - \u001b[0;36mRunning Epoch 1 of 1\u001b[0m\n",
      "[\u001b[0;36mDEBUG\u001b[0m | src.tasks.clm_training.fabric.base]: \u001b[0;36mRunning Epoch 1 of 1\u001b[0m\n",
      "  0%|\u001b[34m                                                     \u001b[0m| 0/1 [00:00<?, ?it/s]\u001b[0m2025-06-16 11:11:07 - src.tasks.clm_training.fabric.base - \u001b[0;36mDEBUG\u001b[0m - \u001b[0;36mgradient_accumulation_steps raw value: 2, type: <class 'int'>\u001b[0m\n",
      "[\u001b[0;36mDEBUG\u001b[0m | src.tasks.clm_training.fabric.base]: \u001b[0;36mgradient_accumulation_steps raw value: 2, type: <class 'int'>\u001b[0m\n",
      "2025-06-16 11:11:07 - src.tasks.clm_training.fabric.base - \u001b[0;36mDEBUG\u001b[0m - \u001b[0;36mgradient_accumulation_steps converted: 2, type: <class 'int'>\u001b[0m\n",
      "[\u001b[0;36mDEBUG\u001b[0m | src.tasks.clm_training.fabric.base]: \u001b[0;36mgradient_accumulation_steps converted: 2, type: <class 'int'>\u001b[0m\n",
      "2025-06-16 11:11:07 - src.tasks.clm_training.fabric.base - \u001b[0;36mDEBUG\u001b[0m - \u001b[0;36miter_num value: 0, type: <class 'int'>\u001b[0m\n",
      "[\u001b[0;36mDEBUG\u001b[0m | src.tasks.clm_training.fabric.base]: \u001b[0;36miter_num value: 0, type: <class 'int'>\u001b[0m\n",
      "2025-06-16 11:11:07 - src.tasks.clm_training.fabric.base - \u001b[0;36mDEBUG\u001b[0m - \u001b[0;36mAbout to check: (iter_num + 1) % gradient_accumulation_steps != 0 -> (0 + 1) % 2 != 0\u001b[0m\n",
      "[\u001b[0;36mDEBUG\u001b[0m | src.tasks.clm_training.fabric.base]: \u001b[0;36mAbout to check: (iter_num + 1) % gradient_accumulation_steps != 0 -> (0 + 1) % 2 != 0\u001b[0m\n",
      "2025-06-16 11:11:07 - src.tasks.clm_training.fabric.base - \u001b[0;36mDEBUG\u001b[0m - \u001b[0;36miter 1 step 0: loss 9.4375, iter time: 211.78ms remaining time: \u001b[0m\n",
      "[\u001b[0;36mDEBUG\u001b[0m | src.tasks.clm_training.fabric.base]: \u001b[0;36miter 1 step 0: loss 9.4375, iter time: 211.78ms remaining time: \u001b[0m\n",
      "100%|\u001b[34m█████████████████████████████████████████████\u001b[0m| 1/1 [00:00<00:00,  3.38it/s]\u001b[0m\n",
      "2025-06-16 11:11:07 - src.tasks.clm_training.fabric.base - \u001b[0;36mDEBUG\u001b[0m - \u001b[0;36mvalidate_after_k_steps value: 1000, type: <class 'int'>\u001b[0m\n",
      "[\u001b[0;36mDEBUG\u001b[0m | src.tasks.clm_training.fabric.base]: \u001b[0;36mvalidate_after_k_steps value: 1000, type: <class 'int'>\u001b[0m\n",
      "2025-06-16 11:11:07 - src.tasks.clm_training.fabric.base - \u001b[0;36mDEBUG\u001b[0m - \u001b[0;36mstep_count value: 0, type: <class 'int'>\u001b[0m\n",
      "[\u001b[0;36mDEBUG\u001b[0m | src.tasks.clm_training.fabric.base]: \u001b[0;36mstep_count value: 0, type: <class 'int'>\u001b[0m\n",
      "2025-06-16 11:11:07 - src.tasks.clm_training.fabric.base - \u001b[0;36mDEBUG\u001b[0m - \u001b[0;36mValidation condition check - epochFinished: True, trainingFinished: False\u001b[0m\n",
      "[\u001b[0;36mDEBUG\u001b[0m | src.tasks.clm_training.fabric.base]: \u001b[0;36mValidation condition check - epochFinished: True, trainingFinished: False\u001b[0m\n",
      "2025-06-16 11:11:07 - src.tasks.clm_training.fabric.base - \u001b[0;36mDEBUG\u001b[0m - \u001b[0;36mAbout to check: step_count % validate_after_k_steps == 0 -> 0 % 1000 == 0\u001b[0m\n",
      "[\u001b[0;36mDEBUG\u001b[0m | src.tasks.clm_training.fabric.base]: \u001b[0;36mAbout to check: step_count % validate_after_k_steps == 0 -> 0 % 1000 == 0\u001b[0m\n",
      "2025-06-16 11:11:07 - src.tasks.clm_training.fabric.base - \u001b[0;36mDEBUG\u001b[0m - \u001b[0;36mvalidate_after_k_steps value: 1000, type: <class 'int'>\u001b[0m\n",
      "[\u001b[0;36mDEBUG\u001b[0m | src.tasks.clm_training.fabric.base]: \u001b[0;36mvalidate_after_k_steps value: 1000, type: <class 'int'>\u001b[0m\n",
      "2025-06-16 11:11:07 - src.tasks.clm_training.fabric.base - \u001b[0;36mDEBUG\u001b[0m - \u001b[0;36mstep_count value: 0, type: <class 'int'>\u001b[0m\n",
      "[\u001b[0;36mDEBUG\u001b[0m | src.tasks.clm_training.fabric.base]: \u001b[0;36mstep_count value: 0, type: <class 'int'>\u001b[0m\n",
      "2025-06-16 11:11:07 - src.tasks.clm_training.fabric.base - \u001b[0;36mDEBUG\u001b[0m - \u001b[0;36mValidation condition check - epochFinished: False, trainingFinished: True\u001b[0m\n",
      "[\u001b[0;36mDEBUG\u001b[0m | src.tasks.clm_training.fabric.base]: \u001b[0;36mValidation condition check - epochFinished: False, trainingFinished: True\u001b[0m\n",
      "2025-06-16 11:11:07 - src.tasks.clm_training.fabric.base - \u001b[0;36mDEBUG\u001b[0m - \u001b[0;36mAbout to check: step_count % validate_after_k_steps == 0 -> 0 % 1000 == 0\u001b[0m\n",
      "[\u001b[0;36mDEBUG\u001b[0m | src.tasks.clm_training.fabric.base]: \u001b[0;36mAbout to check: step_count % validate_after_k_steps == 0 -> 0 % 1000 == 0\u001b[0m\n",
      "2025-06-16 11:11:07 - src.tasks.clm_training.fabric.base - \u001b[0;32mINFO\u001b[0m - \u001b[0;32mSaving final checkpoint\u001b[0m\n",
      "[\u001b[0;32mINFO\u001b[0m | src.tasks.clm_training.fabric.base]: \u001b[0;32mSaving final checkpoint\u001b[0m\n",
      "2025-06-16 11:11:07 - src.tasks.clm_training.fabric.base - \u001b[0;36mDEBUG\u001b[0m - \u001b[0;36mSaving checkpoint to 'tutorials/output/epoch-001-final-ckpt.pth'\u001b[0m\n",
      "[\u001b[0;36mDEBUG\u001b[0m | src.tasks.clm_training.fabric.base]: \u001b[0;36mSaving checkpoint to 'tutorials/output/epoch-001-final-ckpt.pth'\u001b[0m\n",
      "2025-06-16 11:11:08 - src.tasks.clm_training.fabric.base - \u001b[0;32mINFO\u001b[0m - \u001b[0;32mTraining time: 0.84s\u001b[0m\n",
      "[\u001b[0;32mINFO\u001b[0m | src.tasks.clm_training.fabric.base]: \u001b[0;32mTraining time: 0.84s\u001b[0m\n",
      "2025-06-16 11:11:08 - src.tasks.clm_training.fabric.base - \u001b[0;32mINFO\u001b[0m - \u001b[0;32mMemory used: 0.89 GB\u001b[0m\n",
      "[\u001b[0;32mINFO\u001b[0m | src.tasks.clm_training.fabric.base]: \u001b[0;32mMemory used: 0.89 GB\u001b[0m\n",
      "2025-06-16 11:11:08 - utils.orchestrator - \u001b[0;32mINFO\u001b[0m - \u001b[0;32mFSDP training finished\u001b[0m\n",
      "[\u001b[0;32mINFO\u001b[0m | utils.orchestrator ]: \u001b[0;32mFSDP training finished\u001b[0m\n",
      "2025-06-16 11:11:08 - utils.orchestrator - \u001b[0;32mINFO\u001b[0m - \u001b[0;32mContinual Pretraining completed successfully\u001b[0m\n",
      "[\u001b[0;32mINFO\u001b[0m | utils.orchestrator ]: \u001b[0;32mContinual Pretraining completed successfully\u001b[0m\n"
     ]
    }
   ],
   "source": [
    "## Run Training from the Notebook\n",
    "\n",
    "# If not already in the workspace root, change directory\n",
    "import os\n",
    "os.chdir('/workspace')\n",
    "\n",
    "# Now run the training script\n",
    "!python src/main.py --config /workspace/tutorials/configs/clm_training_tutorial.yaml"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# ============================================================================="
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Understanding Distributed Training Strategies\n",
    "\n",
    "The Continual Pretraining Framework is intended to support all the distributed training strategies from Fabric Lightning:\n",
    "\n",
    "1. **FSDP (Fully Sharded Data Parallel)**: Shards model parameters, gradients, and optimizer states across GPUs, enabling training of very large models.\n",
    "2. **DDP (Distributed Data Parallel)**: Replicates the model on each GPU and synchronizes gradients, suitable for medium-sized models.\n",
    "3. **DeepSpeed**: Implements ZeRO optimization for efficient large model training with memory optimizations. (Not implemented yet)\n",
    "4. **DP (Data Parallel)**: Simple data parallelism for single-node multi-GPU setups.  (Not implemented yet)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# ============================================================================="
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Best Practices and Optimization Tips\n",
    "\n",
    "Here are some best practices and optimization tips for CLM training:\n",
    "\n",
    "### Model Selection and Hardware Requirements\n",
    "\n",
    "- **Model Size**: Choose a model size appropriate for your hardware. Larger models require more memory and compute.\n",
    "- **Distributed Training**: For models larger than 1B parameters, consider using distributed training strategies like FSDP or DeepSpeed.\n",
    "\n",
    "### Training Optimization\n",
    "\n",
    "- **Gradient Accumulation**: Use gradient accumulation to simulate larger batch sizes on limited hardware.\n",
    "- **Gradient Checkpointing**: Enable gradient checkpointing to reduce memory usage at the cost of increased computation time.\n",
    "- **Mixed Precision Training**: Use mixed precision training (bf16 or fp16) to reduce memory usage and speed up training.\n",
    "- **Learning Rate Scheduling**: Use a learning rate scheduler with warmup to improve training stability.\n",
    "\n",
    "### Dataset Preparation\n",
    "\n",
    "- **Dataset Size**: Larger datasets generally lead to better models, but also require more training time.\n",
    "- **Dataset Quality**: High-quality, diverse data is crucial for good model performance.\n",
    "- **Validation Split**: Always include a validation split to monitor training progress and prevent overfitting.\n",
    "\n",
    "### Monitoring and Debugging\n",
    "\n",
    "- **Regular Validation**: Validate the model regularly to catch issues early.\n",
    "- **Gradient Norms**: Monitor gradient norms to detect exploding or vanishing gradients.\n",
    "- **Learning Rate**: Start with a small learning rate and gradually increase it if training is stable.\n",
    "- **Memory Usage**: Monitor GPU memory usage to detect memory leaks or inefficient memory usage."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# ============================================================================="
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Integration with Tokenization\n",
    "\n",
    "The CLM training task is designed to work seamlessly with the tokenized dataset produced by the tokenization task. Here's how to integrate the two tasks:\n",
    "\n",
    "1. **Run the Tokenization Task**: First, run the tokenization task to prepare your dataset.\n",
    "2. **Configure CLM Training**: Set up your CLM training configuration to use the tokenized dataset.\n",
    "3. **Run CLM Training**: Execute the CLM training task using the tokenized dataset.\n",
    "\n",
    "Example workflow:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "/usr/local/lib/python3.12/dist-packages/transformers/utils/hub.py:124: FutureWarning: Using `TRANSFORMERS_CACHE` is deprecated and will be removed in v5 of Transformers. Use `HF_HOME` instead.\n",
      "  warnings.warn(\n",
      "2025-06-16 13:54:14 - src.utils.orchestrator - \u001b[0;32mINFO\u001b[0m - \u001b[0;32mStarting tokenization workflow\u001b[0m\n",
      "2025-06-16 13:54:14 - src.utils.orchestrator - \u001b[0;32mINFO\u001b[0m - \u001b[0;32mLoading dataset from files at dir 'tutorials/data/raw_text_data'\u001b[0m\n",
      "2025-06-16 13:54:14 - src.utils.dataset.storage - \u001b[0;32mINFO\u001b[0m - \u001b[0;32mProcessing files from 'tutorials/data/raw_text_data' and grouping by file extension.\u001b[0m\n",
      "2025-06-16 13:54:14 - src.utils.dataset.storage - \u001b[0;32mINFO\u001b[0m - \u001b[0;32mStarting directory scan in: tutorials/data/raw_text_data\u001b[0m\n",
      "2025-06-16 13:54:14 - src.utils.dataset.storage - \u001b[0;36mDEBUG\u001b[0m - \u001b[0;36mDirectory: tutorials/data/raw_text_data - Found 1/1 files with supported extensions ['txt', 'csv', 'json', 'jsonl']\u001b[0m\n",
      "2025-06-16 13:54:14 - src.utils.dataset.storage - \u001b[0;36mDEBUG\u001b[0m - \u001b[0;36mScan completed: Found 1 matching files across 1 directories\u001b[0m\n",
      "2025-06-16 13:54:14 - src.utils.dataset.storage - \u001b[0;36mDEBUG\u001b[0m - \u001b[0;36mGrouped files by extensions: ['txt (1)']\u001b[0m\n",
      "2025-06-16 13:54:15 - src.utils.dataset.storage - \u001b[0;32mINFO\u001b[0m - \u001b[0;32mDataset successfully built from 'txt' files.\u001b[0m\n",
      "2025-06-16 13:54:15 - src.tasks.tokenization.tokenizer.base - \u001b[0;32mINFO\u001b[0m - \u001b[0;32mInitializing tokenizer\u001b[0m\n",
      "2025-06-16 13:54:15 - src.tasks.tokenization.tokenizer.base - \u001b[0;32mINFO\u001b[0m - \u001b[0;32mInitializing tokenizer: openai-community/gpt2\u001b[0m\n",
      "/usr/local/lib/python3.12/dist-packages/huggingface_hub/file_download.py:943: FutureWarning: `resume_download` is deprecated and will be removed in version 1.0.0. Downloads always resume when possible. If you want to force a new download, use `force_download=True`.\n",
      "  warnings.warn(\n",
      "2025-06-16 13:54:15 - src.tasks.tokenization.tokenizer.base - \u001b[0;36mDEBUG\u001b[0m - \u001b[0;36mFast tokenizer detected - using internal Rust parallelism (num_proc=None)\u001b[0m\n",
      "2025-06-16 13:54:15 - src.tasks.tokenization.tokenizer.base - \u001b[0;32mINFO\u001b[0m - \u001b[0;32mPerformance configuration: batch_size=1024, parallelism=fast_tokenizer_internal\u001b[0m\n",
      "2025-06-16 13:54:15 - src.tasks.tokenization.tokenizer.base - \u001b[0;32mINFO\u001b[0m - \u001b[0;32mDetected 'DatasetDict' instance\u001b[0m\n",
      "2025-06-16 13:54:15 - src.tasks.tokenization.tokenizer.base - \u001b[0;32mINFO\u001b[0m - \u001b[0;32mTokenizing split 'train' with 37453 examples\u001b[0m\n",
      "2025-06-16 13:54:15 - src.tasks.tokenization.tokenizer.base - \u001b[0;32mINFO\u001b[0m - \u001b[0;32mCompleted tokenizing split 'train' in 0.01 seconds\u001b[0m\n",
      "2025-06-16 13:54:15 - src.tasks.tokenization.tokenizer.base - \u001b[0;32mINFO\u001b[0m - \u001b[0;32mProcessed 37453 examples at 3687281.8 examples/sec\u001b[0m\n",
      "2025-06-16 13:54:15 - src.tasks.tokenization.tokenizer.base - \u001b[0;36mDEBUG\u001b[0m - \u001b[0;36mOnly one dataset split found, returning single tokenized dataset\u001b[0m\n",
      "2025-06-16 13:54:15 - src.utils.dataset.storage - \u001b[0;32mINFO\u001b[0m - \u001b[0;32mSaving dataset to 'tutorials/data/sample_tokenized_dataset'\u001b[0m\n",
      "Saving the dataset (1/1 shards): 100%|█| 37453/37453 [00:00<00:00, 107641.03 exa\n",
      "2025-06-16 13:54:16 - src.utils.orchestrator - \u001b[0;32mINFO\u001b[0m - \u001b[0;32mTokenization workflow completed successfully\u001b[0m\n",
      "Running CLM training...\n",
      "/usr/local/lib/python3.12/dist-packages/transformers/utils/hub.py:124: FutureWarning: Using `TRANSFORMERS_CACHE` is deprecated and will be removed in v5 of Transformers. Use `HF_HOME` instead.\n",
      "  warnings.warn(\n",
      "2025-06-16 13:54:25 - utils.orchestrator - \u001b[0;32mINFO\u001b[0m - \u001b[0;32mFound 1 CUDA devices available for training\u001b[0m\n",
      "[\u001b[0;32mINFO\u001b[0m | utils.orchestrator ]: \u001b[0;32mFound 1 CUDA devices available for training\u001b[0m\n",
      "2025-06-16 13:54:25 - utils.orchestrator - \u001b[0;32mINFO\u001b[0m - \u001b[0;32mFound 1 CUDA devices available for training\u001b[0m\n",
      "[\u001b[0;32mINFO\u001b[0m | utils.orchestrator ]: \u001b[0;32mFound 1 CUDA devices available for training\u001b[0m\n",
      "2025-06-16 13:54:25 - utils.orchestrator - \u001b[0;32mINFO\u001b[0m - \u001b[0;32mStarting training pipeline\u001b[0m\n",
      "[\u001b[0;32mINFO\u001b[0m | utils.orchestrator ]: \u001b[0;32mStarting training pipeline\u001b[0m\n",
      "2025-06-16 13:54:25 - utils.orchestrator - \u001b[0;36mDEBUG\u001b[0m - \u001b[0;36mOrchestrator config gradient_accumulation_steps: value=2, type=<class 'int'>\u001b[0m\n",
      "[\u001b[0;36mDEBUG\u001b[0m | utils.orchestrator ]: \u001b[0;36mOrchestrator config gradient_accumulation_steps: value=2, type=<class 'int'>\u001b[0m\n",
      "2025-06-16 13:54:25 - utils.orchestrator - \u001b[0;36mDEBUG\u001b[0m - \u001b[0;36mOrchestrator config validate_after_k_steps: value=1000, type=<class 'int'>\u001b[0m\n",
      "[\u001b[0;36mDEBUG\u001b[0m | utils.orchestrator ]: \u001b[0;36mOrchestrator config validate_after_k_steps: value=1000, type=<class 'int'>\u001b[0m\n",
      "2025-06-16 13:54:25 - utils.orchestrator - \u001b[0;36mDEBUG\u001b[0m - \u001b[0;36mOrchestrator config batch_size: value=8, type=<class 'int'>\u001b[0m\n",
      "[\u001b[0;36mDEBUG\u001b[0m | utils.orchestrator ]: \u001b[0;36mOrchestrator config batch_size: value=8, type=<class 'int'>\u001b[0m\n",
      "2025-06-16 13:54:25 - utils.orchestrator - \u001b[0;32mINFO\u001b[0m - \u001b[0;32mLoading dataset from path '/workspace/tutorials/data/sample_tokenized_dataset'\u001b[0m\n",
      "[\u001b[0;32mINFO\u001b[0m | utils.orchestrator ]: \u001b[0;32mLoading dataset from path '/workspace/tutorials/data/sample_tokenized_dataset'\u001b[0m\n",
      "2025-06-16 13:54:25 - utils.orchestrator - \u001b[0;32mINFO\u001b[0m - \u001b[0;32mStarting FSDP continual pretraining task\u001b[0m\n",
      "[\u001b[0;32mINFO\u001b[0m | utils.orchestrator ]: \u001b[0;32mStarting FSDP continual pretraining task\u001b[0m\n",
      "2025-06-16 13:54:25 - src.tasks.clm_training.fabric.base - \u001b[0;32mINFO\u001b[0m - \u001b[0;32mSetting up FSDP strategy.\u001b[0m\n",
      "[\u001b[0;32mINFO\u001b[0m | src.tasks.clm_training.fabric.base]: \u001b[0;32mSetting up FSDP strategy.\u001b[0m\n",
      "2025-06-16 13:54:25 - src.tasks.clm_training.fabric.base - \u001b[0;36mDEBUG\u001b[0m - \u001b[0;36mConfig gradient_accumulation_steps: value=2, type=<class 'int'>\u001b[0m\n",
      "[\u001b[0;36mDEBUG\u001b[0m | src.tasks.clm_training.fabric.base]: \u001b[0;36mConfig gradient_accumulation_steps: value=2, type=<class 'int'>\u001b[0m\n",
      "2025-06-16 13:54:25 - src.tasks.clm_training.fabric.base - \u001b[0;36mDEBUG\u001b[0m - \u001b[0;36mConfig validate_after_k_steps: value=1000, type=<class 'int'>\u001b[0m\n",
      "[\u001b[0;36mDEBUG\u001b[0m | src.tasks.clm_training.fabric.base]: \u001b[0;36mConfig validate_after_k_steps: value=1000, type=<class 'int'>\u001b[0m\n",
      "2025-06-16 13:54:25 - src.tasks.clm_training.fabric.base - \u001b[0;36mDEBUG\u001b[0m - \u001b[0;36mConfig batch_size: value=8, type=<class 'int'>\u001b[0m\n",
      "[\u001b[0;36mDEBUG\u001b[0m | src.tasks.clm_training.fabric.base]: \u001b[0;36mConfig batch_size: value=8, type=<class 'int'>\u001b[0m\n",
      "2025-06-16 13:54:25 - src.tasks.clm_training.fabric.base - \u001b[0;32mINFO\u001b[0m - \u001b[0;32mSetting up FSDP strategy.\u001b[0m\n",
      "[\u001b[0;32mINFO\u001b[0m | src.tasks.clm_training.fabric.base]: \u001b[0;32mSetting up FSDP strategy.\u001b[0m\n",
      "2025-06-16 13:54:25 - src.tasks.clm_training.fabric.base - \u001b[0;32mINFO\u001b[0m - \u001b[0;32mUsing Devices: 1\u001b[0m\n",
      "[\u001b[0;32mINFO\u001b[0m | src.tasks.clm_training.fabric.base]: \u001b[0;32mUsing Devices: 1\u001b[0m\n",
      "2025-06-16 13:54:25 - src.tasks.clm_training.fabric.base - \u001b[0;33mWARNING\u001b[0m - \u001b[0;33mUsing automatic strategy for 1 device.\u001b[0m\n",
      "[\u001b[0;33mWARNING\u001b[0m | src.tasks.clm_training.fabric.base]: \u001b[0;33mUsing automatic strategy for 1 device.\u001b[0m\n",
      "2025-06-16 13:54:25 - src.tasks.clm_training.fabric.base - \u001b[0;36mDEBUG\u001b[0m - \u001b[0;36m{'key': 'eval_batch_size', 'value': 8, 'strategy': 'auto'}\u001b[0m\n",
      "[\u001b[0;36mDEBUG\u001b[0m | src.tasks.clm_training.fabric.base]: \u001b[0;36m{'key': 'eval_batch_size', 'value': 8, 'strategy': 'auto'}\u001b[0m\n",
      "/usr/local/lib/python3.12/dist-packages/huggingface_hub/file_download.py:943: FutureWarning: `resume_download` is deprecated and will be removed in version 1.0.0. Downloads always resume when possible. If you want to force a new download, use `force_download=True`.\n",
      "  warnings.warn(\n",
      "2025-06-16 13:54:26 - src.tasks.clm_training.fabric.base - \u001b[0;32mINFO\u001b[0m - \u001b[0;32mTime to instantiate model: 0.51 seconds.\u001b[0m\n",
      "[\u001b[0;32mINFO\u001b[0m | src.tasks.clm_training.fabric.base]: \u001b[0;32mTime to instantiate model: 0.51 seconds.\u001b[0m\n",
      "2025-06-16 13:54:26 - src.tasks.clm_training.fabric.base - \u001b[0;36mDEBUG\u001b[0m - \u001b[0;36mRunning Epoch 1 of 1\u001b[0m\n",
      "[\u001b[0;36mDEBUG\u001b[0m | src.tasks.clm_training.fabric.base]: \u001b[0;36mRunning Epoch 1 of 1\u001b[0m\n",
      "  0%|\u001b[34m                                                  \u001b[0m| 0/4682 [00:00<?, ?it/s]\u001b[0m2025-06-16 13:54:26 - src.tasks.clm_training.fabric.base - \u001b[0;36mDEBUG\u001b[0m - \u001b[0;36mgradient_accumulation_steps raw value: 2, type: <class 'int'>\u001b[0m\n",
      "[\u001b[0;36mDEBUG\u001b[0m | src.tasks.clm_training.fabric.base]: \u001b[0;36mgradient_accumulation_steps raw value: 2, type: <class 'int'>\u001b[0m\n",
      "2025-06-16 13:54:26 - src.tasks.clm_training.fabric.base - \u001b[0;36mDEBUG\u001b[0m - \u001b[0;36mgradient_accumulation_steps converted: 2, type: <class 'int'>\u001b[0m\n",
      "[\u001b[0;36mDEBUG\u001b[0m | src.tasks.clm_training.fabric.base]: \u001b[0;36mgradient_accumulation_steps converted: 2, type: <class 'int'>\u001b[0m\n",
      "2025-06-16 13:54:26 - src.tasks.clm_training.fabric.base - \u001b[0;36mDEBUG\u001b[0m - \u001b[0;36miter_num value: 0, type: <class 'int'>\u001b[0m\n",
      "[\u001b[0;36mDEBUG\u001b[0m | src.tasks.clm_training.fabric.base]: \u001b[0;36miter_num value: 0, type: <class 'int'>\u001b[0m\n",
      "2025-06-16 13:54:26 - src.tasks.clm_training.fabric.base - \u001b[0;36mDEBUG\u001b[0m - \u001b[0;36mAbout to check: (iter_num + 1) % gradient_accumulation_steps != 0 -> (0 + 1) % 2 != 0\u001b[0m\n",
      "[\u001b[0;36mDEBUG\u001b[0m | src.tasks.clm_training.fabric.base]: \u001b[0;36mAbout to check: (iter_num + 1) % gradient_accumulation_steps != 0 -> (0 + 1) % 2 != 0\u001b[0m\n",
      "2025-06-16 13:54:26 - src.tasks.clm_training.fabric.base - \u001b[0;36mDEBUG\u001b[0m - \u001b[0;36miter 1 step 0: loss 5.7188, iter time: 583.31ms remaining time: \u001b[0m\n",
      "[\u001b[0;36mDEBUG\u001b[0m | src.tasks.clm_training.fabric.base]: \u001b[0;36miter 1 step 0: loss 5.7188, iter time: 583.31ms remaining time: \u001b[0m\n",
      "  0%|\u001b[34m                                          \u001b[0m| 1/4682 [00:00<51:09,  1.52it/s]\u001b[0m2025-06-16 13:54:26 - src.tasks.clm_training.fabric.base - \u001b[0;36mDEBUG\u001b[0m - \u001b[0;36mgradient_accumulation_steps raw value: 2, type: <class 'int'>\u001b[0m\n",
      "[\u001b[0;36mDEBUG\u001b[0m | src.tasks.clm_training.fabric.base]: \u001b[0;36mgradient_accumulation_steps raw value: 2, type: <class 'int'>\u001b[0m\n",
      "2025-06-16 13:54:26 - src.tasks.clm_training.fabric.base - \u001b[0;36mDEBUG\u001b[0m - \u001b[0;36mgradient_accumulation_steps converted: 2, type: <class 'int'>\u001b[0m\n",
      "[\u001b[0;36mDEBUG\u001b[0m | src.tasks.clm_training.fabric.base]: \u001b[0;36mgradient_accumulation_steps converted: 2, type: <class 'int'>\u001b[0m\n",
      "2025-06-16 13:54:26 - src.tasks.clm_training.fabric.base - \u001b[0;36mDEBUG\u001b[0m - \u001b[0;36miter_num value: 1, type: <class 'int'>\u001b[0m\n",
      "[\u001b[0;36mDEBUG\u001b[0m | src.tasks.clm_training.fabric.base]: \u001b[0;36miter_num value: 1, type: <class 'int'>\u001b[0m\n",
      "2025-06-16 13:54:26 - src.tasks.clm_training.fabric.base - \u001b[0;36mDEBUG\u001b[0m - \u001b[0;36mAbout to check: (iter_num + 1) % gradient_accumulation_steps != 0 -> (1 + 1) % 2 != 0\u001b[0m\n",
      "[\u001b[0;36mDEBUG\u001b[0m | src.tasks.clm_training.fabric.base]: \u001b[0;36mAbout to check: (iter_num + 1) % gradient_accumulation_steps != 0 -> (1 + 1) % 2 != 0\u001b[0m\n",
      "2025-06-16 13:54:27 - src.tasks.clm_training.fabric.base - \u001b[0;36mDEBUG\u001b[0m - \u001b[0;36mGradient norm before clipping: 24.7500\u001b[0m\n",
      "[\u001b[0;36mDEBUG\u001b[0m | src.tasks.clm_training.fabric.base]: \u001b[0;36mGradient norm before clipping: 24.7500\u001b[0m\n",
      "2025-06-16 13:54:27 - src.tasks.clm_training.fabric.base - \u001b[0;36mDEBUG\u001b[0m - \u001b[0;36mvalidate_after_k_steps value: 1000, type: <class 'int'>\u001b[0m\n",
      "[\u001b[0;36mDEBUG\u001b[0m | src.tasks.clm_training.fabric.base]: \u001b[0;36mvalidate_after_k_steps value: 1000, type: <class 'int'>\u001b[0m\n",
      "2025-06-16 13:54:27 - src.tasks.clm_training.fabric.base - \u001b[0;36mDEBUG\u001b[0m - \u001b[0;36mstep_count value: 1, type: <class 'int'>\u001b[0m\n",
      "[\u001b[0;36mDEBUG\u001b[0m | src.tasks.clm_training.fabric.base]: \u001b[0;36mstep_count value: 1, type: <class 'int'>\u001b[0m\n",
      "2025-06-16 13:54:27 - src.tasks.clm_training.fabric.base - \u001b[0;36mDEBUG\u001b[0m - \u001b[0;36mValidation condition check - epochFinished: False, trainingFinished: False\u001b[0m\n",
      "[\u001b[0;36mDEBUG\u001b[0m | src.tasks.clm_training.fabric.base]: \u001b[0;36mValidation condition check - epochFinished: False, trainingFinished: False\u001b[0m\n",
      "2025-06-16 13:54:27 - src.tasks.clm_training.fabric.base - \u001b[0;36mDEBUG\u001b[0m - \u001b[0;36mAbout to check: step_count % validate_after_k_steps == 0 -> 1 % 1000 == 0\u001b[0m\n",
      "[\u001b[0;36mDEBUG\u001b[0m | src.tasks.clm_training.fabric.base]: \u001b[0;36mAbout to check: step_count % validate_after_k_steps == 0 -> 1 % 1000 == 0\u001b[0m\n",
      "2025-06-16 13:54:27 - src.tasks.clm_training.fabric.base - \u001b[0;36mDEBUG\u001b[0m - \u001b[0;36miter 2 step 1: loss 5.7188, iter time: 503.08ms remaining time: \u001b[0m\n",
      "[\u001b[0;36mDEBUG\u001b[0m | src.tasks.clm_training.fabric.base]: \u001b[0;36miter 2 step 1: loss 5.7188, iter time: 503.08ms remaining time: \u001b[0m\n",
      "  0%|\u001b[34m                                          \u001b[0m| 2/4682 [00:01<44:18,  1.76it/s]\u001b[0m2025-06-16 13:54:27 - src.tasks.clm_training.fabric.base - \u001b[0;36mDEBUG\u001b[0m - \u001b[0;36mgradient_accumulation_steps raw value: 2, type: <class 'int'>\u001b[0m\n",
      "[\u001b[0;36mDEBUG\u001b[0m | src.tasks.clm_training.fabric.base]: \u001b[0;36mgradient_accumulation_steps raw value: 2, type: <class 'int'>\u001b[0m\n",
      "2025-06-16 13:54:27 - src.tasks.clm_training.fabric.base - \u001b[0;36mDEBUG\u001b[0m - \u001b[0;36mgradient_accumulation_steps converted: 2, type: <class 'int'>\u001b[0m\n",
      "[\u001b[0;36mDEBUG\u001b[0m | src.tasks.clm_training.fabric.base]: \u001b[0;36mgradient_accumulation_steps converted: 2, type: <class 'int'>\u001b[0m\n",
      "2025-06-16 13:54:27 - src.tasks.clm_training.fabric.base - \u001b[0;36mDEBUG\u001b[0m - \u001b[0;36miter_num value: 2, type: <class 'int'>\u001b[0m\n",
      "[\u001b[0;36mDEBUG\u001b[0m | src.tasks.clm_training.fabric.base]: \u001b[0;36miter_num value: 2, type: <class 'int'>\u001b[0m\n",
      "2025-06-16 13:54:27 - src.tasks.clm_training.fabric.base - \u001b[0;36mDEBUG\u001b[0m - \u001b[0;36mAbout to check: (iter_num + 1) % gradient_accumulation_steps != 0 -> (2 + 1) % 2 != 0\u001b[0m\n",
      "[\u001b[0;36mDEBUG\u001b[0m | src.tasks.clm_training.fabric.base]: \u001b[0;36mAbout to check: (iter_num + 1) % gradient_accumulation_steps != 0 -> (2 + 1) % 2 != 0\u001b[0m\n",
      "2025-06-16 13:54:27 - src.tasks.clm_training.fabric.base - \u001b[0;36mDEBUG\u001b[0m - \u001b[0;36miter 3 step 1: loss 5.6875, iter time: 121.21ms remaining time: \u001b[0m\n",
      "[\u001b[0;36mDEBUG\u001b[0m | src.tasks.clm_training.fabric.base]: \u001b[0;36miter 3 step 1: loss 5.6875, iter time: 121.21ms remaining time: \u001b[0m\n",
      "  0%|\u001b[34m                                          \u001b[0m| 3/4682 [00:01<38:48,  2.01it/s]\u001b[0m2025-06-16 13:54:27 - src.tasks.clm_training.fabric.base - \u001b[0;36mDEBUG\u001b[0m - \u001b[0;36mgradient_accumulation_steps raw value: 2, type: <class 'int'>\u001b[0m\n",
      "[\u001b[0;36mDEBUG\u001b[0m | src.tasks.clm_training.fabric.base]: \u001b[0;36mgradient_accumulation_steps raw value: 2, type: <class 'int'>\u001b[0m\n",
      "2025-06-16 13:54:27 - src.tasks.clm_training.fabric.base - \u001b[0;36mDEBUG\u001b[0m - \u001b[0;36mgradient_accumulation_steps converted: 2, type: <class 'int'>\u001b[0m\n",
      "[\u001b[0;36mDEBUG\u001b[0m | src.tasks.clm_training.fabric.base]: \u001b[0;36mgradient_accumulation_steps converted: 2, type: <class 'int'>\u001b[0m\n",
      "2025-06-16 13:54:27 - src.tasks.clm_training.fabric.base - \u001b[0;36mDEBUG\u001b[0m - \u001b[0;36miter_num value: 3, type: <class 'int'>\u001b[0m\n",
      "[\u001b[0;36mDEBUG\u001b[0m | src.tasks.clm_training.fabric.base]: \u001b[0;36miter_num value: 3, type: <class 'int'>\u001b[0m\n",
      "2025-06-16 13:54:27 - src.tasks.clm_training.fabric.base - \u001b[0;36mDEBUG\u001b[0m - \u001b[0;36mAbout to check: (iter_num + 1) % gradient_accumulation_steps != 0 -> (3 + 1) % 2 != 0\u001b[0m\n",
      "[\u001b[0;36mDEBUG\u001b[0m | src.tasks.clm_training.fabric.base]: \u001b[0;36mAbout to check: (iter_num + 1) % gradient_accumulation_steps != 0 -> (3 + 1) % 2 != 0\u001b[0m\n",
      "2025-06-16 13:54:28 - src.tasks.clm_training.fabric.base - \u001b[0;36mDEBUG\u001b[0m - \u001b[0;36mGradient norm before clipping: 24.0000\u001b[0m\n",
      "[\u001b[0;36mDEBUG\u001b[0m | src.tasks.clm_training.fabric.base]: \u001b[0;36mGradient norm before clipping: 24.0000\u001b[0m\n",
      "2025-06-16 13:54:28 - src.tasks.clm_training.fabric.base - \u001b[0;36mDEBUG\u001b[0m - \u001b[0;36mvalidate_after_k_steps value: 1000, type: <class 'int'>\u001b[0m\n",
      "[\u001b[0;36mDEBUG\u001b[0m | src.tasks.clm_training.fabric.base]: \u001b[0;36mvalidate_after_k_steps value: 1000, type: <class 'int'>\u001b[0m\n",
      "2025-06-16 13:54:28 - src.tasks.clm_training.fabric.base - \u001b[0;36mDEBUG\u001b[0m - \u001b[0;36mstep_count value: 2, type: <class 'int'>\u001b[0m\n",
      "[\u001b[0;36mDEBUG\u001b[0m | src.tasks.clm_training.fabric.base]: \u001b[0;36mstep_count value: 2, type: <class 'int'>\u001b[0m\n",
      "2025-06-16 13:54:28 - src.tasks.clm_training.fabric.base - \u001b[0;36mDEBUG\u001b[0m - \u001b[0;36mValidation condition check - epochFinished: False, trainingFinished: False\u001b[0m\n",
      "[\u001b[0;36mDEBUG\u001b[0m | src.tasks.clm_training.fabric.base]: \u001b[0;36mValidation condition check - epochFinished: False, trainingFinished: False\u001b[0m\n",
      "2025-06-16 13:54:28 - src.tasks.clm_training.fabric.base - \u001b[0;36mDEBUG\u001b[0m - \u001b[0;36mAbout to check: step_count % validate_after_k_steps == 0 -> 2 % 1000 == 0\u001b[0m\n",
      "[\u001b[0;36mDEBUG\u001b[0m | src.tasks.clm_training.fabric.base]: \u001b[0;36mAbout to check: step_count % validate_after_k_steps == 0 -> 2 % 1000 == 0\u001b[0m\n",
      "2025-06-16 13:54:28 - src.tasks.clm_training.fabric.base - \u001b[0;36mDEBUG\u001b[0m - \u001b[0;36miter 4 step 2: loss 5.8750, iter time: 417.48ms remaining time: \u001b[0m\n",
      "[\u001b[0;36mDEBUG\u001b[0m | src.tasks.clm_training.fabric.base]: \u001b[0;36miter 4 step 2: loss 5.8750, iter time: 417.48ms remaining time: \u001b[0m\n",
      "  0%|\u001b[34m                                          \u001b[0m| 4/4682 [00:02<36:38,  2.13it/s]\u001b[0m2025-06-16 13:54:28 - src.tasks.clm_training.fabric.base - \u001b[0;36mDEBUG\u001b[0m - \u001b[0;36mgradient_accumulation_steps raw value: 2, type: <class 'int'>\u001b[0m\n",
      "[\u001b[0;36mDEBUG\u001b[0m | src.tasks.clm_training.fabric.base]: \u001b[0;36mgradient_accumulation_steps raw value: 2, type: <class 'int'>\u001b[0m\n",
      "2025-06-16 13:54:28 - src.tasks.clm_training.fabric.base - \u001b[0;36mDEBUG\u001b[0m - \u001b[0;36mgradient_accumulation_steps converted: 2, type: <class 'int'>\u001b[0m\n",
      "[\u001b[0;36mDEBUG\u001b[0m | src.tasks.clm_training.fabric.base]: \u001b[0;36mgradient_accumulation_steps converted: 2, type: <class 'int'>\u001b[0m\n",
      "2025-06-16 13:54:28 - src.tasks.clm_training.fabric.base - \u001b[0;36mDEBUG\u001b[0m - \u001b[0;36miter_num value: 4, type: <class 'int'>\u001b[0m\n",
      "[\u001b[0;36mDEBUG\u001b[0m | src.tasks.clm_training.fabric.base]: \u001b[0;36miter_num value: 4, type: <class 'int'>\u001b[0m\n",
      "2025-06-16 13:54:28 - src.tasks.clm_training.fabric.base - \u001b[0;36mDEBUG\u001b[0m - \u001b[0;36mAbout to check: (iter_num + 1) % gradient_accumulation_steps != 0 -> (4 + 1) % 2 != 0\u001b[0m\n",
      "[\u001b[0;36mDEBUG\u001b[0m | src.tasks.clm_training.fabric.base]: \u001b[0;36mAbout to check: (iter_num + 1) % gradient_accumulation_steps != 0 -> (4 + 1) % 2 != 0\u001b[0m\n",
      "2025-06-16 13:54:28 - src.tasks.clm_training.fabric.base - \u001b[0;36mDEBUG\u001b[0m - \u001b[0;36miter 5 step 2: loss 5.6562, iter time: 121.25ms remaining time: \u001b[0m\n",
      "[\u001b[0;36mDEBUG\u001b[0m | src.tasks.clm_training.fabric.base]: \u001b[0;36miter 5 step 2: loss 5.6562, iter time: 121.25ms remaining time: \u001b[0m\n",
      "  0%|\u001b[34m                                          \u001b[0m| 5/4682 [00:02<35:04,  2.22it/s]\u001b[0m2025-06-16 13:54:28 - src.tasks.clm_training.fabric.base - \u001b[0;36mDEBUG\u001b[0m - \u001b[0;36mgradient_accumulation_steps raw value: 2, type: <class 'int'>\u001b[0m\n",
      "[\u001b[0;36mDEBUG\u001b[0m | src.tasks.clm_training.fabric.base]: \u001b[0;36mgradient_accumulation_steps raw value: 2, type: <class 'int'>\u001b[0m\n",
      "2025-06-16 13:54:28 - src.tasks.clm_training.fabric.base - \u001b[0;36mDEBUG\u001b[0m - \u001b[0;36mgradient_accumulation_steps converted: 2, type: <class 'int'>\u001b[0m\n",
      "[\u001b[0;36mDEBUG\u001b[0m | src.tasks.clm_training.fabric.base]: \u001b[0;36mgradient_accumulation_steps converted: 2, type: <class 'int'>\u001b[0m\n",
      "2025-06-16 13:54:28 - src.tasks.clm_training.fabric.base - \u001b[0;36mDEBUG\u001b[0m - \u001b[0;36miter_num value: 5, type: <class 'int'>\u001b[0m\n",
      "[\u001b[0;36mDEBUG\u001b[0m | src.tasks.clm_training.fabric.base]: \u001b[0;36miter_num value: 5, type: <class 'int'>\u001b[0m\n",
      "2025-06-16 13:54:28 - src.tasks.clm_training.fabric.base - \u001b[0;36mDEBUG\u001b[0m - \u001b[0;36mAbout to check: (iter_num + 1) % gradient_accumulation_steps != 0 -> (5 + 1) % 2 != 0\u001b[0m\n",
      "[\u001b[0;36mDEBUG\u001b[0m | src.tasks.clm_training.fabric.base]: \u001b[0;36mAbout to check: (iter_num + 1) % gradient_accumulation_steps != 0 -> (5 + 1) % 2 != 0\u001b[0m\n",
      "2025-06-16 13:54:28 - src.tasks.clm_training.fabric.base - \u001b[0;36mDEBUG\u001b[0m - \u001b[0;36mGradient norm before clipping: 24.2500\u001b[0m\n",
      "[\u001b[0;36mDEBUG\u001b[0m | src.tasks.clm_training.fabric.base]: \u001b[0;36mGradient norm before clipping: 24.2500\u001b[0m\n",
      "2025-06-16 13:54:28 - src.tasks.clm_training.fabric.base - \u001b[0;36mDEBUG\u001b[0m - \u001b[0;36mvalidate_after_k_steps value: 1000, type: <class 'int'>\u001b[0m\n",
      "[\u001b[0;36mDEBUG\u001b[0m | src.tasks.clm_training.fabric.base]: \u001b[0;36mvalidate_after_k_steps value: 1000, type: <class 'int'>\u001b[0m\n",
      "2025-06-16 13:54:28 - src.tasks.clm_training.fabric.base - \u001b[0;36mDEBUG\u001b[0m - \u001b[0;36mstep_count value: 3, type: <class 'int'>\u001b[0m\n",
      "[\u001b[0;36mDEBUG\u001b[0m | src.tasks.clm_training.fabric.base]: \u001b[0;36mstep_count value: 3, type: <class 'int'>\u001b[0m\n",
      "2025-06-16 13:54:28 - src.tasks.clm_training.fabric.base - \u001b[0;36mDEBUG\u001b[0m - \u001b[0;36mValidation condition check - epochFinished: False, trainingFinished: False\u001b[0m\n",
      "[\u001b[0;36mDEBUG\u001b[0m | src.tasks.clm_training.fabric.base]: \u001b[0;36mValidation condition check - epochFinished: False, trainingFinished: False\u001b[0m\n",
      "2025-06-16 13:54:28 - src.tasks.clm_training.fabric.base - \u001b[0;36mDEBUG\u001b[0m - \u001b[0;36mAbout to check: step_count % validate_after_k_steps == 0 -> 3 % 1000 == 0\u001b[0m\n",
      "[\u001b[0;36mDEBUG\u001b[0m | src.tasks.clm_training.fabric.base]: \u001b[0;36mAbout to check: step_count % validate_after_k_steps == 0 -> 3 % 1000 == 0\u001b[0m\n",
      "2025-06-16 13:54:28 - src.tasks.clm_training.fabric.base - \u001b[0;36mDEBUG\u001b[0m - \u001b[0;36miter 6 step 3: loss 5.7500, iter time: 417.41ms remaining time: \u001b[0m\n",
      "[\u001b[0;36mDEBUG\u001b[0m | src.tasks.clm_training.fabric.base]: \u001b[0;36miter 6 step 3: loss 5.7500, iter time: 417.41ms remaining time: \u001b[0m\n",
      "  0%|\u001b[34m                                          \u001b[0m| 6/4682 [00:02<34:27,  2.26it/s]\u001b[0m2025-06-16 13:54:28 - src.tasks.clm_training.fabric.base - \u001b[0;36mDEBUG\u001b[0m - \u001b[0;36mgradient_accumulation_steps raw value: 2, type: <class 'int'>\u001b[0m\n",
      "[\u001b[0;36mDEBUG\u001b[0m | src.tasks.clm_training.fabric.base]: \u001b[0;36mgradient_accumulation_steps raw value: 2, type: <class 'int'>\u001b[0m\n",
      "2025-06-16 13:54:28 - src.tasks.clm_training.fabric.base - \u001b[0;36mDEBUG\u001b[0m - \u001b[0;36mgradient_accumulation_steps converted: 2, type: <class 'int'>\u001b[0m\n",
      "[\u001b[0;36mDEBUG\u001b[0m | src.tasks.clm_training.fabric.base]: \u001b[0;36mgradient_accumulation_steps converted: 2, type: <class 'int'>\u001b[0m\n",
      "2025-06-16 13:54:28 - src.tasks.clm_training.fabric.base - \u001b[0;36mDEBUG\u001b[0m - \u001b[0;36miter_num value: 6, type: <class 'int'>\u001b[0m\n",
      "[\u001b[0;36mDEBUG\u001b[0m | src.tasks.clm_training.fabric.base]: \u001b[0;36miter_num value: 6, type: <class 'int'>\u001b[0m\n",
      "2025-06-16 13:54:28 - src.tasks.clm_training.fabric.base - \u001b[0;36mDEBUG\u001b[0m - \u001b[0;36mAbout to check: (iter_num + 1) % gradient_accumulation_steps != 0 -> (6 + 1) % 2 != 0\u001b[0m\n",
      "[\u001b[0;36mDEBUG\u001b[0m | src.tasks.clm_training.fabric.base]: \u001b[0;36mAbout to check: (iter_num + 1) % gradient_accumulation_steps != 0 -> (6 + 1) % 2 != 0\u001b[0m\n",
      "2025-06-16 13:54:29 - src.tasks.clm_training.fabric.base - \u001b[0;36mDEBUG\u001b[0m - \u001b[0;36miter 7 step 3: loss 5.5000, iter time: 121.52ms remaining time: \u001b[0m\n",
      "[\u001b[0;36mDEBUG\u001b[0m | src.tasks.clm_training.fabric.base]: \u001b[0;36miter 7 step 3: loss 5.5000, iter time: 121.52ms remaining time: \u001b[0m\n",
      "  0%|\u001b[34m                                          \u001b[0m| 7/4682 [00:03<33:44,  2.31it/s]\u001b[0m2025-06-16 13:54:29 - src.tasks.clm_training.fabric.base - \u001b[0;36mDEBUG\u001b[0m - \u001b[0;36mgradient_accumulation_steps raw value: 2, type: <class 'int'>\u001b[0m\n",
      "[\u001b[0;36mDEBUG\u001b[0m | src.tasks.clm_training.fabric.base]: \u001b[0;36mgradient_accumulation_steps raw value: 2, type: <class 'int'>\u001b[0m\n",
      "2025-06-16 13:54:29 - src.tasks.clm_training.fabric.base - \u001b[0;36mDEBUG\u001b[0m - \u001b[0;36mgradient_accumulation_steps converted: 2, type: <class 'int'>\u001b[0m\n",
      "[\u001b[0;36mDEBUG\u001b[0m | src.tasks.clm_training.fabric.base]: \u001b[0;36mgradient_accumulation_steps converted: 2, type: <class 'int'>\u001b[0m\n",
      "2025-06-16 13:54:29 - src.tasks.clm_training.fabric.base - \u001b[0;36mDEBUG\u001b[0m - \u001b[0;36miter_num value: 7, type: <class 'int'>\u001b[0m\n",
      "[\u001b[0;36mDEBUG\u001b[0m | src.tasks.clm_training.fabric.base]: \u001b[0;36miter_num value: 7, type: <class 'int'>\u001b[0m\n",
      "2025-06-16 13:54:29 - src.tasks.clm_training.fabric.base - \u001b[0;36mDEBUG\u001b[0m - \u001b[0;36mAbout to check: (iter_num + 1) % gradient_accumulation_steps != 0 -> (7 + 1) % 2 != 0\u001b[0m\n",
      "[\u001b[0;36mDEBUG\u001b[0m | src.tasks.clm_training.fabric.base]: \u001b[0;36mAbout to check: (iter_num + 1) % gradient_accumulation_steps != 0 -> (7 + 1) % 2 != 0\u001b[0m\n",
      "2025-06-16 13:54:29 - src.tasks.clm_training.fabric.base - \u001b[0;36mDEBUG\u001b[0m - \u001b[0;36mGradient norm before clipping: 24.0000\u001b[0m\n",
      "[\u001b[0;36mDEBUG\u001b[0m | src.tasks.clm_training.fabric.base]: \u001b[0;36mGradient norm before clipping: 24.0000\u001b[0m\n",
      "2025-06-16 13:54:29 - src.tasks.clm_training.fabric.base - \u001b[0;36mDEBUG\u001b[0m - \u001b[0;36mvalidate_after_k_steps value: 1000, type: <class 'int'>\u001b[0m\n",
      "[\u001b[0;36mDEBUG\u001b[0m | src.tasks.clm_training.fabric.base]: \u001b[0;36mvalidate_after_k_steps value: 1000, type: <class 'int'>\u001b[0m\n",
      "2025-06-16 13:54:29 - src.tasks.clm_training.fabric.base - \u001b[0;36mDEBUG\u001b[0m - \u001b[0;36mstep_count value: 4, type: <class 'int'>\u001b[0m\n",
      "[\u001b[0;36mDEBUG\u001b[0m | src.tasks.clm_training.fabric.base]: \u001b[0;36mstep_count value: 4, type: <class 'int'>\u001b[0m\n",
      "2025-06-16 13:54:29 - src.tasks.clm_training.fabric.base - \u001b[0;36mDEBUG\u001b[0m - \u001b[0;36mValidation condition check - epochFinished: False, trainingFinished: False\u001b[0m\n",
      "[\u001b[0;36mDEBUG\u001b[0m | src.tasks.clm_training.fabric.base]: \u001b[0;36mValidation condition check - epochFinished: False, trainingFinished: False\u001b[0m\n",
      "2025-06-16 13:54:29 - src.tasks.clm_training.fabric.base - \u001b[0;36mDEBUG\u001b[0m - \u001b[0;36mAbout to check: step_count % validate_after_k_steps == 0 -> 4 % 1000 == 0\u001b[0m\n",
      "[\u001b[0;36mDEBUG\u001b[0m | src.tasks.clm_training.fabric.base]: \u001b[0;36mAbout to check: step_count % validate_after_k_steps == 0 -> 4 % 1000 == 0\u001b[0m\n",
      "2025-06-16 13:54:29 - src.tasks.clm_training.fabric.base - \u001b[0;36mDEBUG\u001b[0m - \u001b[0;36miter 8 step 4: loss 5.5000, iter time: 419.14ms remaining time: \u001b[0m\n",
      "[\u001b[0;36mDEBUG\u001b[0m | src.tasks.clm_training.fabric.base]: \u001b[0;36miter 8 step 4: loss 5.5000, iter time: 419.14ms remaining time: \u001b[0m\n",
      "  0%|\u001b[34m                                          \u001b[0m| 8/4682 [00:03<33:38,  2.32it/s]\u001b[0m2025-06-16 13:54:29 - src.tasks.clm_training.fabric.base - \u001b[0;36mDEBUG\u001b[0m - \u001b[0;36mgradient_accumulation_steps raw value: 2, type: <class 'int'>\u001b[0m\n",
      "[\u001b[0;36mDEBUG\u001b[0m | src.tasks.clm_training.fabric.base]: \u001b[0;36mgradient_accumulation_steps raw value: 2, type: <class 'int'>\u001b[0m\n",
      "2025-06-16 13:54:29 - src.tasks.clm_training.fabric.base - \u001b[0;36mDEBUG\u001b[0m - \u001b[0;36mgradient_accumulation_steps converted: 2, type: <class 'int'>\u001b[0m\n",
      "[\u001b[0;36mDEBUG\u001b[0m | src.tasks.clm_training.fabric.base]: \u001b[0;36mgradient_accumulation_steps converted: 2, type: <class 'int'>\u001b[0m\n",
      "2025-06-16 13:54:29 - src.tasks.clm_training.fabric.base - \u001b[0;36mDEBUG\u001b[0m - \u001b[0;36miter_num value: 8, type: <class 'int'>\u001b[0m\n",
      "[\u001b[0;36mDEBUG\u001b[0m | src.tasks.clm_training.fabric.base]: \u001b[0;36miter_num value: 8, type: <class 'int'>\u001b[0m\n",
      "2025-06-16 13:54:29 - src.tasks.clm_training.fabric.base - \u001b[0;36mDEBUG\u001b[0m - \u001b[0;36mAbout to check: (iter_num + 1) % gradient_accumulation_steps != 0 -> (8 + 1) % 2 != 0\u001b[0m\n",
      "[\u001b[0;36mDEBUG\u001b[0m | src.tasks.clm_training.fabric.base]: \u001b[0;36mAbout to check: (iter_num + 1) % gradient_accumulation_steps != 0 -> (8 + 1) % 2 != 0\u001b[0m\n",
      "2025-06-16 13:54:30 - src.tasks.clm_training.fabric.base - \u001b[0;36mDEBUG\u001b[0m - \u001b[0;36miter 9 step 4: loss 5.8438, iter time: 121.19ms remaining time: \u001b[0m\n",
      "[\u001b[0;36mDEBUG\u001b[0m | src.tasks.clm_training.fabric.base]: \u001b[0;36miter 9 step 4: loss 5.8438, iter time: 121.19ms remaining time: \u001b[0m\n",
      "  0%|\u001b[34m                                          \u001b[0m| 9/4682 [00:04<33:11,  2.35it/s]\u001b[0m2025-06-16 13:54:30 - src.tasks.clm_training.fabric.base - \u001b[0;36mDEBUG\u001b[0m - \u001b[0;36mgradient_accumulation_steps raw value: 2, type: <class 'int'>\u001b[0m\n",
      "[\u001b[0;36mDEBUG\u001b[0m | src.tasks.clm_training.fabric.base]: \u001b[0;36mgradient_accumulation_steps raw value: 2, type: <class 'int'>\u001b[0m\n",
      "2025-06-16 13:54:30 - src.tasks.clm_training.fabric.base - \u001b[0;36mDEBUG\u001b[0m - \u001b[0;36mgradient_accumulation_steps converted: 2, type: <class 'int'>\u001b[0m\n",
      "[\u001b[0;36mDEBUG\u001b[0m | src.tasks.clm_training.fabric.base]: \u001b[0;36mgradient_accumulation_steps converted: 2, type: <class 'int'>\u001b[0m\n",
      "2025-06-16 13:54:30 - src.tasks.clm_training.fabric.base - \u001b[0;36mDEBUG\u001b[0m - \u001b[0;36miter_num value: 9, type: <class 'int'>\u001b[0m\n",
      "[\u001b[0;36mDEBUG\u001b[0m | src.tasks.clm_training.fabric.base]: \u001b[0;36miter_num value: 9, type: <class 'int'>\u001b[0m\n",
      "2025-06-16 13:54:30 - src.tasks.clm_training.fabric.base - \u001b[0;36mDEBUG\u001b[0m - \u001b[0;36mAbout to check: (iter_num + 1) % gradient_accumulation_steps != 0 -> (9 + 1) % 2 != 0\u001b[0m\n",
      "[\u001b[0;36mDEBUG\u001b[0m | src.tasks.clm_training.fabric.base]: \u001b[0;36mAbout to check: (iter_num + 1) % gradient_accumulation_steps != 0 -> (9 + 1) % 2 != 0\u001b[0m\n",
      "2025-06-16 13:54:30 - src.tasks.clm_training.fabric.base - \u001b[0;36mDEBUG\u001b[0m - \u001b[0;36mGradient norm before clipping: 24.3750\u001b[0m\n",
      "[\u001b[0;36mDEBUG\u001b[0m | src.tasks.clm_training.fabric.base]: \u001b[0;36mGradient norm before clipping: 24.3750\u001b[0m\n",
      "2025-06-16 13:54:30 - src.tasks.clm_training.fabric.base - \u001b[0;36mDEBUG\u001b[0m - \u001b[0;36mvalidate_after_k_steps value: 1000, type: <class 'int'>\u001b[0m\n",
      "[\u001b[0;36mDEBUG\u001b[0m | src.tasks.clm_training.fabric.base]: \u001b[0;36mvalidate_after_k_steps value: 1000, type: <class 'int'>\u001b[0m\n",
      "2025-06-16 13:54:30 - src.tasks.clm_training.fabric.base - \u001b[0;36mDEBUG\u001b[0m - \u001b[0;36mstep_count value: 5, type: <class 'int'>\u001b[0m\n",
      "[\u001b[0;36mDEBUG\u001b[0m | src.tasks.clm_training.fabric.base]: \u001b[0;36mstep_count value: 5, type: <class 'int'>\u001b[0m\n",
      "2025-06-16 13:54:30 - src.tasks.clm_training.fabric.base - \u001b[0;36mDEBUG\u001b[0m - \u001b[0;36mValidation condition check - epochFinished: False, trainingFinished: False\u001b[0m\n",
      "[\u001b[0;36mDEBUG\u001b[0m | src.tasks.clm_training.fabric.base]: \u001b[0;36mValidation condition check - epochFinished: False, trainingFinished: False\u001b[0m\n",
      "2025-06-16 13:54:30 - src.tasks.clm_training.fabric.base - \u001b[0;36mDEBUG\u001b[0m - \u001b[0;36mAbout to check: step_count % validate_after_k_steps == 0 -> 5 % 1000 == 0\u001b[0m\n",
      "[\u001b[0;36mDEBUG\u001b[0m | src.tasks.clm_training.fabric.base]: \u001b[0;36mAbout to check: step_count % validate_after_k_steps == 0 -> 5 % 1000 == 0\u001b[0m\n",
      "2025-06-16 13:54:30 - src.tasks.clm_training.fabric.base - \u001b[0;36mDEBUG\u001b[0m - \u001b[0;36miter 10 step 5: loss 5.6875, iter time: 418.89ms remaining time: \u001b[0m\n",
      "[\u001b[0;36mDEBUG\u001b[0m | src.tasks.clm_training.fabric.base]: \u001b[0;36miter 10 step 5: loss 5.6875, iter time: 418.89ms remaining time: \u001b[0m\n",
      "  0%|\u001b[34m                                         \u001b[0m| 10/4682 [00:04<33:14,  2.34it/s]\u001b[0m2025-06-16 13:54:30 - src.tasks.clm_training.fabric.base - \u001b[0;36mDEBUG\u001b[0m - \u001b[0;36mgradient_accumulation_steps raw value: 2, type: <class 'int'>\u001b[0m\n",
      "[\u001b[0;36mDEBUG\u001b[0m | src.tasks.clm_training.fabric.base]: \u001b[0;36mgradient_accumulation_steps raw value: 2, type: <class 'int'>\u001b[0m\n",
      "2025-06-16 13:54:30 - src.tasks.clm_training.fabric.base - \u001b[0;36mDEBUG\u001b[0m - \u001b[0;36mgradient_accumulation_steps converted: 2, type: <class 'int'>\u001b[0m\n",
      "[\u001b[0;36mDEBUG\u001b[0m | src.tasks.clm_training.fabric.base]: \u001b[0;36mgradient_accumulation_steps converted: 2, type: <class 'int'>\u001b[0m\n",
      "2025-06-16 13:54:30 - src.tasks.clm_training.fabric.base - \u001b[0;36mDEBUG\u001b[0m - \u001b[0;36miter_num value: 10, type: <class 'int'>\u001b[0m\n",
      "[\u001b[0;36mDEBUG\u001b[0m | src.tasks.clm_training.fabric.base]: \u001b[0;36miter_num value: 10, type: <class 'int'>\u001b[0m\n",
      "2025-06-16 13:54:30 - src.tasks.clm_training.fabric.base - \u001b[0;36mDEBUG\u001b[0m - \u001b[0;36mAbout to check: (iter_num + 1) % gradient_accumulation_steps != 0 -> (10 + 1) % 2 != 0\u001b[0m\n",
      "[\u001b[0;36mDEBUG\u001b[0m | src.tasks.clm_training.fabric.base]: \u001b[0;36mAbout to check: (iter_num + 1) % gradient_accumulation_steps != 0 -> (10 + 1) % 2 != 0\u001b[0m\n",
      "2025-06-16 13:54:31 - src.tasks.clm_training.fabric.base - \u001b[0;36mDEBUG\u001b[0m - \u001b[0;36miter 11 step 5: loss 5.8125, iter time: 121.87ms remaining time: \u001b[0m\n",
      "[\u001b[0;36mDEBUG\u001b[0m | src.tasks.clm_training.fabric.base]: \u001b[0;36miter 11 step 5: loss 5.8125, iter time: 121.87ms remaining time: \u001b[0m\n",
      "  0%|\u001b[34m                                         \u001b[0m| 11/4682 [00:04<32:56,  2.36it/s]\u001b[0m2025-06-16 13:54:31 - src.tasks.clm_training.fabric.base - \u001b[0;36mDEBUG\u001b[0m - \u001b[0;36mgradient_accumulation_steps raw value: 2, type: <class 'int'>\u001b[0m\n",
      "[\u001b[0;36mDEBUG\u001b[0m | src.tasks.clm_training.fabric.base]: \u001b[0;36mgradient_accumulation_steps raw value: 2, type: <class 'int'>\u001b[0m\n",
      "2025-06-16 13:54:31 - src.tasks.clm_training.fabric.base - \u001b[0;36mDEBUG\u001b[0m - \u001b[0;36mgradient_accumulation_steps converted: 2, type: <class 'int'>\u001b[0m\n",
      "[\u001b[0;36mDEBUG\u001b[0m | src.tasks.clm_training.fabric.base]: \u001b[0;36mgradient_accumulation_steps converted: 2, type: <class 'int'>\u001b[0m\n",
      "2025-06-16 13:54:31 - src.tasks.clm_training.fabric.base - \u001b[0;36mDEBUG\u001b[0m - \u001b[0;36miter_num value: 11, type: <class 'int'>\u001b[0m\n",
      "[\u001b[0;36mDEBUG\u001b[0m | src.tasks.clm_training.fabric.base]: \u001b[0;36miter_num value: 11, type: <class 'int'>\u001b[0m\n",
      "2025-06-16 13:54:31 - src.tasks.clm_training.fabric.base - \u001b[0;36mDEBUG\u001b[0m - \u001b[0;36mAbout to check: (iter_num + 1) % gradient_accumulation_steps != 0 -> (11 + 1) % 2 != 0\u001b[0m\n",
      "[\u001b[0;36mDEBUG\u001b[0m | src.tasks.clm_training.fabric.base]: \u001b[0;36mAbout to check: (iter_num + 1) % gradient_accumulation_steps != 0 -> (11 + 1) % 2 != 0\u001b[0m\n",
      "2025-06-16 13:54:31 - src.tasks.clm_training.fabric.base - \u001b[0;36mDEBUG\u001b[0m - \u001b[0;36mGradient norm before clipping: 21.7500\u001b[0m\n",
      "[\u001b[0;36mDEBUG\u001b[0m | src.tasks.clm_training.fabric.base]: \u001b[0;36mGradient norm before clipping: 21.7500\u001b[0m\n",
      "2025-06-16 13:54:31 - src.tasks.clm_training.fabric.base - \u001b[0;36mDEBUG\u001b[0m - \u001b[0;36mvalidate_after_k_steps value: 1000, type: <class 'int'>\u001b[0m\n",
      "[\u001b[0;36mDEBUG\u001b[0m | src.tasks.clm_training.fabric.base]: \u001b[0;36mvalidate_after_k_steps value: 1000, type: <class 'int'>\u001b[0m\n",
      "2025-06-16 13:54:31 - src.tasks.clm_training.fabric.base - \u001b[0;36mDEBUG\u001b[0m - \u001b[0;36mstep_count value: 6, type: <class 'int'>\u001b[0m\n",
      "[\u001b[0;36mDEBUG\u001b[0m | src.tasks.clm_training.fabric.base]: \u001b[0;36mstep_count value: 6, type: <class 'int'>\u001b[0m\n",
      "2025-06-16 13:54:31 - src.tasks.clm_training.fabric.base - \u001b[0;36mDEBUG\u001b[0m - \u001b[0;36mValidation condition check - epochFinished: False, trainingFinished: False\u001b[0m\n",
      "[\u001b[0;36mDEBUG\u001b[0m | src.tasks.clm_training.fabric.base]: \u001b[0;36mValidation condition check - epochFinished: False, trainingFinished: False\u001b[0m\n",
      "2025-06-16 13:54:31 - src.tasks.clm_training.fabric.base - \u001b[0;36mDEBUG\u001b[0m - \u001b[0;36mAbout to check: step_count % validate_after_k_steps == 0 -> 6 % 1000 == 0\u001b[0m\n",
      "[\u001b[0;36mDEBUG\u001b[0m | src.tasks.clm_training.fabric.base]: \u001b[0;36mAbout to check: step_count % validate_after_k_steps == 0 -> 6 % 1000 == 0\u001b[0m\n",
      "2025-06-16 13:54:31 - src.tasks.clm_training.fabric.base - \u001b[0;36mDEBUG\u001b[0m - \u001b[0;36miter 12 step 6: loss 5.8125, iter time: 419.53ms remaining time: \u001b[0m\n",
      "[\u001b[0;36mDEBUG\u001b[0m | src.tasks.clm_training.fabric.base]: \u001b[0;36miter 12 step 6: loss 5.8125, iter time: 419.53ms remaining time: \u001b[0m\n",
      "  0%|\u001b[34m                                         \u001b[0m| 12/4682 [00:05<33:04,  2.35it/s]\u001b[0m2025-06-16 13:54:31 - src.tasks.clm_training.fabric.base - \u001b[0;36mDEBUG\u001b[0m - \u001b[0;36mgradient_accumulation_steps raw value: 2, type: <class 'int'>\u001b[0m\n",
      "[\u001b[0;36mDEBUG\u001b[0m | src.tasks.clm_training.fabric.base]: \u001b[0;36mgradient_accumulation_steps raw value: 2, type: <class 'int'>\u001b[0m\n",
      "2025-06-16 13:54:31 - src.tasks.clm_training.fabric.base - \u001b[0;36mDEBUG\u001b[0m - \u001b[0;36mgradient_accumulation_steps converted: 2, type: <class 'int'>\u001b[0m\n",
      "[\u001b[0;36mDEBUG\u001b[0m | src.tasks.clm_training.fabric.base]: \u001b[0;36mgradient_accumulation_steps converted: 2, type: <class 'int'>\u001b[0m\n",
      "2025-06-16 13:54:31 - src.tasks.clm_training.fabric.base - \u001b[0;36mDEBUG\u001b[0m - \u001b[0;36miter_num value: 12, type: <class 'int'>\u001b[0m\n",
      "[\u001b[0;36mDEBUG\u001b[0m | src.tasks.clm_training.fabric.base]: \u001b[0;36miter_num value: 12, type: <class 'int'>\u001b[0m\n",
      "2025-06-16 13:54:31 - src.tasks.clm_training.fabric.base - \u001b[0;36mDEBUG\u001b[0m - \u001b[0;36mAbout to check: (iter_num + 1) % gradient_accumulation_steps != 0 -> (12 + 1) % 2 != 0\u001b[0m\n",
      "[\u001b[0;36mDEBUG\u001b[0m | src.tasks.clm_training.fabric.base]: \u001b[0;36mAbout to check: (iter_num + 1) % gradient_accumulation_steps != 0 -> (12 + 1) % 2 != 0\u001b[0m\n",
      "2025-06-16 13:54:31 - src.tasks.clm_training.fabric.base - \u001b[0;36mDEBUG\u001b[0m - \u001b[0;36miter 13 step 6: loss 5.8438, iter time: 121.47ms remaining time: \u001b[0m\n",
      "[\u001b[0;36mDEBUG\u001b[0m | src.tasks.clm_training.fabric.base]: \u001b[0;36miter 13 step 6: loss 5.8438, iter time: 121.47ms remaining time: \u001b[0m\n",
      "  0%|\u001b[34m                                         \u001b[0m| 13/4682 [00:05<32:50,  2.37it/s]\u001b[0m2025-06-16 13:54:31 - src.tasks.clm_training.fabric.base - \u001b[0;36mDEBUG\u001b[0m - \u001b[0;36mgradient_accumulation_steps raw value: 2, type: <class 'int'>\u001b[0m\n",
      "[\u001b[0;36mDEBUG\u001b[0m | src.tasks.clm_training.fabric.base]: \u001b[0;36mgradient_accumulation_steps raw value: 2, type: <class 'int'>\u001b[0m\n",
      "2025-06-16 13:54:31 - src.tasks.clm_training.fabric.base - \u001b[0;36mDEBUG\u001b[0m - \u001b[0;36mgradient_accumulation_steps converted: 2, type: <class 'int'>\u001b[0m\n",
      "[\u001b[0;36mDEBUG\u001b[0m | src.tasks.clm_training.fabric.base]: \u001b[0;36mgradient_accumulation_steps converted: 2, type: <class 'int'>\u001b[0m\n",
      "2025-06-16 13:54:31 - src.tasks.clm_training.fabric.base - \u001b[0;36mDEBUG\u001b[0m - \u001b[0;36miter_num value: 13, type: <class 'int'>\u001b[0m\n",
      "[\u001b[0;36mDEBUG\u001b[0m | src.tasks.clm_training.fabric.base]: \u001b[0;36miter_num value: 13, type: <class 'int'>\u001b[0m\n",
      "2025-06-16 13:54:31 - src.tasks.clm_training.fabric.base - \u001b[0;36mDEBUG\u001b[0m - \u001b[0;36mAbout to check: (iter_num + 1) % gradient_accumulation_steps != 0 -> (13 + 1) % 2 != 0\u001b[0m\n",
      "[\u001b[0;36mDEBUG\u001b[0m | src.tasks.clm_training.fabric.base]: \u001b[0;36mAbout to check: (iter_num + 1) % gradient_accumulation_steps != 0 -> (13 + 1) % 2 != 0\u001b[0m\n",
      "2025-06-16 13:54:32 - src.tasks.clm_training.fabric.base - \u001b[0;36mDEBUG\u001b[0m - \u001b[0;36mGradient norm before clipping: 24.8750\u001b[0m\n",
      "[\u001b[0;36mDEBUG\u001b[0m | src.tasks.clm_training.fabric.base]: \u001b[0;36mGradient norm before clipping: 24.8750\u001b[0m\n",
      "2025-06-16 13:54:32 - src.tasks.clm_training.fabric.base - \u001b[0;36mDEBUG\u001b[0m - \u001b[0;36mvalidate_after_k_steps value: 1000, type: <class 'int'>\u001b[0m\n",
      "[\u001b[0;36mDEBUG\u001b[0m | src.tasks.clm_training.fabric.base]: \u001b[0;36mvalidate_after_k_steps value: 1000, type: <class 'int'>\u001b[0m\n",
      "2025-06-16 13:54:32 - src.tasks.clm_training.fabric.base - \u001b[0;36mDEBUG\u001b[0m - \u001b[0;36mstep_count value: 7, type: <class 'int'>\u001b[0m\n",
      "[\u001b[0;36mDEBUG\u001b[0m | src.tasks.clm_training.fabric.base]: \u001b[0;36mstep_count value: 7, type: <class 'int'>\u001b[0m\n",
      "2025-06-16 13:54:32 - src.tasks.clm_training.fabric.base - \u001b[0;36mDEBUG\u001b[0m - \u001b[0;36mValidation condition check - epochFinished: False, trainingFinished: False\u001b[0m\n",
      "[\u001b[0;36mDEBUG\u001b[0m | src.tasks.clm_training.fabric.base]: \u001b[0;36mValidation condition check - epochFinished: False, trainingFinished: False\u001b[0m\n",
      "2025-06-16 13:54:32 - src.tasks.clm_training.fabric.base - \u001b[0;36mDEBUG\u001b[0m - \u001b[0;36mAbout to check: step_count % validate_after_k_steps == 0 -> 7 % 1000 == 0\u001b[0m\n",
      "[\u001b[0;36mDEBUG\u001b[0m | src.tasks.clm_training.fabric.base]: \u001b[0;36mAbout to check: step_count % validate_after_k_steps == 0 -> 7 % 1000 == 0\u001b[0m\n",
      "2025-06-16 13:54:32 - src.tasks.clm_training.fabric.base - \u001b[0;36mDEBUG\u001b[0m - \u001b[0;36miter 14 step 7: loss 6.1875, iter time: 419.34ms remaining time: \u001b[0m\n",
      "[\u001b[0;36mDEBUG\u001b[0m | src.tasks.clm_training.fabric.base]: \u001b[0;36miter 14 step 7: loss 6.1875, iter time: 419.34ms remaining time: \u001b[0m\n",
      "  0%|\u001b[34m                                         \u001b[0m| 14/4682 [00:06<33:00,  2.36it/s]\u001b[0m2025-06-16 13:54:32 - src.tasks.clm_training.fabric.base - \u001b[0;36mDEBUG\u001b[0m - \u001b[0;36mgradient_accumulation_steps raw value: 2, type: <class 'int'>\u001b[0m\n",
      "[\u001b[0;36mDEBUG\u001b[0m | src.tasks.clm_training.fabric.base]: \u001b[0;36mgradient_accumulation_steps raw value: 2, type: <class 'int'>\u001b[0m\n",
      "2025-06-16 13:54:32 - src.tasks.clm_training.fabric.base - \u001b[0;36mDEBUG\u001b[0m - \u001b[0;36mgradient_accumulation_steps converted: 2, type: <class 'int'>\u001b[0m\n",
      "[\u001b[0;36mDEBUG\u001b[0m | src.tasks.clm_training.fabric.base]: \u001b[0;36mgradient_accumulation_steps converted: 2, type: <class 'int'>\u001b[0m\n",
      "2025-06-16 13:54:32 - src.tasks.clm_training.fabric.base - \u001b[0;36mDEBUG\u001b[0m - \u001b[0;36miter_num value: 14, type: <class 'int'>\u001b[0m\n",
      "[\u001b[0;36mDEBUG\u001b[0m | src.tasks.clm_training.fabric.base]: \u001b[0;36miter_num value: 14, type: <class 'int'>\u001b[0m\n",
      "2025-06-16 13:54:32 - src.tasks.clm_training.fabric.base - \u001b[0;36mDEBUG\u001b[0m - \u001b[0;36mAbout to check: (iter_num + 1) % gradient_accumulation_steps != 0 -> (14 + 1) % 2 != 0\u001b[0m\n",
      "[\u001b[0;36mDEBUG\u001b[0m | src.tasks.clm_training.fabric.base]: \u001b[0;36mAbout to check: (iter_num + 1) % gradient_accumulation_steps != 0 -> (14 + 1) % 2 != 0\u001b[0m\n",
      "2025-06-16 13:54:32 - src.tasks.clm_training.fabric.base - \u001b[0;36mDEBUG\u001b[0m - \u001b[0;36miter 15 step 7: loss 5.8438, iter time: 121.45ms remaining time: \u001b[0m\n",
      "[\u001b[0;36mDEBUG\u001b[0m | src.tasks.clm_training.fabric.base]: \u001b[0;36miter 15 step 7: loss 5.8438, iter time: 121.45ms remaining time: \u001b[0m\n",
      "  0%|\u001b[34m▏                                        \u001b[0m| 15/4682 [00:06<32:46,  2.37it/s]\u001b[0m2025-06-16 13:54:32 - src.tasks.clm_training.fabric.base - \u001b[0;36mDEBUG\u001b[0m - \u001b[0;36mgradient_accumulation_steps raw value: 2, type: <class 'int'>\u001b[0m\n",
      "[\u001b[0;36mDEBUG\u001b[0m | src.tasks.clm_training.fabric.base]: \u001b[0;36mgradient_accumulation_steps raw value: 2, type: <class 'int'>\u001b[0m\n",
      "2025-06-16 13:54:32 - src.tasks.clm_training.fabric.base - \u001b[0;36mDEBUG\u001b[0m - \u001b[0;36mgradient_accumulation_steps converted: 2, type: <class 'int'>\u001b[0m\n",
      "[\u001b[0;36mDEBUG\u001b[0m | src.tasks.clm_training.fabric.base]: \u001b[0;36mgradient_accumulation_steps converted: 2, type: <class 'int'>\u001b[0m\n",
      "2025-06-16 13:54:32 - src.tasks.clm_training.fabric.base - \u001b[0;36mDEBUG\u001b[0m - \u001b[0;36miter_num value: 15, type: <class 'int'>\u001b[0m\n",
      "[\u001b[0;36mDEBUG\u001b[0m | src.tasks.clm_training.fabric.base]: \u001b[0;36miter_num value: 15, type: <class 'int'>\u001b[0m\n",
      "2025-06-16 13:54:32 - src.tasks.clm_training.fabric.base - \u001b[0;36mDEBUG\u001b[0m - \u001b[0;36mAbout to check: (iter_num + 1) % gradient_accumulation_steps != 0 -> (15 + 1) % 2 != 0\u001b[0m\n",
      "[\u001b[0;36mDEBUG\u001b[0m | src.tasks.clm_training.fabric.base]: \u001b[0;36mAbout to check: (iter_num + 1) % gradient_accumulation_steps != 0 -> (15 + 1) % 2 != 0\u001b[0m\n",
      "2025-06-16 13:54:33 - src.tasks.clm_training.fabric.base - \u001b[0;36mDEBUG\u001b[0m - \u001b[0;36mGradient norm before clipping: 23.7500\u001b[0m\n",
      "[\u001b[0;36mDEBUG\u001b[0m | src.tasks.clm_training.fabric.base]: \u001b[0;36mGradient norm before clipping: 23.7500\u001b[0m\n",
      "2025-06-16 13:54:33 - src.tasks.clm_training.fabric.base - \u001b[0;36mDEBUG\u001b[0m - \u001b[0;36mvalidate_after_k_steps value: 1000, type: <class 'int'>\u001b[0m\n",
      "[\u001b[0;36mDEBUG\u001b[0m | src.tasks.clm_training.fabric.base]: \u001b[0;36mvalidate_after_k_steps value: 1000, type: <class 'int'>\u001b[0m\n",
      "2025-06-16 13:54:33 - src.tasks.clm_training.fabric.base - \u001b[0;36mDEBUG\u001b[0m - \u001b[0;36mstep_count value: 8, type: <class 'int'>\u001b[0m\n",
      "[\u001b[0;36mDEBUG\u001b[0m | src.tasks.clm_training.fabric.base]: \u001b[0;36mstep_count value: 8, type: <class 'int'>\u001b[0m\n",
      "2025-06-16 13:54:33 - src.tasks.clm_training.fabric.base - \u001b[0;36mDEBUG\u001b[0m - \u001b[0;36mValidation condition check - epochFinished: False, trainingFinished: False\u001b[0m\n",
      "[\u001b[0;36mDEBUG\u001b[0m | src.tasks.clm_training.fabric.base]: \u001b[0;36mValidation condition check - epochFinished: False, trainingFinished: False\u001b[0m\n",
      "2025-06-16 13:54:33 - src.tasks.clm_training.fabric.base - \u001b[0;36mDEBUG\u001b[0m - \u001b[0;36mAbout to check: step_count % validate_after_k_steps == 0 -> 8 % 1000 == 0\u001b[0m\n",
      "[\u001b[0;36mDEBUG\u001b[0m | src.tasks.clm_training.fabric.base]: \u001b[0;36mAbout to check: step_count % validate_after_k_steps == 0 -> 8 % 1000 == 0\u001b[0m\n",
      "2025-06-16 13:54:33 - src.tasks.clm_training.fabric.base - \u001b[0;36mDEBUG\u001b[0m - \u001b[0;36miter 16 step 8: loss 5.3438, iter time: 418.97ms remaining time: \u001b[0m\n",
      "[\u001b[0;36mDEBUG\u001b[0m | src.tasks.clm_training.fabric.base]: \u001b[0;36miter 16 step 8: loss 5.3438, iter time: 418.97ms remaining time: \u001b[0m\n",
      "  0%|\u001b[34m▏                                        \u001b[0m| 16/4682 [00:07<32:56,  2.36it/s]\u001b[0m2025-06-16 13:54:33 - src.tasks.clm_training.fabric.base - \u001b[0;36mDEBUG\u001b[0m - \u001b[0;36mgradient_accumulation_steps raw value: 2, type: <class 'int'>\u001b[0m\n",
      "[\u001b[0;36mDEBUG\u001b[0m | src.tasks.clm_training.fabric.base]: \u001b[0;36mgradient_accumulation_steps raw value: 2, type: <class 'int'>\u001b[0m\n",
      "2025-06-16 13:54:33 - src.tasks.clm_training.fabric.base - \u001b[0;36mDEBUG\u001b[0m - \u001b[0;36mgradient_accumulation_steps converted: 2, type: <class 'int'>\u001b[0m\n",
      "[\u001b[0;36mDEBUG\u001b[0m | src.tasks.clm_training.fabric.base]: \u001b[0;36mgradient_accumulation_steps converted: 2, type: <class 'int'>\u001b[0m\n",
      "2025-06-16 13:54:33 - src.tasks.clm_training.fabric.base - \u001b[0;36mDEBUG\u001b[0m - \u001b[0;36miter_num value: 16, type: <class 'int'>\u001b[0m\n",
      "[\u001b[0;36mDEBUG\u001b[0m | src.tasks.clm_training.fabric.base]: \u001b[0;36miter_num value: 16, type: <class 'int'>\u001b[0m\n",
      "2025-06-16 13:54:33 - src.tasks.clm_training.fabric.base - \u001b[0;36mDEBUG\u001b[0m - \u001b[0;36mAbout to check: (iter_num + 1) % gradient_accumulation_steps != 0 -> (16 + 1) % 2 != 0\u001b[0m\n",
      "[\u001b[0;36mDEBUG\u001b[0m | src.tasks.clm_training.fabric.base]: \u001b[0;36mAbout to check: (iter_num + 1) % gradient_accumulation_steps != 0 -> (16 + 1) % 2 != 0\u001b[0m\n",
      "2025-06-16 13:54:33 - src.tasks.clm_training.fabric.base - \u001b[0;36mDEBUG\u001b[0m - \u001b[0;36miter 17 step 8: loss 5.8750, iter time: 121.85ms remaining time: \u001b[0m\n",
      "[\u001b[0;36mDEBUG\u001b[0m | src.tasks.clm_training.fabric.base]: \u001b[0;36miter 17 step 8: loss 5.8750, iter time: 121.85ms remaining time: \u001b[0m\n",
      "  0%|\u001b[34m▏                                        \u001b[0m| 17/4682 [00:07<32:43,  2.38it/s]\u001b[0m2025-06-16 13:54:33 - src.tasks.clm_training.fabric.base - \u001b[0;36mDEBUG\u001b[0m - \u001b[0;36mgradient_accumulation_steps raw value: 2, type: <class 'int'>\u001b[0m\n",
      "[\u001b[0;36mDEBUG\u001b[0m | src.tasks.clm_training.fabric.base]: \u001b[0;36mgradient_accumulation_steps raw value: 2, type: <class 'int'>\u001b[0m\n",
      "2025-06-16 13:54:33 - src.tasks.clm_training.fabric.base - \u001b[0;36mDEBUG\u001b[0m - \u001b[0;36mgradient_accumulation_steps converted: 2, type: <class 'int'>\u001b[0m\n",
      "[\u001b[0;36mDEBUG\u001b[0m | src.tasks.clm_training.fabric.base]: \u001b[0;36mgradient_accumulation_steps converted: 2, type: <class 'int'>\u001b[0m\n",
      "2025-06-16 13:54:33 - src.tasks.clm_training.fabric.base - \u001b[0;36mDEBUG\u001b[0m - \u001b[0;36miter_num value: 17, type: <class 'int'>\u001b[0m\n",
      "[\u001b[0;36mDEBUG\u001b[0m | src.tasks.clm_training.fabric.base]: \u001b[0;36miter_num value: 17, type: <class 'int'>\u001b[0m\n",
      "2025-06-16 13:54:33 - src.tasks.clm_training.fabric.base - \u001b[0;36mDEBUG\u001b[0m - \u001b[0;36mAbout to check: (iter_num + 1) % gradient_accumulation_steps != 0 -> (17 + 1) % 2 != 0\u001b[0m\n",
      "[\u001b[0;36mDEBUG\u001b[0m | src.tasks.clm_training.fabric.base]: \u001b[0;36mAbout to check: (iter_num + 1) % gradient_accumulation_steps != 0 -> (17 + 1) % 2 != 0\u001b[0m\n",
      "2025-06-16 13:54:33 - src.tasks.clm_training.fabric.base - \u001b[0;36mDEBUG\u001b[0m - \u001b[0;36mGradient norm before clipping: 25.7500\u001b[0m\n",
      "[\u001b[0;36mDEBUG\u001b[0m | src.tasks.clm_training.fabric.base]: \u001b[0;36mGradient norm before clipping: 25.7500\u001b[0m\n",
      "2025-06-16 13:54:33 - src.tasks.clm_training.fabric.base - \u001b[0;36mDEBUG\u001b[0m - \u001b[0;36mvalidate_after_k_steps value: 1000, type: <class 'int'>\u001b[0m\n",
      "[\u001b[0;36mDEBUG\u001b[0m | src.tasks.clm_training.fabric.base]: \u001b[0;36mvalidate_after_k_steps value: 1000, type: <class 'int'>\u001b[0m\n",
      "2025-06-16 13:54:33 - src.tasks.clm_training.fabric.base - \u001b[0;36mDEBUG\u001b[0m - \u001b[0;36mstep_count value: 9, type: <class 'int'>\u001b[0m\n",
      "[\u001b[0;36mDEBUG\u001b[0m | src.tasks.clm_training.fabric.base]: \u001b[0;36mstep_count value: 9, type: <class 'int'>\u001b[0m\n",
      "2025-06-16 13:54:33 - src.tasks.clm_training.fabric.base - \u001b[0;36mDEBUG\u001b[0m - \u001b[0;36mValidation condition check - epochFinished: False, trainingFinished: False\u001b[0m\n",
      "[\u001b[0;36mDEBUG\u001b[0m | src.tasks.clm_training.fabric.base]: \u001b[0;36mValidation condition check - epochFinished: False, trainingFinished: False\u001b[0m\n",
      "2025-06-16 13:54:33 - src.tasks.clm_training.fabric.base - \u001b[0;36mDEBUG\u001b[0m - \u001b[0;36mAbout to check: step_count % validate_after_k_steps == 0 -> 9 % 1000 == 0\u001b[0m\n",
      "[\u001b[0;36mDEBUG\u001b[0m | src.tasks.clm_training.fabric.base]: \u001b[0;36mAbout to check: step_count % validate_after_k_steps == 0 -> 9 % 1000 == 0\u001b[0m\n",
      "2025-06-16 13:54:33 - src.tasks.clm_training.fabric.base - \u001b[0;36mDEBUG\u001b[0m - \u001b[0;36miter 18 step 9: loss 5.5938, iter time: 419.33ms remaining time: \u001b[0m\n",
      "[\u001b[0;36mDEBUG\u001b[0m | src.tasks.clm_training.fabric.base]: \u001b[0;36miter 18 step 9: loss 5.5938, iter time: 419.33ms remaining time: \u001b[0m\n",
      "  0%|\u001b[34m▏                                        \u001b[0m| 18/4682 [00:07<32:55,  2.36it/s]\u001b[0m2025-06-16 13:54:33 - src.tasks.clm_training.fabric.base - \u001b[0;36mDEBUG\u001b[0m - \u001b[0;36mgradient_accumulation_steps raw value: 2, type: <class 'int'>\u001b[0m\n",
      "[\u001b[0;36mDEBUG\u001b[0m | src.tasks.clm_training.fabric.base]: \u001b[0;36mgradient_accumulation_steps raw value: 2, type: <class 'int'>\u001b[0m\n",
      "2025-06-16 13:54:33 - src.tasks.clm_training.fabric.base - \u001b[0;36mDEBUG\u001b[0m - \u001b[0;36mgradient_accumulation_steps converted: 2, type: <class 'int'>\u001b[0m\n",
      "[\u001b[0;36mDEBUG\u001b[0m | src.tasks.clm_training.fabric.base]: \u001b[0;36mgradient_accumulation_steps converted: 2, type: <class 'int'>\u001b[0m\n",
      "2025-06-16 13:54:33 - src.tasks.clm_training.fabric.base - \u001b[0;36mDEBUG\u001b[0m - \u001b[0;36miter_num value: 18, type: <class 'int'>\u001b[0m\n",
      "[\u001b[0;36mDEBUG\u001b[0m | src.tasks.clm_training.fabric.base]: \u001b[0;36miter_num value: 18, type: <class 'int'>\u001b[0m\n",
      "2025-06-16 13:54:33 - src.tasks.clm_training.fabric.base - \u001b[0;36mDEBUG\u001b[0m - \u001b[0;36mAbout to check: (iter_num + 1) % gradient_accumulation_steps != 0 -> (18 + 1) % 2 != 0\u001b[0m\n",
      "[\u001b[0;36mDEBUG\u001b[0m | src.tasks.clm_training.fabric.base]: \u001b[0;36mAbout to check: (iter_num + 1) % gradient_accumulation_steps != 0 -> (18 + 1) % 2 != 0\u001b[0m\n",
      "2025-06-16 13:54:34 - src.tasks.clm_training.fabric.base - \u001b[0;36mDEBUG\u001b[0m - \u001b[0;36miter 19 step 9: loss 5.6875, iter time: 121.62ms remaining time: \u001b[0m\n",
      "[\u001b[0;36mDEBUG\u001b[0m | src.tasks.clm_training.fabric.base]: \u001b[0;36miter 19 step 9: loss 5.6875, iter time: 121.62ms remaining time: \u001b[0m\n",
      "  0%|\u001b[34m▏                                        \u001b[0m| 19/4682 [00:08<32:42,  2.38it/s]\u001b[0m2025-06-16 13:54:34 - src.tasks.clm_training.fabric.base - \u001b[0;36mDEBUG\u001b[0m - \u001b[0;36mgradient_accumulation_steps raw value: 2, type: <class 'int'>\u001b[0m\n",
      "[\u001b[0;36mDEBUG\u001b[0m | src.tasks.clm_training.fabric.base]: \u001b[0;36mgradient_accumulation_steps raw value: 2, type: <class 'int'>\u001b[0m\n",
      "2025-06-16 13:54:34 - src.tasks.clm_training.fabric.base - \u001b[0;36mDEBUG\u001b[0m - \u001b[0;36mgradient_accumulation_steps converted: 2, type: <class 'int'>\u001b[0m\n",
      "[\u001b[0;36mDEBUG\u001b[0m | src.tasks.clm_training.fabric.base]: \u001b[0;36mgradient_accumulation_steps converted: 2, type: <class 'int'>\u001b[0m\n",
      "2025-06-16 13:54:34 - src.tasks.clm_training.fabric.base - \u001b[0;36mDEBUG\u001b[0m - \u001b[0;36miter_num value: 19, type: <class 'int'>\u001b[0m\n",
      "[\u001b[0;36mDEBUG\u001b[0m | src.tasks.clm_training.fabric.base]: \u001b[0;36miter_num value: 19, type: <class 'int'>\u001b[0m\n",
      "2025-06-16 13:54:34 - src.tasks.clm_training.fabric.base - \u001b[0;36mDEBUG\u001b[0m - \u001b[0;36mAbout to check: (iter_num + 1) % gradient_accumulation_steps != 0 -> (19 + 1) % 2 != 0\u001b[0m\n",
      "[\u001b[0;36mDEBUG\u001b[0m | src.tasks.clm_training.fabric.base]: \u001b[0;36mAbout to check: (iter_num + 1) % gradient_accumulation_steps != 0 -> (19 + 1) % 2 != 0\u001b[0m\n",
      "2025-06-16 13:54:34 - src.tasks.clm_training.fabric.base - \u001b[0;36mDEBUG\u001b[0m - \u001b[0;36mGradient norm before clipping: 27.2500\u001b[0m\n",
      "[\u001b[0;36mDEBUG\u001b[0m | src.tasks.clm_training.fabric.base]: \u001b[0;36mGradient norm before clipping: 27.2500\u001b[0m\n",
      "2025-06-16 13:54:34 - src.tasks.clm_training.fabric.base - \u001b[0;36mDEBUG\u001b[0m - \u001b[0;36mvalidate_after_k_steps value: 1000, type: <class 'int'>\u001b[0m\n",
      "[\u001b[0;36mDEBUG\u001b[0m | src.tasks.clm_training.fabric.base]: \u001b[0;36mvalidate_after_k_steps value: 1000, type: <class 'int'>\u001b[0m\n",
      "2025-06-16 13:54:34 - src.tasks.clm_training.fabric.base - \u001b[0;36mDEBUG\u001b[0m - \u001b[0;36mstep_count value: 10, type: <class 'int'>\u001b[0m\n",
      "[\u001b[0;36mDEBUG\u001b[0m | src.tasks.clm_training.fabric.base]: \u001b[0;36mstep_count value: 10, type: <class 'int'>\u001b[0m\n",
      "2025-06-16 13:54:34 - src.tasks.clm_training.fabric.base - \u001b[0;36mDEBUG\u001b[0m - \u001b[0;36mValidation condition check - epochFinished: False, trainingFinished: False\u001b[0m\n",
      "[\u001b[0;36mDEBUG\u001b[0m | src.tasks.clm_training.fabric.base]: \u001b[0;36mValidation condition check - epochFinished: False, trainingFinished: False\u001b[0m\n",
      "2025-06-16 13:54:34 - src.tasks.clm_training.fabric.base - \u001b[0;36mDEBUG\u001b[0m - \u001b[0;36mAbout to check: step_count % validate_after_k_steps == 0 -> 10 % 1000 == 0\u001b[0m\n",
      "[\u001b[0;36mDEBUG\u001b[0m | src.tasks.clm_training.fabric.base]: \u001b[0;36mAbout to check: step_count % validate_after_k_steps == 0 -> 10 % 1000 == 0\u001b[0m\n",
      "2025-06-16 13:54:34 - src.tasks.clm_training.fabric.base - \u001b[0;36mDEBUG\u001b[0m - \u001b[0;36miter 20 step 10: loss 5.9688, iter time: 419.22ms remaining time: \u001b[0m\n",
      "[\u001b[0;36mDEBUG\u001b[0m | src.tasks.clm_training.fabric.base]: \u001b[0;36miter 20 step 10: loss 5.9688, iter time: 419.22ms remaining time: \u001b[0m\n",
      "  0%|\u001b[34m▏                                        \u001b[0m| 20/4682 [00:08<32:53,  2.36it/s]\u001b[0m2025-06-16 13:54:34 - src.tasks.clm_training.fabric.base - \u001b[0;36mDEBUG\u001b[0m - \u001b[0;36mgradient_accumulation_steps raw value: 2, type: <class 'int'>\u001b[0m\n",
      "[\u001b[0;36mDEBUG\u001b[0m | src.tasks.clm_training.fabric.base]: \u001b[0;36mgradient_accumulation_steps raw value: 2, type: <class 'int'>\u001b[0m\n",
      "2025-06-16 13:54:34 - src.tasks.clm_training.fabric.base - \u001b[0;36mDEBUG\u001b[0m - \u001b[0;36mgradient_accumulation_steps converted: 2, type: <class 'int'>\u001b[0m\n",
      "[\u001b[0;36mDEBUG\u001b[0m | src.tasks.clm_training.fabric.base]: \u001b[0;36mgradient_accumulation_steps converted: 2, type: <class 'int'>\u001b[0m\n",
      "2025-06-16 13:54:34 - src.tasks.clm_training.fabric.base - \u001b[0;36mDEBUG\u001b[0m - \u001b[0;36miter_num value: 20, type: <class 'int'>\u001b[0m\n",
      "[\u001b[0;36mDEBUG\u001b[0m | src.tasks.clm_training.fabric.base]: \u001b[0;36miter_num value: 20, type: <class 'int'>\u001b[0m\n",
      "2025-06-16 13:54:34 - src.tasks.clm_training.fabric.base - \u001b[0;36mDEBUG\u001b[0m - \u001b[0;36mAbout to check: (iter_num + 1) % gradient_accumulation_steps != 0 -> (20 + 1) % 2 != 0\u001b[0m\n",
      "[\u001b[0;36mDEBUG\u001b[0m | src.tasks.clm_training.fabric.base]: \u001b[0;36mAbout to check: (iter_num + 1) % gradient_accumulation_steps != 0 -> (20 + 1) % 2 != 0\u001b[0m\n",
      "2025-06-16 13:54:35 - src.tasks.clm_training.fabric.base - \u001b[0;36mDEBUG\u001b[0m - \u001b[0;36miter 21 step 10: loss 6.1250, iter time: 121.61ms remaining time: \u001b[0m\n",
      "[\u001b[0;36mDEBUG\u001b[0m | src.tasks.clm_training.fabric.base]: \u001b[0;36miter 21 step 10: loss 6.1250, iter time: 121.61ms remaining time: \u001b[0m\n",
      "  0%|\u001b[34m▏                                        \u001b[0m| 21/4682 [00:09<32:41,  2.38it/s]\u001b[0m2025-06-16 13:54:35 - src.tasks.clm_training.fabric.base - \u001b[0;36mDEBUG\u001b[0m - \u001b[0;36mgradient_accumulation_steps raw value: 2, type: <class 'int'>\u001b[0m\n",
      "[\u001b[0;36mDEBUG\u001b[0m | src.tasks.clm_training.fabric.base]: \u001b[0;36mgradient_accumulation_steps raw value: 2, type: <class 'int'>\u001b[0m\n",
      "2025-06-16 13:54:35 - src.tasks.clm_training.fabric.base - \u001b[0;36mDEBUG\u001b[0m - \u001b[0;36mgradient_accumulation_steps converted: 2, type: <class 'int'>\u001b[0m\n",
      "[\u001b[0;36mDEBUG\u001b[0m | src.tasks.clm_training.fabric.base]: \u001b[0;36mgradient_accumulation_steps converted: 2, type: <class 'int'>\u001b[0m\n",
      "2025-06-16 13:54:35 - src.tasks.clm_training.fabric.base - \u001b[0;36mDEBUG\u001b[0m - \u001b[0;36miter_num value: 21, type: <class 'int'>\u001b[0m\n",
      "[\u001b[0;36mDEBUG\u001b[0m | src.tasks.clm_training.fabric.base]: \u001b[0;36miter_num value: 21, type: <class 'int'>\u001b[0m\n",
      "2025-06-16 13:54:35 - src.tasks.clm_training.fabric.base - \u001b[0;36mDEBUG\u001b[0m - \u001b[0;36mAbout to check: (iter_num + 1) % gradient_accumulation_steps != 0 -> (21 + 1) % 2 != 0\u001b[0m\n",
      "[\u001b[0;36mDEBUG\u001b[0m | src.tasks.clm_training.fabric.base]: \u001b[0;36mAbout to check: (iter_num + 1) % gradient_accumulation_steps != 0 -> (21 + 1) % 2 != 0\u001b[0m\n",
      "2025-06-16 13:54:35 - src.tasks.clm_training.fabric.base - \u001b[0;36mDEBUG\u001b[0m - \u001b[0;36mGradient norm before clipping: 26.0000\u001b[0m\n",
      "[\u001b[0;36mDEBUG\u001b[0m | src.tasks.clm_training.fabric.base]: \u001b[0;36mGradient norm before clipping: 26.0000\u001b[0m\n",
      "2025-06-16 13:54:35 - src.tasks.clm_training.fabric.base - \u001b[0;36mDEBUG\u001b[0m - \u001b[0;36mvalidate_after_k_steps value: 1000, type: <class 'int'>\u001b[0m\n",
      "[\u001b[0;36mDEBUG\u001b[0m | src.tasks.clm_training.fabric.base]: \u001b[0;36mvalidate_after_k_steps value: 1000, type: <class 'int'>\u001b[0m\n",
      "2025-06-16 13:54:35 - src.tasks.clm_training.fabric.base - \u001b[0;36mDEBUG\u001b[0m - \u001b[0;36mstep_count value: 11, type: <class 'int'>\u001b[0m\n",
      "[\u001b[0;36mDEBUG\u001b[0m | src.tasks.clm_training.fabric.base]: \u001b[0;36mstep_count value: 11, type: <class 'int'>\u001b[0m\n",
      "2025-06-16 13:54:35 - src.tasks.clm_training.fabric.base - \u001b[0;36mDEBUG\u001b[0m - \u001b[0;36mValidation condition check - epochFinished: False, trainingFinished: False\u001b[0m\n",
      "[\u001b[0;36mDEBUG\u001b[0m | src.tasks.clm_training.fabric.base]: \u001b[0;36mValidation condition check - epochFinished: False, trainingFinished: False\u001b[0m\n",
      "2025-06-16 13:54:35 - src.tasks.clm_training.fabric.base - \u001b[0;36mDEBUG\u001b[0m - \u001b[0;36mAbout to check: step_count % validate_after_k_steps == 0 -> 11 % 1000 == 0\u001b[0m\n",
      "[\u001b[0;36mDEBUG\u001b[0m | src.tasks.clm_training.fabric.base]: \u001b[0;36mAbout to check: step_count % validate_after_k_steps == 0 -> 11 % 1000 == 0\u001b[0m\n",
      "2025-06-16 13:54:35 - src.tasks.clm_training.fabric.base - \u001b[0;36mDEBUG\u001b[0m - \u001b[0;36miter 22 step 11: loss 5.8750, iter time: 418.97ms remaining time: \u001b[0m\n",
      "[\u001b[0;36mDEBUG\u001b[0m | src.tasks.clm_training.fabric.base]: \u001b[0;36miter 22 step 11: loss 5.8750, iter time: 418.97ms remaining time: \u001b[0m\n",
      "  0%|\u001b[34m▏                                        \u001b[0m| 22/4682 [00:09<32:52,  2.36it/s]\u001b[0m2025-06-16 13:54:35 - src.tasks.clm_training.fabric.base - \u001b[0;36mDEBUG\u001b[0m - \u001b[0;36mgradient_accumulation_steps raw value: 2, type: <class 'int'>\u001b[0m\n",
      "[\u001b[0;36mDEBUG\u001b[0m | src.tasks.clm_training.fabric.base]: \u001b[0;36mgradient_accumulation_steps raw value: 2, type: <class 'int'>\u001b[0m\n",
      "2025-06-16 13:54:35 - src.tasks.clm_training.fabric.base - \u001b[0;36mDEBUG\u001b[0m - \u001b[0;36mgradient_accumulation_steps converted: 2, type: <class 'int'>\u001b[0m\n",
      "[\u001b[0;36mDEBUG\u001b[0m | src.tasks.clm_training.fabric.base]: \u001b[0;36mgradient_accumulation_steps converted: 2, type: <class 'int'>\u001b[0m\n",
      "2025-06-16 13:54:35 - src.tasks.clm_training.fabric.base - \u001b[0;36mDEBUG\u001b[0m - \u001b[0;36miter_num value: 22, type: <class 'int'>\u001b[0m\n",
      "[\u001b[0;36mDEBUG\u001b[0m | src.tasks.clm_training.fabric.base]: \u001b[0;36miter_num value: 22, type: <class 'int'>\u001b[0m\n",
      "2025-06-16 13:54:35 - src.tasks.clm_training.fabric.base - \u001b[0;36mDEBUG\u001b[0m - \u001b[0;36mAbout to check: (iter_num + 1) % gradient_accumulation_steps != 0 -> (22 + 1) % 2 != 0\u001b[0m\n",
      "[\u001b[0;36mDEBUG\u001b[0m | src.tasks.clm_training.fabric.base]: \u001b[0;36mAbout to check: (iter_num + 1) % gradient_accumulation_steps != 0 -> (22 + 1) % 2 != 0\u001b[0m\n",
      "2025-06-16 13:54:36 - src.tasks.clm_training.fabric.base - \u001b[0;36mDEBUG\u001b[0m - \u001b[0;36miter 23 step 11: loss 5.9688, iter time: 121.65ms remaining time: \u001b[0m\n",
      "[\u001b[0;36mDEBUG\u001b[0m | src.tasks.clm_training.fabric.base]: \u001b[0;36miter 23 step 11: loss 5.9688, iter time: 121.65ms remaining time: \u001b[0m\n",
      "  0%|\u001b[34m▏                                        \u001b[0m| 23/4682 [00:10<32:39,  2.38it/s]\u001b[0m2025-06-16 13:54:36 - src.tasks.clm_training.fabric.base - \u001b[0;36mDEBUG\u001b[0m - \u001b[0;36mgradient_accumulation_steps raw value: 2, type: <class 'int'>\u001b[0m\n",
      "[\u001b[0;36mDEBUG\u001b[0m | src.tasks.clm_training.fabric.base]: \u001b[0;36mgradient_accumulation_steps raw value: 2, type: <class 'int'>\u001b[0m\n",
      "2025-06-16 13:54:36 - src.tasks.clm_training.fabric.base - \u001b[0;36mDEBUG\u001b[0m - \u001b[0;36mgradient_accumulation_steps converted: 2, type: <class 'int'>\u001b[0m\n",
      "[\u001b[0;36mDEBUG\u001b[0m | src.tasks.clm_training.fabric.base]: \u001b[0;36mgradient_accumulation_steps converted: 2, type: <class 'int'>\u001b[0m\n",
      "2025-06-16 13:54:36 - src.tasks.clm_training.fabric.base - \u001b[0;36mDEBUG\u001b[0m - \u001b[0;36miter_num value: 23, type: <class 'int'>\u001b[0m\n",
      "[\u001b[0;36mDEBUG\u001b[0m | src.tasks.clm_training.fabric.base]: \u001b[0;36miter_num value: 23, type: <class 'int'>\u001b[0m\n",
      "2025-06-16 13:54:36 - src.tasks.clm_training.fabric.base - \u001b[0;36mDEBUG\u001b[0m - \u001b[0;36mAbout to check: (iter_num + 1) % gradient_accumulation_steps != 0 -> (23 + 1) % 2 != 0\u001b[0m\n",
      "[\u001b[0;36mDEBUG\u001b[0m | src.tasks.clm_training.fabric.base]: \u001b[0;36mAbout to check: (iter_num + 1) % gradient_accumulation_steps != 0 -> (23 + 1) % 2 != 0\u001b[0m\n",
      "2025-06-16 13:54:36 - src.tasks.clm_training.fabric.base - \u001b[0;36mDEBUG\u001b[0m - \u001b[0;36mGradient norm before clipping: 23.7500\u001b[0m\n",
      "[\u001b[0;36mDEBUG\u001b[0m | src.tasks.clm_training.fabric.base]: \u001b[0;36mGradient norm before clipping: 23.7500\u001b[0m\n",
      "2025-06-16 13:54:36 - src.tasks.clm_training.fabric.base - \u001b[0;36mDEBUG\u001b[0m - \u001b[0;36mvalidate_after_k_steps value: 1000, type: <class 'int'>\u001b[0m\n",
      "[\u001b[0;36mDEBUG\u001b[0m | src.tasks.clm_training.fabric.base]: \u001b[0;36mvalidate_after_k_steps value: 1000, type: <class 'int'>\u001b[0m\n",
      "2025-06-16 13:54:36 - src.tasks.clm_training.fabric.base - \u001b[0;36mDEBUG\u001b[0m - \u001b[0;36mstep_count value: 12, type: <class 'int'>\u001b[0m\n",
      "[\u001b[0;36mDEBUG\u001b[0m | src.tasks.clm_training.fabric.base]: \u001b[0;36mstep_count value: 12, type: <class 'int'>\u001b[0m\n",
      "2025-06-16 13:54:36 - src.tasks.clm_training.fabric.base - \u001b[0;36mDEBUG\u001b[0m - \u001b[0;36mValidation condition check - epochFinished: False, trainingFinished: False\u001b[0m\n",
      "[\u001b[0;36mDEBUG\u001b[0m | src.tasks.clm_training.fabric.base]: \u001b[0;36mValidation condition check - epochFinished: False, trainingFinished: False\u001b[0m\n",
      "2025-06-16 13:54:36 - src.tasks.clm_training.fabric.base - \u001b[0;36mDEBUG\u001b[0m - \u001b[0;36mAbout to check: step_count % validate_after_k_steps == 0 -> 12 % 1000 == 0\u001b[0m\n",
      "[\u001b[0;36mDEBUG\u001b[0m | src.tasks.clm_training.fabric.base]: \u001b[0;36mAbout to check: step_count % validate_after_k_steps == 0 -> 12 % 1000 == 0\u001b[0m\n",
      "2025-06-16 13:54:36 - src.tasks.clm_training.fabric.base - \u001b[0;36mDEBUG\u001b[0m - \u001b[0;36miter 24 step 12: loss 5.9688, iter time: 419.75ms remaining time: \u001b[0m\n",
      "[\u001b[0;36mDEBUG\u001b[0m | src.tasks.clm_training.fabric.base]: \u001b[0;36miter 24 step 12: loss 5.9688, iter time: 419.75ms remaining time: \u001b[0m\n",
      "  1%|\u001b[34m▏                                        \u001b[0m| 24/4682 [00:10<32:51,  2.36it/s]\u001b[0m2025-06-16 13:54:36 - src.tasks.clm_training.fabric.base - \u001b[0;36mDEBUG\u001b[0m - \u001b[0;36mgradient_accumulation_steps raw value: 2, type: <class 'int'>\u001b[0m\n",
      "[\u001b[0;36mDEBUG\u001b[0m | src.tasks.clm_training.fabric.base]: \u001b[0;36mgradient_accumulation_steps raw value: 2, type: <class 'int'>\u001b[0m\n",
      "2025-06-16 13:54:36 - src.tasks.clm_training.fabric.base - \u001b[0;36mDEBUG\u001b[0m - \u001b[0;36mgradient_accumulation_steps converted: 2, type: <class 'int'>\u001b[0m\n",
      "[\u001b[0;36mDEBUG\u001b[0m | src.tasks.clm_training.fabric.base]: \u001b[0;36mgradient_accumulation_steps converted: 2, type: <class 'int'>\u001b[0m\n",
      "2025-06-16 13:54:36 - src.tasks.clm_training.fabric.base - \u001b[0;36mDEBUG\u001b[0m - \u001b[0;36miter_num value: 24, type: <class 'int'>\u001b[0m\n",
      "[\u001b[0;36mDEBUG\u001b[0m | src.tasks.clm_training.fabric.base]: \u001b[0;36miter_num value: 24, type: <class 'int'>\u001b[0m\n",
      "2025-06-16 13:54:36 - src.tasks.clm_training.fabric.base - \u001b[0;36mDEBUG\u001b[0m - \u001b[0;36mAbout to check: (iter_num + 1) % gradient_accumulation_steps != 0 -> (24 + 1) % 2 != 0\u001b[0m\n",
      "[\u001b[0;36mDEBUG\u001b[0m | src.tasks.clm_training.fabric.base]: \u001b[0;36mAbout to check: (iter_num + 1) % gradient_accumulation_steps != 0 -> (24 + 1) % 2 != 0\u001b[0m\n",
      "2025-06-16 13:54:36 - src.tasks.clm_training.fabric.base - \u001b[0;36mDEBUG\u001b[0m - \u001b[0;36miter 25 step 12: loss 5.8750, iter time: 121.76ms remaining time: \u001b[0m\n",
      "[\u001b[0;36mDEBUG\u001b[0m | src.tasks.clm_training.fabric.base]: \u001b[0;36miter 25 step 12: loss 5.8750, iter time: 121.76ms remaining time: \u001b[0m\n",
      "  1%|\u001b[34m▏                                        \u001b[0m| 25/4682 [00:10<32:40,  2.38it/s]\u001b[0m2025-06-16 13:54:36 - src.tasks.clm_training.fabric.base - \u001b[0;36mDEBUG\u001b[0m - \u001b[0;36mgradient_accumulation_steps raw value: 2, type: <class 'int'>\u001b[0m\n",
      "[\u001b[0;36mDEBUG\u001b[0m | src.tasks.clm_training.fabric.base]: \u001b[0;36mgradient_accumulation_steps raw value: 2, type: <class 'int'>\u001b[0m\n",
      "2025-06-16 13:54:36 - src.tasks.clm_training.fabric.base - \u001b[0;36mDEBUG\u001b[0m - \u001b[0;36mgradient_accumulation_steps converted: 2, type: <class 'int'>\u001b[0m\n",
      "[\u001b[0;36mDEBUG\u001b[0m | src.tasks.clm_training.fabric.base]: \u001b[0;36mgradient_accumulation_steps converted: 2, type: <class 'int'>\u001b[0m\n",
      "2025-06-16 13:54:36 - src.tasks.clm_training.fabric.base - \u001b[0;36mDEBUG\u001b[0m - \u001b[0;36miter_num value: 25, type: <class 'int'>\u001b[0m\n",
      "[\u001b[0;36mDEBUG\u001b[0m | src.tasks.clm_training.fabric.base]: \u001b[0;36miter_num value: 25, type: <class 'int'>\u001b[0m\n",
      "2025-06-16 13:54:36 - src.tasks.clm_training.fabric.base - \u001b[0;36mDEBUG\u001b[0m - \u001b[0;36mAbout to check: (iter_num + 1) % gradient_accumulation_steps != 0 -> (25 + 1) % 2 != 0\u001b[0m\n",
      "[\u001b[0;36mDEBUG\u001b[0m | src.tasks.clm_training.fabric.base]: \u001b[0;36mAbout to check: (iter_num + 1) % gradient_accumulation_steps != 0 -> (25 + 1) % 2 != 0\u001b[0m\n",
      "2025-06-16 13:54:37 - src.tasks.clm_training.fabric.base - \u001b[0;36mDEBUG\u001b[0m - \u001b[0;36mGradient norm before clipping: 22.7500\u001b[0m\n",
      "[\u001b[0;36mDEBUG\u001b[0m | src.tasks.clm_training.fabric.base]: \u001b[0;36mGradient norm before clipping: 22.7500\u001b[0m\n",
      "2025-06-16 13:54:37 - src.tasks.clm_training.fabric.base - \u001b[0;36mDEBUG\u001b[0m - \u001b[0;36mvalidate_after_k_steps value: 1000, type: <class 'int'>\u001b[0m\n",
      "[\u001b[0;36mDEBUG\u001b[0m | src.tasks.clm_training.fabric.base]: \u001b[0;36mvalidate_after_k_steps value: 1000, type: <class 'int'>\u001b[0m\n",
      "2025-06-16 13:54:37 - src.tasks.clm_training.fabric.base - \u001b[0;36mDEBUG\u001b[0m - \u001b[0;36mstep_count value: 13, type: <class 'int'>\u001b[0m\n",
      "[\u001b[0;36mDEBUG\u001b[0m | src.tasks.clm_training.fabric.base]: \u001b[0;36mstep_count value: 13, type: <class 'int'>\u001b[0m\n",
      "2025-06-16 13:54:37 - src.tasks.clm_training.fabric.base - \u001b[0;36mDEBUG\u001b[0m - \u001b[0;36mValidation condition check - epochFinished: False, trainingFinished: False\u001b[0m\n",
      "[\u001b[0;36mDEBUG\u001b[0m | src.tasks.clm_training.fabric.base]: \u001b[0;36mValidation condition check - epochFinished: False, trainingFinished: False\u001b[0m\n",
      "2025-06-16 13:54:37 - src.tasks.clm_training.fabric.base - \u001b[0;36mDEBUG\u001b[0m - \u001b[0;36mAbout to check: step_count % validate_after_k_steps == 0 -> 13 % 1000 == 0\u001b[0m\n",
      "[\u001b[0;36mDEBUG\u001b[0m | src.tasks.clm_training.fabric.base]: \u001b[0;36mAbout to check: step_count % validate_after_k_steps == 0 -> 13 % 1000 == 0\u001b[0m\n",
      "2025-06-16 13:54:37 - src.tasks.clm_training.fabric.base - \u001b[0;36mDEBUG\u001b[0m - \u001b[0;36miter 26 step 13: loss 5.9688, iter time: 418.98ms remaining time: \u001b[0m\n",
      "[\u001b[0;36mDEBUG\u001b[0m | src.tasks.clm_training.fabric.base]: \u001b[0;36miter 26 step 13: loss 5.9688, iter time: 418.98ms remaining time: \u001b[0m\n",
      "  1%|\u001b[34m▏                                        \u001b[0m| 26/4682 [00:11<32:51,  2.36it/s]\u001b[0m2025-06-16 13:54:37 - src.tasks.clm_training.fabric.base - \u001b[0;36mDEBUG\u001b[0m - \u001b[0;36mgradient_accumulation_steps raw value: 2, type: <class 'int'>\u001b[0m\n",
      "[\u001b[0;36mDEBUG\u001b[0m | src.tasks.clm_training.fabric.base]: \u001b[0;36mgradient_accumulation_steps raw value: 2, type: <class 'int'>\u001b[0m\n",
      "2025-06-16 13:54:37 - src.tasks.clm_training.fabric.base - \u001b[0;36mDEBUG\u001b[0m - \u001b[0;36mgradient_accumulation_steps converted: 2, type: <class 'int'>\u001b[0m\n",
      "[\u001b[0;36mDEBUG\u001b[0m | src.tasks.clm_training.fabric.base]: \u001b[0;36mgradient_accumulation_steps converted: 2, type: <class 'int'>\u001b[0m\n",
      "2025-06-16 13:54:37 - src.tasks.clm_training.fabric.base - \u001b[0;36mDEBUG\u001b[0m - \u001b[0;36miter_num value: 26, type: <class 'int'>\u001b[0m\n",
      "[\u001b[0;36mDEBUG\u001b[0m | src.tasks.clm_training.fabric.base]: \u001b[0;36miter_num value: 26, type: <class 'int'>\u001b[0m\n",
      "2025-06-16 13:54:37 - src.tasks.clm_training.fabric.base - \u001b[0;36mDEBUG\u001b[0m - \u001b[0;36mAbout to check: (iter_num + 1) % gradient_accumulation_steps != 0 -> (26 + 1) % 2 != 0\u001b[0m\n",
      "[\u001b[0;36mDEBUG\u001b[0m | src.tasks.clm_training.fabric.base]: \u001b[0;36mAbout to check: (iter_num + 1) % gradient_accumulation_steps != 0 -> (26 + 1) % 2 != 0\u001b[0m\n",
      "2025-06-16 13:54:37 - src.tasks.clm_training.fabric.base - \u001b[0;36mDEBUG\u001b[0m - \u001b[0;36miter 27 step 13: loss 5.8438, iter time: 121.21ms remaining time: \u001b[0m\n",
      "[\u001b[0;36mDEBUG\u001b[0m | src.tasks.clm_training.fabric.base]: \u001b[0;36miter 27 step 13: loss 5.8438, iter time: 121.21ms remaining time: \u001b[0m\n",
      "  1%|\u001b[34m▏                                        \u001b[0m| 27/4682 [00:11<32:38,  2.38it/s]\u001b[0m2025-06-16 13:54:37 - src.tasks.clm_training.fabric.base - \u001b[0;36mDEBUG\u001b[0m - \u001b[0;36mgradient_accumulation_steps raw value: 2, type: <class 'int'>\u001b[0m\n",
      "[\u001b[0;36mDEBUG\u001b[0m | src.tasks.clm_training.fabric.base]: \u001b[0;36mgradient_accumulation_steps raw value: 2, type: <class 'int'>\u001b[0m\n",
      "2025-06-16 13:54:37 - src.tasks.clm_training.fabric.base - \u001b[0;36mDEBUG\u001b[0m - \u001b[0;36mgradient_accumulation_steps converted: 2, type: <class 'int'>\u001b[0m\n",
      "[\u001b[0;36mDEBUG\u001b[0m | src.tasks.clm_training.fabric.base]: \u001b[0;36mgradient_accumulation_steps converted: 2, type: <class 'int'>\u001b[0m\n",
      "2025-06-16 13:54:37 - src.tasks.clm_training.fabric.base - \u001b[0;36mDEBUG\u001b[0m - \u001b[0;36miter_num value: 27, type: <class 'int'>\u001b[0m\n",
      "[\u001b[0;36mDEBUG\u001b[0m | src.tasks.clm_training.fabric.base]: \u001b[0;36miter_num value: 27, type: <class 'int'>\u001b[0m\n",
      "2025-06-16 13:54:37 - src.tasks.clm_training.fabric.base - \u001b[0;36mDEBUG\u001b[0m - \u001b[0;36mAbout to check: (iter_num + 1) % gradient_accumulation_steps != 0 -> (27 + 1) % 2 != 0\u001b[0m\n",
      "[\u001b[0;36mDEBUG\u001b[0m | src.tasks.clm_training.fabric.base]: \u001b[0;36mAbout to check: (iter_num + 1) % gradient_accumulation_steps != 0 -> (27 + 1) % 2 != 0\u001b[0m\n",
      "2025-06-16 13:54:38 - src.tasks.clm_training.fabric.base - \u001b[0;36mDEBUG\u001b[0m - \u001b[0;36mGradient norm before clipping: 22.6250\u001b[0m\n",
      "[\u001b[0;36mDEBUG\u001b[0m | src.tasks.clm_training.fabric.base]: \u001b[0;36mGradient norm before clipping: 22.6250\u001b[0m\n",
      "2025-06-16 13:54:38 - src.tasks.clm_training.fabric.base - \u001b[0;36mDEBUG\u001b[0m - \u001b[0;36mvalidate_after_k_steps value: 1000, type: <class 'int'>\u001b[0m\n",
      "[\u001b[0;36mDEBUG\u001b[0m | src.tasks.clm_training.fabric.base]: \u001b[0;36mvalidate_after_k_steps value: 1000, type: <class 'int'>\u001b[0m\n",
      "2025-06-16 13:54:38 - src.tasks.clm_training.fabric.base - \u001b[0;36mDEBUG\u001b[0m - \u001b[0;36mstep_count value: 14, type: <class 'int'>\u001b[0m\n",
      "[\u001b[0;36mDEBUG\u001b[0m | src.tasks.clm_training.fabric.base]: \u001b[0;36mstep_count value: 14, type: <class 'int'>\u001b[0m\n",
      "2025-06-16 13:54:38 - src.tasks.clm_training.fabric.base - \u001b[0;36mDEBUG\u001b[0m - \u001b[0;36mValidation condition check - epochFinished: False, trainingFinished: False\u001b[0m\n",
      "[\u001b[0;36mDEBUG\u001b[0m | src.tasks.clm_training.fabric.base]: \u001b[0;36mValidation condition check - epochFinished: False, trainingFinished: False\u001b[0m\n",
      "2025-06-16 13:54:38 - src.tasks.clm_training.fabric.base - \u001b[0;36mDEBUG\u001b[0m - \u001b[0;36mAbout to check: step_count % validate_after_k_steps == 0 -> 14 % 1000 == 0\u001b[0m\n",
      "[\u001b[0;36mDEBUG\u001b[0m | src.tasks.clm_training.fabric.base]: \u001b[0;36mAbout to check: step_count % validate_after_k_steps == 0 -> 14 % 1000 == 0\u001b[0m\n",
      "2025-06-16 13:54:38 - src.tasks.clm_training.fabric.base - \u001b[0;36mDEBUG\u001b[0m - \u001b[0;36miter 28 step 14: loss 5.9062, iter time: 419.75ms remaining time: \u001b[0m\n",
      "[\u001b[0;36mDEBUG\u001b[0m | src.tasks.clm_training.fabric.base]: \u001b[0;36miter 28 step 14: loss 5.9062, iter time: 419.75ms remaining time: \u001b[0m\n",
      "  1%|\u001b[34m▏                                        \u001b[0m| 28/4682 [00:12<32:50,  2.36it/s]\u001b[0m2025-06-16 13:54:38 - src.tasks.clm_training.fabric.base - \u001b[0;36mDEBUG\u001b[0m - \u001b[0;36mgradient_accumulation_steps raw value: 2, type: <class 'int'>\u001b[0m\n",
      "[\u001b[0;36mDEBUG\u001b[0m | src.tasks.clm_training.fabric.base]: \u001b[0;36mgradient_accumulation_steps raw value: 2, type: <class 'int'>\u001b[0m\n",
      "2025-06-16 13:54:38 - src.tasks.clm_training.fabric.base - \u001b[0;36mDEBUG\u001b[0m - \u001b[0;36mgradient_accumulation_steps converted: 2, type: <class 'int'>\u001b[0m\n",
      "[\u001b[0;36mDEBUG\u001b[0m | src.tasks.clm_training.fabric.base]: \u001b[0;36mgradient_accumulation_steps converted: 2, type: <class 'int'>\u001b[0m\n",
      "2025-06-16 13:54:38 - src.tasks.clm_training.fabric.base - \u001b[0;36mDEBUG\u001b[0m - \u001b[0;36miter_num value: 28, type: <class 'int'>\u001b[0m\n",
      "[\u001b[0;36mDEBUG\u001b[0m | src.tasks.clm_training.fabric.base]: \u001b[0;36miter_num value: 28, type: <class 'int'>\u001b[0m\n",
      "2025-06-16 13:54:38 - src.tasks.clm_training.fabric.base - \u001b[0;36mDEBUG\u001b[0m - \u001b[0;36mAbout to check: (iter_num + 1) % gradient_accumulation_steps != 0 -> (28 + 1) % 2 != 0\u001b[0m\n",
      "[\u001b[0;36mDEBUG\u001b[0m | src.tasks.clm_training.fabric.base]: \u001b[0;36mAbout to check: (iter_num + 1) % gradient_accumulation_steps != 0 -> (28 + 1) % 2 != 0\u001b[0m\n",
      "2025-06-16 13:54:38 - src.tasks.clm_training.fabric.base - \u001b[0;36mDEBUG\u001b[0m - \u001b[0;36miter 29 step 14: loss 6.0625, iter time: 121.71ms remaining time: \u001b[0m\n",
      "[\u001b[0;36mDEBUG\u001b[0m | src.tasks.clm_training.fabric.base]: \u001b[0;36miter 29 step 14: loss 6.0625, iter time: 121.71ms remaining time: \u001b[0m\n",
      "  1%|\u001b[34m▎                                        \u001b[0m| 29/4682 [00:12<32:38,  2.38it/s]\u001b[0m2025-06-16 13:54:38 - src.tasks.clm_training.fabric.base - \u001b[0;36mDEBUG\u001b[0m - \u001b[0;36mgradient_accumulation_steps raw value: 2, type: <class 'int'>\u001b[0m\n",
      "[\u001b[0;36mDEBUG\u001b[0m | src.tasks.clm_training.fabric.base]: \u001b[0;36mgradient_accumulation_steps raw value: 2, type: <class 'int'>\u001b[0m\n",
      "2025-06-16 13:54:38 - src.tasks.clm_training.fabric.base - \u001b[0;36mDEBUG\u001b[0m - \u001b[0;36mgradient_accumulation_steps converted: 2, type: <class 'int'>\u001b[0m\n",
      "[\u001b[0;36mDEBUG\u001b[0m | src.tasks.clm_training.fabric.base]: \u001b[0;36mgradient_accumulation_steps converted: 2, type: <class 'int'>\u001b[0m\n",
      "2025-06-16 13:54:38 - src.tasks.clm_training.fabric.base - \u001b[0;36mDEBUG\u001b[0m - \u001b[0;36miter_num value: 29, type: <class 'int'>\u001b[0m\n",
      "[\u001b[0;36mDEBUG\u001b[0m | src.tasks.clm_training.fabric.base]: \u001b[0;36miter_num value: 29, type: <class 'int'>\u001b[0m\n",
      "2025-06-16 13:54:38 - src.tasks.clm_training.fabric.base - \u001b[0;36mDEBUG\u001b[0m - \u001b[0;36mAbout to check: (iter_num + 1) % gradient_accumulation_steps != 0 -> (29 + 1) % 2 != 0\u001b[0m\n",
      "[\u001b[0;36mDEBUG\u001b[0m | src.tasks.clm_training.fabric.base]: \u001b[0;36mAbout to check: (iter_num + 1) % gradient_accumulation_steps != 0 -> (29 + 1) % 2 != 0\u001b[0m\n",
      "2025-06-16 13:54:39 - src.tasks.clm_training.fabric.base - \u001b[0;36mDEBUG\u001b[0m - \u001b[0;36mGradient norm before clipping: 24.0000\u001b[0m\n",
      "[\u001b[0;36mDEBUG\u001b[0m | src.tasks.clm_training.fabric.base]: \u001b[0;36mGradient norm before clipping: 24.0000\u001b[0m\n",
      "2025-06-16 13:54:39 - src.tasks.clm_training.fabric.base - \u001b[0;36mDEBUG\u001b[0m - \u001b[0;36mvalidate_after_k_steps value: 1000, type: <class 'int'>\u001b[0m\n",
      "[\u001b[0;36mDEBUG\u001b[0m | src.tasks.clm_training.fabric.base]: \u001b[0;36mvalidate_after_k_steps value: 1000, type: <class 'int'>\u001b[0m\n",
      "2025-06-16 13:54:39 - src.tasks.clm_training.fabric.base - \u001b[0;36mDEBUG\u001b[0m - \u001b[0;36mstep_count value: 15, type: <class 'int'>\u001b[0m\n",
      "[\u001b[0;36mDEBUG\u001b[0m | src.tasks.clm_training.fabric.base]: \u001b[0;36mstep_count value: 15, type: <class 'int'>\u001b[0m\n",
      "2025-06-16 13:54:39 - src.tasks.clm_training.fabric.base - \u001b[0;36mDEBUG\u001b[0m - \u001b[0;36mValidation condition check - epochFinished: False, trainingFinished: False\u001b[0m\n",
      "[\u001b[0;36mDEBUG\u001b[0m | src.tasks.clm_training.fabric.base]: \u001b[0;36mValidation condition check - epochFinished: False, trainingFinished: False\u001b[0m\n",
      "2025-06-16 13:54:39 - src.tasks.clm_training.fabric.base - \u001b[0;36mDEBUG\u001b[0m - \u001b[0;36mAbout to check: step_count % validate_after_k_steps == 0 -> 15 % 1000 == 0\u001b[0m\n",
      "[\u001b[0;36mDEBUG\u001b[0m | src.tasks.clm_training.fabric.base]: \u001b[0;36mAbout to check: step_count % validate_after_k_steps == 0 -> 15 % 1000 == 0\u001b[0m\n",
      "2025-06-16 13:54:39 - src.tasks.clm_training.fabric.base - \u001b[0;36mDEBUG\u001b[0m - \u001b[0;36miter 30 step 15: loss 5.5938, iter time: 420.17ms remaining time: \u001b[0m\n",
      "[\u001b[0;36mDEBUG\u001b[0m | src.tasks.clm_training.fabric.base]: \u001b[0;36miter 30 step 15: loss 5.5938, iter time: 420.17ms remaining time: \u001b[0m\n",
      "  1%|\u001b[34m▎                                        \u001b[0m| 30/4682 [00:12<32:50,  2.36it/s]\u001b[0m2025-06-16 13:54:39 - src.tasks.clm_training.fabric.base - \u001b[0;36mDEBUG\u001b[0m - \u001b[0;36mgradient_accumulation_steps raw value: 2, type: <class 'int'>\u001b[0m\n",
      "[\u001b[0;36mDEBUG\u001b[0m | src.tasks.clm_training.fabric.base]: \u001b[0;36mgradient_accumulation_steps raw value: 2, type: <class 'int'>\u001b[0m\n",
      "2025-06-16 13:54:39 - src.tasks.clm_training.fabric.base - \u001b[0;36mDEBUG\u001b[0m - \u001b[0;36mgradient_accumulation_steps converted: 2, type: <class 'int'>\u001b[0m\n",
      "[\u001b[0;36mDEBUG\u001b[0m | src.tasks.clm_training.fabric.base]: \u001b[0;36mgradient_accumulation_steps converted: 2, type: <class 'int'>\u001b[0m\n",
      "2025-06-16 13:54:39 - src.tasks.clm_training.fabric.base - \u001b[0;36mDEBUG\u001b[0m - \u001b[0;36miter_num value: 30, type: <class 'int'>\u001b[0m\n",
      "[\u001b[0;36mDEBUG\u001b[0m | src.tasks.clm_training.fabric.base]: \u001b[0;36miter_num value: 30, type: <class 'int'>\u001b[0m\n",
      "2025-06-16 13:54:39 - src.tasks.clm_training.fabric.base - \u001b[0;36mDEBUG\u001b[0m - \u001b[0;36mAbout to check: (iter_num + 1) % gradient_accumulation_steps != 0 -> (30 + 1) % 2 != 0\u001b[0m\n",
      "[\u001b[0;36mDEBUG\u001b[0m | src.tasks.clm_training.fabric.base]: \u001b[0;36mAbout to check: (iter_num + 1) % gradient_accumulation_steps != 0 -> (30 + 1) % 2 != 0\u001b[0m\n",
      "2025-06-16 13:54:39 - src.tasks.clm_training.fabric.base - \u001b[0;36mDEBUG\u001b[0m - \u001b[0;36miter 31 step 15: loss 5.8750, iter time: 121.76ms remaining time: \u001b[0m\n",
      "[\u001b[0;36mDEBUG\u001b[0m | src.tasks.clm_training.fabric.base]: \u001b[0;36miter 31 step 15: loss 5.8750, iter time: 121.76ms remaining time: \u001b[0m\n",
      "  1%|\u001b[34m▎                                        \u001b[0m| 31/4682 [00:13<32:37,  2.38it/s]\u001b[0m2025-06-16 13:54:39 - src.tasks.clm_training.fabric.base - \u001b[0;36mDEBUG\u001b[0m - \u001b[0;36mgradient_accumulation_steps raw value: 2, type: <class 'int'>\u001b[0m\n",
      "[\u001b[0;36mDEBUG\u001b[0m | src.tasks.clm_training.fabric.base]: \u001b[0;36mgradient_accumulation_steps raw value: 2, type: <class 'int'>\u001b[0m\n",
      "2025-06-16 13:54:39 - src.tasks.clm_training.fabric.base - \u001b[0;36mDEBUG\u001b[0m - \u001b[0;36mgradient_accumulation_steps converted: 2, type: <class 'int'>\u001b[0m\n",
      "[\u001b[0;36mDEBUG\u001b[0m | src.tasks.clm_training.fabric.base]: \u001b[0;36mgradient_accumulation_steps converted: 2, type: <class 'int'>\u001b[0m\n",
      "2025-06-16 13:54:39 - src.tasks.clm_training.fabric.base - \u001b[0;36mDEBUG\u001b[0m - \u001b[0;36miter_num value: 31, type: <class 'int'>\u001b[0m\n",
      "[\u001b[0;36mDEBUG\u001b[0m | src.tasks.clm_training.fabric.base]: \u001b[0;36miter_num value: 31, type: <class 'int'>\u001b[0m\n",
      "2025-06-16 13:54:39 - src.tasks.clm_training.fabric.base - \u001b[0;36mDEBUG\u001b[0m - \u001b[0;36mAbout to check: (iter_num + 1) % gradient_accumulation_steps != 0 -> (31 + 1) % 2 != 0\u001b[0m\n",
      "[\u001b[0;36mDEBUG\u001b[0m | src.tasks.clm_training.fabric.base]: \u001b[0;36mAbout to check: (iter_num + 1) % gradient_accumulation_steps != 0 -> (31 + 1) % 2 != 0\u001b[0m\n",
      "2025-06-16 13:54:39 - src.tasks.clm_training.fabric.base - \u001b[0;36mDEBUG\u001b[0m - \u001b[0;36mGradient norm before clipping: 22.3750\u001b[0m\n",
      "[\u001b[0;36mDEBUG\u001b[0m | src.tasks.clm_training.fabric.base]: \u001b[0;36mGradient norm before clipping: 22.3750\u001b[0m\n",
      "2025-06-16 13:54:39 - src.tasks.clm_training.fabric.base - \u001b[0;36mDEBUG\u001b[0m - \u001b[0;36mvalidate_after_k_steps value: 1000, type: <class 'int'>\u001b[0m\n",
      "[\u001b[0;36mDEBUG\u001b[0m | src.tasks.clm_training.fabric.base]: \u001b[0;36mvalidate_after_k_steps value: 1000, type: <class 'int'>\u001b[0m\n",
      "2025-06-16 13:54:39 - src.tasks.clm_training.fabric.base - \u001b[0;36mDEBUG\u001b[0m - \u001b[0;36mstep_count value: 16, type: <class 'int'>\u001b[0m\n",
      "[\u001b[0;36mDEBUG\u001b[0m | src.tasks.clm_training.fabric.base]: \u001b[0;36mstep_count value: 16, type: <class 'int'>\u001b[0m\n",
      "2025-06-16 13:54:39 - src.tasks.clm_training.fabric.base - \u001b[0;36mDEBUG\u001b[0m - \u001b[0;36mValidation condition check - epochFinished: False, trainingFinished: False\u001b[0m\n",
      "[\u001b[0;36mDEBUG\u001b[0m | src.tasks.clm_training.fabric.base]: \u001b[0;36mValidation condition check - epochFinished: False, trainingFinished: False\u001b[0m\n",
      "2025-06-16 13:54:39 - src.tasks.clm_training.fabric.base - \u001b[0;36mDEBUG\u001b[0m - \u001b[0;36mAbout to check: step_count % validate_after_k_steps == 0 -> 16 % 1000 == 0\u001b[0m\n",
      "[\u001b[0;36mDEBUG\u001b[0m | src.tasks.clm_training.fabric.base]: \u001b[0;36mAbout to check: step_count % validate_after_k_steps == 0 -> 16 % 1000 == 0\u001b[0m\n",
      "2025-06-16 13:54:39 - src.tasks.clm_training.fabric.base - \u001b[0;36mDEBUG\u001b[0m - \u001b[0;36miter 32 step 16: loss 5.8438, iter time: 419.89ms remaining time: \u001b[0m\n",
      "[\u001b[0;36mDEBUG\u001b[0m | src.tasks.clm_training.fabric.base]: \u001b[0;36miter 32 step 16: loss 5.8438, iter time: 419.89ms remaining time: \u001b[0m\n",
      "  1%|\u001b[34m▎                                        \u001b[0m| 32/4682 [00:13<32:50,  2.36it/s]\u001b[0m2025-06-16 13:54:39 - src.tasks.clm_training.fabric.base - \u001b[0;36mDEBUG\u001b[0m - \u001b[0;36mgradient_accumulation_steps raw value: 2, type: <class 'int'>\u001b[0m\n",
      "[\u001b[0;36mDEBUG\u001b[0m | src.tasks.clm_training.fabric.base]: \u001b[0;36mgradient_accumulation_steps raw value: 2, type: <class 'int'>\u001b[0m\n",
      "2025-06-16 13:54:39 - src.tasks.clm_training.fabric.base - \u001b[0;36mDEBUG\u001b[0m - \u001b[0;36mgradient_accumulation_steps converted: 2, type: <class 'int'>\u001b[0m\n",
      "[\u001b[0;36mDEBUG\u001b[0m | src.tasks.clm_training.fabric.base]: \u001b[0;36mgradient_accumulation_steps converted: 2, type: <class 'int'>\u001b[0m\n",
      "2025-06-16 13:54:39 - src.tasks.clm_training.fabric.base - \u001b[0;36mDEBUG\u001b[0m - \u001b[0;36miter_num value: 32, type: <class 'int'>\u001b[0m\n",
      "[\u001b[0;36mDEBUG\u001b[0m | src.tasks.clm_training.fabric.base]: \u001b[0;36miter_num value: 32, type: <class 'int'>\u001b[0m\n",
      "2025-06-16 13:54:39 - src.tasks.clm_training.fabric.base - \u001b[0;36mDEBUG\u001b[0m - \u001b[0;36mAbout to check: (iter_num + 1) % gradient_accumulation_steps != 0 -> (32 + 1) % 2 != 0\u001b[0m\n",
      "[\u001b[0;36mDEBUG\u001b[0m | src.tasks.clm_training.fabric.base]: \u001b[0;36mAbout to check: (iter_num + 1) % gradient_accumulation_steps != 0 -> (32 + 1) % 2 != 0\u001b[0m\n",
      "2025-06-16 13:54:40 - src.tasks.clm_training.fabric.base - \u001b[0;36mDEBUG\u001b[0m - \u001b[0;36miter 33 step 16: loss 5.8750, iter time: 121.62ms remaining time: \u001b[0m\n",
      "[\u001b[0;36mDEBUG\u001b[0m | src.tasks.clm_training.fabric.base]: \u001b[0;36miter 33 step 16: loss 5.8750, iter time: 121.62ms remaining time: \u001b[0m\n",
      "  1%|\u001b[34m▎                                        \u001b[0m| 33/4682 [00:14<32:36,  2.38it/s]\u001b[0m2025-06-16 13:54:40 - src.tasks.clm_training.fabric.base - \u001b[0;36mDEBUG\u001b[0m - \u001b[0;36mgradient_accumulation_steps raw value: 2, type: <class 'int'>\u001b[0m\n",
      "[\u001b[0;36mDEBUG\u001b[0m | src.tasks.clm_training.fabric.base]: \u001b[0;36mgradient_accumulation_steps raw value: 2, type: <class 'int'>\u001b[0m\n",
      "2025-06-16 13:54:40 - src.tasks.clm_training.fabric.base - \u001b[0;36mDEBUG\u001b[0m - \u001b[0;36mgradient_accumulation_steps converted: 2, type: <class 'int'>\u001b[0m\n",
      "[\u001b[0;36mDEBUG\u001b[0m | src.tasks.clm_training.fabric.base]: \u001b[0;36mgradient_accumulation_steps converted: 2, type: <class 'int'>\u001b[0m\n",
      "2025-06-16 13:54:40 - src.tasks.clm_training.fabric.base - \u001b[0;36mDEBUG\u001b[0m - \u001b[0;36miter_num value: 33, type: <class 'int'>\u001b[0m\n",
      "[\u001b[0;36mDEBUG\u001b[0m | src.tasks.clm_training.fabric.base]: \u001b[0;36miter_num value: 33, type: <class 'int'>\u001b[0m\n",
      "2025-06-16 13:54:40 - src.tasks.clm_training.fabric.base - \u001b[0;36mDEBUG\u001b[0m - \u001b[0;36mAbout to check: (iter_num + 1) % gradient_accumulation_steps != 0 -> (33 + 1) % 2 != 0\u001b[0m\n",
      "[\u001b[0;36mDEBUG\u001b[0m | src.tasks.clm_training.fabric.base]: \u001b[0;36mAbout to check: (iter_num + 1) % gradient_accumulation_steps != 0 -> (33 + 1) % 2 != 0\u001b[0m\n",
      "2025-06-16 13:54:40 - src.tasks.clm_training.fabric.base - \u001b[0;36mDEBUG\u001b[0m - \u001b[0;36mGradient norm before clipping: 24.5000\u001b[0m\n",
      "[\u001b[0;36mDEBUG\u001b[0m | src.tasks.clm_training.fabric.base]: \u001b[0;36mGradient norm before clipping: 24.5000\u001b[0m\n",
      "2025-06-16 13:54:40 - src.tasks.clm_training.fabric.base - \u001b[0;36mDEBUG\u001b[0m - \u001b[0;36mvalidate_after_k_steps value: 1000, type: <class 'int'>\u001b[0m\n",
      "[\u001b[0;36mDEBUG\u001b[0m | src.tasks.clm_training.fabric.base]: \u001b[0;36mvalidate_after_k_steps value: 1000, type: <class 'int'>\u001b[0m\n",
      "2025-06-16 13:54:40 - src.tasks.clm_training.fabric.base - \u001b[0;36mDEBUG\u001b[0m - \u001b[0;36mstep_count value: 17, type: <class 'int'>\u001b[0m\n",
      "[\u001b[0;36mDEBUG\u001b[0m | src.tasks.clm_training.fabric.base]: \u001b[0;36mstep_count value: 17, type: <class 'int'>\u001b[0m\n",
      "2025-06-16 13:54:40 - src.tasks.clm_training.fabric.base - \u001b[0;36mDEBUG\u001b[0m - \u001b[0;36mValidation condition check - epochFinished: False, trainingFinished: False\u001b[0m\n",
      "[\u001b[0;36mDEBUG\u001b[0m | src.tasks.clm_training.fabric.base]: \u001b[0;36mValidation condition check - epochFinished: False, trainingFinished: False\u001b[0m\n",
      "2025-06-16 13:54:40 - src.tasks.clm_training.fabric.base - \u001b[0;36mDEBUG\u001b[0m - \u001b[0;36mAbout to check: step_count % validate_after_k_steps == 0 -> 17 % 1000 == 0\u001b[0m\n",
      "[\u001b[0;36mDEBUG\u001b[0m | src.tasks.clm_training.fabric.base]: \u001b[0;36mAbout to check: step_count % validate_after_k_steps == 0 -> 17 % 1000 == 0\u001b[0m\n",
      "2025-06-16 13:54:40 - src.tasks.clm_training.fabric.base - \u001b[0;36mDEBUG\u001b[0m - \u001b[0;36miter 34 step 17: loss 6.1562, iter time: 419.11ms remaining time: \u001b[0m\n",
      "[\u001b[0;36mDEBUG\u001b[0m | src.tasks.clm_training.fabric.base]: \u001b[0;36miter 34 step 17: loss 6.1562, iter time: 419.11ms remaining time: \u001b[0m\n",
      "  1%|\u001b[34m▎                                        \u001b[0m| 34/4682 [00:14<32:47,  2.36it/s]\u001b[0m2025-06-16 13:54:40 - src.tasks.clm_training.fabric.base - \u001b[0;36mDEBUG\u001b[0m - \u001b[0;36mgradient_accumulation_steps raw value: 2, type: <class 'int'>\u001b[0m\n",
      "[\u001b[0;36mDEBUG\u001b[0m | src.tasks.clm_training.fabric.base]: \u001b[0;36mgradient_accumulation_steps raw value: 2, type: <class 'int'>\u001b[0m\n",
      "2025-06-16 13:54:40 - src.tasks.clm_training.fabric.base - \u001b[0;36mDEBUG\u001b[0m - \u001b[0;36mgradient_accumulation_steps converted: 2, type: <class 'int'>\u001b[0m\n",
      "[\u001b[0;36mDEBUG\u001b[0m | src.tasks.clm_training.fabric.base]: \u001b[0;36mgradient_accumulation_steps converted: 2, type: <class 'int'>\u001b[0m\n",
      "2025-06-16 13:54:40 - src.tasks.clm_training.fabric.base - \u001b[0;36mDEBUG\u001b[0m - \u001b[0;36miter_num value: 34, type: <class 'int'>\u001b[0m\n",
      "[\u001b[0;36mDEBUG\u001b[0m | src.tasks.clm_training.fabric.base]: \u001b[0;36miter_num value: 34, type: <class 'int'>\u001b[0m\n",
      "2025-06-16 13:54:40 - src.tasks.clm_training.fabric.base - \u001b[0;36mDEBUG\u001b[0m - \u001b[0;36mAbout to check: (iter_num + 1) % gradient_accumulation_steps != 0 -> (34 + 1) % 2 != 0\u001b[0m\n",
      "[\u001b[0;36mDEBUG\u001b[0m | src.tasks.clm_training.fabric.base]: \u001b[0;36mAbout to check: (iter_num + 1) % gradient_accumulation_steps != 0 -> (34 + 1) % 2 != 0\u001b[0m\n",
      "2025-06-16 13:54:41 - src.tasks.clm_training.fabric.base - \u001b[0;36mDEBUG\u001b[0m - \u001b[0;36miter 35 step 17: loss 5.7500, iter time: 121.51ms remaining time: \u001b[0m\n",
      "[\u001b[0;36mDEBUG\u001b[0m | src.tasks.clm_training.fabric.base]: \u001b[0;36miter 35 step 17: loss 5.7500, iter time: 121.51ms remaining time: \u001b[0m\n",
      "  1%|\u001b[34m▎                                        \u001b[0m| 35/4682 [00:15<32:34,  2.38it/s]\u001b[0m2025-06-16 13:54:41 - src.tasks.clm_training.fabric.base - \u001b[0;36mDEBUG\u001b[0m - \u001b[0;36mgradient_accumulation_steps raw value: 2, type: <class 'int'>\u001b[0m\n",
      "[\u001b[0;36mDEBUG\u001b[0m | src.tasks.clm_training.fabric.base]: \u001b[0;36mgradient_accumulation_steps raw value: 2, type: <class 'int'>\u001b[0m\n",
      "2025-06-16 13:54:41 - src.tasks.clm_training.fabric.base - \u001b[0;36mDEBUG\u001b[0m - \u001b[0;36mgradient_accumulation_steps converted: 2, type: <class 'int'>\u001b[0m\n",
      "[\u001b[0;36mDEBUG\u001b[0m | src.tasks.clm_training.fabric.base]: \u001b[0;36mgradient_accumulation_steps converted: 2, type: <class 'int'>\u001b[0m\n",
      "2025-06-16 13:54:41 - src.tasks.clm_training.fabric.base - \u001b[0;36mDEBUG\u001b[0m - \u001b[0;36miter_num value: 35, type: <class 'int'>\u001b[0m\n",
      "[\u001b[0;36mDEBUG\u001b[0m | src.tasks.clm_training.fabric.base]: \u001b[0;36miter_num value: 35, type: <class 'int'>\u001b[0m\n",
      "2025-06-16 13:54:41 - src.tasks.clm_training.fabric.base - \u001b[0;36mDEBUG\u001b[0m - \u001b[0;36mAbout to check: (iter_num + 1) % gradient_accumulation_steps != 0 -> (35 + 1) % 2 != 0\u001b[0m\n",
      "[\u001b[0;36mDEBUG\u001b[0m | src.tasks.clm_training.fabric.base]: \u001b[0;36mAbout to check: (iter_num + 1) % gradient_accumulation_steps != 0 -> (35 + 1) % 2 != 0\u001b[0m\n",
      "2025-06-16 13:54:41 - src.tasks.clm_training.fabric.base - \u001b[0;36mDEBUG\u001b[0m - \u001b[0;36mGradient norm before clipping: 24.3750\u001b[0m\n",
      "[\u001b[0;36mDEBUG\u001b[0m | src.tasks.clm_training.fabric.base]: \u001b[0;36mGradient norm before clipping: 24.3750\u001b[0m\n",
      "2025-06-16 13:54:41 - src.tasks.clm_training.fabric.base - \u001b[0;36mDEBUG\u001b[0m - \u001b[0;36mvalidate_after_k_steps value: 1000, type: <class 'int'>\u001b[0m\n",
      "[\u001b[0;36mDEBUG\u001b[0m | src.tasks.clm_training.fabric.base]: \u001b[0;36mvalidate_after_k_steps value: 1000, type: <class 'int'>\u001b[0m\n",
      "2025-06-16 13:54:41 - src.tasks.clm_training.fabric.base - \u001b[0;36mDEBUG\u001b[0m - \u001b[0;36mstep_count value: 18, type: <class 'int'>\u001b[0m\n",
      "[\u001b[0;36mDEBUG\u001b[0m | src.tasks.clm_training.fabric.base]: \u001b[0;36mstep_count value: 18, type: <class 'int'>\u001b[0m\n",
      "2025-06-16 13:54:41 - src.tasks.clm_training.fabric.base - \u001b[0;36mDEBUG\u001b[0m - \u001b[0;36mValidation condition check - epochFinished: False, trainingFinished: False\u001b[0m\n",
      "[\u001b[0;36mDEBUG\u001b[0m | src.tasks.clm_training.fabric.base]: \u001b[0;36mValidation condition check - epochFinished: False, trainingFinished: False\u001b[0m\n",
      "2025-06-16 13:54:41 - src.tasks.clm_training.fabric.base - \u001b[0;36mDEBUG\u001b[0m - \u001b[0;36mAbout to check: step_count % validate_after_k_steps == 0 -> 18 % 1000 == 0\u001b[0m\n",
      "[\u001b[0;36mDEBUG\u001b[0m | src.tasks.clm_training.fabric.base]: \u001b[0;36mAbout to check: step_count % validate_after_k_steps == 0 -> 18 % 1000 == 0\u001b[0m\n",
      "2025-06-16 13:54:41 - src.tasks.clm_training.fabric.base - \u001b[0;36mDEBUG\u001b[0m - \u001b[0;36miter 36 step 18: loss 5.5625, iter time: 419.56ms remaining time: \u001b[0m\n",
      "[\u001b[0;36mDEBUG\u001b[0m | src.tasks.clm_training.fabric.base]: \u001b[0;36miter 36 step 18: loss 5.5625, iter time: 419.56ms remaining time: \u001b[0m\n",
      "  1%|\u001b[34m▎                                        \u001b[0m| 36/4682 [00:15<32:47,  2.36it/s]\u001b[0m2025-06-16 13:54:41 - src.tasks.clm_training.fabric.base - \u001b[0;36mDEBUG\u001b[0m - \u001b[0;36mgradient_accumulation_steps raw value: 2, type: <class 'int'>\u001b[0m\n",
      "[\u001b[0;36mDEBUG\u001b[0m | src.tasks.clm_training.fabric.base]: \u001b[0;36mgradient_accumulation_steps raw value: 2, type: <class 'int'>\u001b[0m\n",
      "2025-06-16 13:54:41 - src.tasks.clm_training.fabric.base - \u001b[0;36mDEBUG\u001b[0m - \u001b[0;36mgradient_accumulation_steps converted: 2, type: <class 'int'>\u001b[0m\n",
      "[\u001b[0;36mDEBUG\u001b[0m | src.tasks.clm_training.fabric.base]: \u001b[0;36mgradient_accumulation_steps converted: 2, type: <class 'int'>\u001b[0m\n",
      "2025-06-16 13:54:41 - src.tasks.clm_training.fabric.base - \u001b[0;36mDEBUG\u001b[0m - \u001b[0;36miter_num value: 36, type: <class 'int'>\u001b[0m\n",
      "[\u001b[0;36mDEBUG\u001b[0m | src.tasks.clm_training.fabric.base]: \u001b[0;36miter_num value: 36, type: <class 'int'>\u001b[0m\n",
      "2025-06-16 13:54:41 - src.tasks.clm_training.fabric.base - \u001b[0;36mDEBUG\u001b[0m - \u001b[0;36mAbout to check: (iter_num + 1) % gradient_accumulation_steps != 0 -> (36 + 1) % 2 != 0\u001b[0m\n",
      "[\u001b[0;36mDEBUG\u001b[0m | src.tasks.clm_training.fabric.base]: \u001b[0;36mAbout to check: (iter_num + 1) % gradient_accumulation_steps != 0 -> (36 + 1) % 2 != 0\u001b[0m\n",
      "2025-06-16 13:54:41 - src.tasks.clm_training.fabric.base - \u001b[0;36mDEBUG\u001b[0m - \u001b[0;36miter 37 step 18: loss 5.5625, iter time: 121.80ms remaining time: \u001b[0m\n",
      "[\u001b[0;36mDEBUG\u001b[0m | src.tasks.clm_training.fabric.base]: \u001b[0;36miter 37 step 18: loss 5.5625, iter time: 121.80ms remaining time: \u001b[0m\n",
      "  1%|\u001b[34m▎                                        \u001b[0m| 37/4682 [00:15<32:34,  2.38it/s]\u001b[0m2025-06-16 13:54:41 - src.tasks.clm_training.fabric.base - \u001b[0;36mDEBUG\u001b[0m - \u001b[0;36mgradient_accumulation_steps raw value: 2, type: <class 'int'>\u001b[0m\n",
      "[\u001b[0;36mDEBUG\u001b[0m | src.tasks.clm_training.fabric.base]: \u001b[0;36mgradient_accumulation_steps raw value: 2, type: <class 'int'>\u001b[0m\n",
      "2025-06-16 13:54:41 - src.tasks.clm_training.fabric.base - \u001b[0;36mDEBUG\u001b[0m - \u001b[0;36mgradient_accumulation_steps converted: 2, type: <class 'int'>\u001b[0m\n",
      "[\u001b[0;36mDEBUG\u001b[0m | src.tasks.clm_training.fabric.base]: \u001b[0;36mgradient_accumulation_steps converted: 2, type: <class 'int'>\u001b[0m\n",
      "2025-06-16 13:54:41 - src.tasks.clm_training.fabric.base - \u001b[0;36mDEBUG\u001b[0m - \u001b[0;36miter_num value: 37, type: <class 'int'>\u001b[0m\n",
      "[\u001b[0;36mDEBUG\u001b[0m | src.tasks.clm_training.fabric.base]: \u001b[0;36miter_num value: 37, type: <class 'int'>\u001b[0m\n",
      "2025-06-16 13:54:41 - src.tasks.clm_training.fabric.base - \u001b[0;36mDEBUG\u001b[0m - \u001b[0;36mAbout to check: (iter_num + 1) % gradient_accumulation_steps != 0 -> (37 + 1) % 2 != 0\u001b[0m\n",
      "[\u001b[0;36mDEBUG\u001b[0m | src.tasks.clm_training.fabric.base]: \u001b[0;36mAbout to check: (iter_num + 1) % gradient_accumulation_steps != 0 -> (37 + 1) % 2 != 0\u001b[0m\n",
      "2025-06-16 13:54:42 - src.tasks.clm_training.fabric.base - \u001b[0;36mDEBUG\u001b[0m - \u001b[0;36mGradient norm before clipping: 22.2500\u001b[0m\n",
      "[\u001b[0;36mDEBUG\u001b[0m | src.tasks.clm_training.fabric.base]: \u001b[0;36mGradient norm before clipping: 22.2500\u001b[0m\n",
      "2025-06-16 13:54:42 - src.tasks.clm_training.fabric.base - \u001b[0;36mDEBUG\u001b[0m - \u001b[0;36mvalidate_after_k_steps value: 1000, type: <class 'int'>\u001b[0m\n",
      "[\u001b[0;36mDEBUG\u001b[0m | src.tasks.clm_training.fabric.base]: \u001b[0;36mvalidate_after_k_steps value: 1000, type: <class 'int'>\u001b[0m\n",
      "2025-06-16 13:54:42 - src.tasks.clm_training.fabric.base - \u001b[0;36mDEBUG\u001b[0m - \u001b[0;36mstep_count value: 19, type: <class 'int'>\u001b[0m\n",
      "[\u001b[0;36mDEBUG\u001b[0m | src.tasks.clm_training.fabric.base]: \u001b[0;36mstep_count value: 19, type: <class 'int'>\u001b[0m\n",
      "2025-06-16 13:54:42 - src.tasks.clm_training.fabric.base - \u001b[0;36mDEBUG\u001b[0m - \u001b[0;36mValidation condition check - epochFinished: False, trainingFinished: False\u001b[0m\n",
      "[\u001b[0;36mDEBUG\u001b[0m | src.tasks.clm_training.fabric.base]: \u001b[0;36mValidation condition check - epochFinished: False, trainingFinished: False\u001b[0m\n",
      "2025-06-16 13:54:42 - src.tasks.clm_training.fabric.base - \u001b[0;36mDEBUG\u001b[0m - \u001b[0;36mAbout to check: step_count % validate_after_k_steps == 0 -> 19 % 1000 == 0\u001b[0m\n",
      "[\u001b[0;36mDEBUG\u001b[0m | src.tasks.clm_training.fabric.base]: \u001b[0;36mAbout to check: step_count % validate_after_k_steps == 0 -> 19 % 1000 == 0\u001b[0m\n",
      "2025-06-16 13:54:42 - src.tasks.clm_training.fabric.base - \u001b[0;36mDEBUG\u001b[0m - \u001b[0;36miter 38 step 19: loss 5.8438, iter time: 420.14ms remaining time: \u001b[0m\n",
      "[\u001b[0;36mDEBUG\u001b[0m | src.tasks.clm_training.fabric.base]: \u001b[0;36miter 38 step 19: loss 5.8438, iter time: 420.14ms remaining time: \u001b[0m\n",
      "  1%|\u001b[34m▎                                        \u001b[0m| 38/4682 [00:16<32:47,  2.36it/s]\u001b[0m2025-06-16 13:54:42 - src.tasks.clm_training.fabric.base - \u001b[0;36mDEBUG\u001b[0m - \u001b[0;36mgradient_accumulation_steps raw value: 2, type: <class 'int'>\u001b[0m\n",
      "[\u001b[0;36mDEBUG\u001b[0m | src.tasks.clm_training.fabric.base]: \u001b[0;36mgradient_accumulation_steps raw value: 2, type: <class 'int'>\u001b[0m\n",
      "2025-06-16 13:54:42 - src.tasks.clm_training.fabric.base - \u001b[0;36mDEBUG\u001b[0m - \u001b[0;36mgradient_accumulation_steps converted: 2, type: <class 'int'>\u001b[0m\n",
      "[\u001b[0;36mDEBUG\u001b[0m | src.tasks.clm_training.fabric.base]: \u001b[0;36mgradient_accumulation_steps converted: 2, type: <class 'int'>\u001b[0m\n",
      "2025-06-16 13:54:42 - src.tasks.clm_training.fabric.base - \u001b[0;36mDEBUG\u001b[0m - \u001b[0;36miter_num value: 38, type: <class 'int'>\u001b[0m\n",
      "[\u001b[0;36mDEBUG\u001b[0m | src.tasks.clm_training.fabric.base]: \u001b[0;36miter_num value: 38, type: <class 'int'>\u001b[0m\n",
      "2025-06-16 13:54:42 - src.tasks.clm_training.fabric.base - \u001b[0;36mDEBUG\u001b[0m - \u001b[0;36mAbout to check: (iter_num + 1) % gradient_accumulation_steps != 0 -> (38 + 1) % 2 != 0\u001b[0m\n",
      "[\u001b[0;36mDEBUG\u001b[0m | src.tasks.clm_training.fabric.base]: \u001b[0;36mAbout to check: (iter_num + 1) % gradient_accumulation_steps != 0 -> (38 + 1) % 2 != 0\u001b[0m\n",
      "2025-06-16 13:54:42 - src.tasks.clm_training.fabric.base - \u001b[0;36mDEBUG\u001b[0m - \u001b[0;36miter 39 step 19: loss 5.7812, iter time: 121.76ms remaining time: \u001b[0m\n",
      "[\u001b[0;36mDEBUG\u001b[0m | src.tasks.clm_training.fabric.base]: \u001b[0;36miter 39 step 19: loss 5.7812, iter time: 121.76ms remaining time: \u001b[0m\n",
      "  1%|\u001b[34m▎                                        \u001b[0m| 39/4682 [00:16<32:33,  2.38it/s]\u001b[0m2025-06-16 13:54:42 - src.tasks.clm_training.fabric.base - \u001b[0;36mDEBUG\u001b[0m - \u001b[0;36mgradient_accumulation_steps raw value: 2, type: <class 'int'>\u001b[0m\n",
      "[\u001b[0;36mDEBUG\u001b[0m | src.tasks.clm_training.fabric.base]: \u001b[0;36mgradient_accumulation_steps raw value: 2, type: <class 'int'>\u001b[0m\n",
      "2025-06-16 13:54:42 - src.tasks.clm_training.fabric.base - \u001b[0;36mDEBUG\u001b[0m - \u001b[0;36mgradient_accumulation_steps converted: 2, type: <class 'int'>\u001b[0m\n",
      "[\u001b[0;36mDEBUG\u001b[0m | src.tasks.clm_training.fabric.base]: \u001b[0;36mgradient_accumulation_steps converted: 2, type: <class 'int'>\u001b[0m\n",
      "2025-06-16 13:54:42 - src.tasks.clm_training.fabric.base - \u001b[0;36mDEBUG\u001b[0m - \u001b[0;36miter_num value: 39, type: <class 'int'>\u001b[0m\n",
      "[\u001b[0;36mDEBUG\u001b[0m | src.tasks.clm_training.fabric.base]: \u001b[0;36miter_num value: 39, type: <class 'int'>\u001b[0m\n",
      "2025-06-16 13:54:42 - src.tasks.clm_training.fabric.base - \u001b[0;36mDEBUG\u001b[0m - \u001b[0;36mAbout to check: (iter_num + 1) % gradient_accumulation_steps != 0 -> (39 + 1) % 2 != 0\u001b[0m\n",
      "[\u001b[0;36mDEBUG\u001b[0m | src.tasks.clm_training.fabric.base]: \u001b[0;36mAbout to check: (iter_num + 1) % gradient_accumulation_steps != 0 -> (39 + 1) % 2 != 0\u001b[0m\n",
      "2025-06-16 13:54:43 - src.tasks.clm_training.fabric.base - \u001b[0;36mDEBUG\u001b[0m - \u001b[0;36mGradient norm before clipping: 26.5000\u001b[0m\n",
      "[\u001b[0;36mDEBUG\u001b[0m | src.tasks.clm_training.fabric.base]: \u001b[0;36mGradient norm before clipping: 26.5000\u001b[0m\n",
      "2025-06-16 13:54:43 - src.tasks.clm_training.fabric.base - \u001b[0;36mDEBUG\u001b[0m - \u001b[0;36mvalidate_after_k_steps value: 1000, type: <class 'int'>\u001b[0m\n",
      "[\u001b[0;36mDEBUG\u001b[0m | src.tasks.clm_training.fabric.base]: \u001b[0;36mvalidate_after_k_steps value: 1000, type: <class 'int'>\u001b[0m\n",
      "2025-06-16 13:54:43 - src.tasks.clm_training.fabric.base - \u001b[0;36mDEBUG\u001b[0m - \u001b[0;36mstep_count value: 20, type: <class 'int'>\u001b[0m\n",
      "[\u001b[0;36mDEBUG\u001b[0m | src.tasks.clm_training.fabric.base]: \u001b[0;36mstep_count value: 20, type: <class 'int'>\u001b[0m\n",
      "2025-06-16 13:54:43 - src.tasks.clm_training.fabric.base - \u001b[0;36mDEBUG\u001b[0m - \u001b[0;36mValidation condition check - epochFinished: False, trainingFinished: False\u001b[0m\n",
      "[\u001b[0;36mDEBUG\u001b[0m | src.tasks.clm_training.fabric.base]: \u001b[0;36mValidation condition check - epochFinished: False, trainingFinished: False\u001b[0m\n",
      "2025-06-16 13:54:43 - src.tasks.clm_training.fabric.base - \u001b[0;36mDEBUG\u001b[0m - \u001b[0;36mAbout to check: step_count % validate_after_k_steps == 0 -> 20 % 1000 == 0\u001b[0m\n",
      "[\u001b[0;36mDEBUG\u001b[0m | src.tasks.clm_training.fabric.base]: \u001b[0;36mAbout to check: step_count % validate_after_k_steps == 0 -> 20 % 1000 == 0\u001b[0m\n",
      "2025-06-16 13:54:43 - src.tasks.clm_training.fabric.base - \u001b[0;36mDEBUG\u001b[0m - \u001b[0;36miter 40 step 20: loss 6.0625, iter time: 418.59ms remaining time: \u001b[0m\n",
      "[\u001b[0;36mDEBUG\u001b[0m | src.tasks.clm_training.fabric.base]: \u001b[0;36miter 40 step 20: loss 6.0625, iter time: 418.59ms remaining time: \u001b[0m\n",
      "  1%|\u001b[34m▎                                        \u001b[0m| 40/4682 [00:17<32:44,  2.36it/s]\u001b[0m2025-06-16 13:54:43 - src.tasks.clm_training.fabric.base - \u001b[0;36mDEBUG\u001b[0m - \u001b[0;36mgradient_accumulation_steps raw value: 2, type: <class 'int'>\u001b[0m\n",
      "[\u001b[0;36mDEBUG\u001b[0m | src.tasks.clm_training.fabric.base]: \u001b[0;36mgradient_accumulation_steps raw value: 2, type: <class 'int'>\u001b[0m\n",
      "2025-06-16 13:54:43 - src.tasks.clm_training.fabric.base - \u001b[0;36mDEBUG\u001b[0m - \u001b[0;36mgradient_accumulation_steps converted: 2, type: <class 'int'>\u001b[0m\n",
      "[\u001b[0;36mDEBUG\u001b[0m | src.tasks.clm_training.fabric.base]: \u001b[0;36mgradient_accumulation_steps converted: 2, type: <class 'int'>\u001b[0m\n",
      "2025-06-16 13:54:43 - src.tasks.clm_training.fabric.base - \u001b[0;36mDEBUG\u001b[0m - \u001b[0;36miter_num value: 40, type: <class 'int'>\u001b[0m\n",
      "[\u001b[0;36mDEBUG\u001b[0m | src.tasks.clm_training.fabric.base]: \u001b[0;36miter_num value: 40, type: <class 'int'>\u001b[0m\n",
      "2025-06-16 13:54:43 - src.tasks.clm_training.fabric.base - \u001b[0;36mDEBUG\u001b[0m - \u001b[0;36mAbout to check: (iter_num + 1) % gradient_accumulation_steps != 0 -> (40 + 1) % 2 != 0\u001b[0m\n",
      "[\u001b[0;36mDEBUG\u001b[0m | src.tasks.clm_training.fabric.base]: \u001b[0;36mAbout to check: (iter_num + 1) % gradient_accumulation_steps != 0 -> (40 + 1) % 2 != 0\u001b[0m\n",
      "2025-06-16 13:54:43 - src.tasks.clm_training.fabric.base - \u001b[0;36mDEBUG\u001b[0m - \u001b[0;36miter 41 step 20: loss 5.8125, iter time: 121.94ms remaining time: \u001b[0m\n",
      "[\u001b[0;36mDEBUG\u001b[0m | src.tasks.clm_training.fabric.base]: \u001b[0;36miter 41 step 20: loss 5.8125, iter time: 121.94ms remaining time: \u001b[0m\n",
      "  1%|\u001b[34m▎                                        \u001b[0m| 41/4682 [00:17<32:34,  2.37it/s]\u001b[0m2025-06-16 13:54:43 - src.tasks.clm_training.fabric.base - \u001b[0;36mDEBUG\u001b[0m - \u001b[0;36mgradient_accumulation_steps raw value: 2, type: <class 'int'>\u001b[0m\n",
      "[\u001b[0;36mDEBUG\u001b[0m | src.tasks.clm_training.fabric.base]: \u001b[0;36mgradient_accumulation_steps raw value: 2, type: <class 'int'>\u001b[0m\n",
      "2025-06-16 13:54:43 - src.tasks.clm_training.fabric.base - \u001b[0;36mDEBUG\u001b[0m - \u001b[0;36mgradient_accumulation_steps converted: 2, type: <class 'int'>\u001b[0m\n",
      "[\u001b[0;36mDEBUG\u001b[0m | src.tasks.clm_training.fabric.base]: \u001b[0;36mgradient_accumulation_steps converted: 2, type: <class 'int'>\u001b[0m\n",
      "2025-06-16 13:54:43 - src.tasks.clm_training.fabric.base - \u001b[0;36mDEBUG\u001b[0m - \u001b[0;36miter_num value: 41, type: <class 'int'>\u001b[0m\n",
      "[\u001b[0;36mDEBUG\u001b[0m | src.tasks.clm_training.fabric.base]: \u001b[0;36miter_num value: 41, type: <class 'int'>\u001b[0m\n",
      "2025-06-16 13:54:43 - src.tasks.clm_training.fabric.base - \u001b[0;36mDEBUG\u001b[0m - \u001b[0;36mAbout to check: (iter_num + 1) % gradient_accumulation_steps != 0 -> (41 + 1) % 2 != 0\u001b[0m\n",
      "[\u001b[0;36mDEBUG\u001b[0m | src.tasks.clm_training.fabric.base]: \u001b[0;36mAbout to check: (iter_num + 1) % gradient_accumulation_steps != 0 -> (41 + 1) % 2 != 0\u001b[0m\n",
      "2025-06-16 13:54:44 - src.tasks.clm_training.fabric.base - \u001b[0;36mDEBUG\u001b[0m - \u001b[0;36mGradient norm before clipping: 28.3750\u001b[0m\n",
      "[\u001b[0;36mDEBUG\u001b[0m | src.tasks.clm_training.fabric.base]: \u001b[0;36mGradient norm before clipping: 28.3750\u001b[0m\n",
      "2025-06-16 13:54:44 - src.tasks.clm_training.fabric.base - \u001b[0;36mDEBUG\u001b[0m - \u001b[0;36mvalidate_after_k_steps value: 1000, type: <class 'int'>\u001b[0m\n",
      "[\u001b[0;36mDEBUG\u001b[0m | src.tasks.clm_training.fabric.base]: \u001b[0;36mvalidate_after_k_steps value: 1000, type: <class 'int'>\u001b[0m\n",
      "2025-06-16 13:54:44 - src.tasks.clm_training.fabric.base - \u001b[0;36mDEBUG\u001b[0m - \u001b[0;36mstep_count value: 21, type: <class 'int'>\u001b[0m\n",
      "[\u001b[0;36mDEBUG\u001b[0m | src.tasks.clm_training.fabric.base]: \u001b[0;36mstep_count value: 21, type: <class 'int'>\u001b[0m\n",
      "2025-06-16 13:54:44 - src.tasks.clm_training.fabric.base - \u001b[0;36mDEBUG\u001b[0m - \u001b[0;36mValidation condition check - epochFinished: False, trainingFinished: False\u001b[0m\n",
      "[\u001b[0;36mDEBUG\u001b[0m | src.tasks.clm_training.fabric.base]: \u001b[0;36mValidation condition check - epochFinished: False, trainingFinished: False\u001b[0m\n",
      "2025-06-16 13:54:44 - src.tasks.clm_training.fabric.base - \u001b[0;36mDEBUG\u001b[0m - \u001b[0;36mAbout to check: step_count % validate_after_k_steps == 0 -> 21 % 1000 == 0\u001b[0m\n",
      "[\u001b[0;36mDEBUG\u001b[0m | src.tasks.clm_training.fabric.base]: \u001b[0;36mAbout to check: step_count % validate_after_k_steps == 0 -> 21 % 1000 == 0\u001b[0m\n",
      "2025-06-16 13:54:44 - src.tasks.clm_training.fabric.base - \u001b[0;36mDEBUG\u001b[0m - \u001b[0;36miter 42 step 21: loss 6.4375, iter time: 420.18ms remaining time: \u001b[0m\n",
      "[\u001b[0;36mDEBUG\u001b[0m | src.tasks.clm_training.fabric.base]: \u001b[0;36miter 42 step 21: loss 6.4375, iter time: 420.18ms remaining time: \u001b[0m\n",
      "  1%|\u001b[34m▎                                        \u001b[0m| 42/4682 [00:18<32:46,  2.36it/s]\u001b[0m2025-06-16 13:54:44 - src.tasks.clm_training.fabric.base - \u001b[0;36mDEBUG\u001b[0m - \u001b[0;36mgradient_accumulation_steps raw value: 2, type: <class 'int'>\u001b[0m\n",
      "[\u001b[0;36mDEBUG\u001b[0m | src.tasks.clm_training.fabric.base]: \u001b[0;36mgradient_accumulation_steps raw value: 2, type: <class 'int'>\u001b[0m\n",
      "2025-06-16 13:54:44 - src.tasks.clm_training.fabric.base - \u001b[0;36mDEBUG\u001b[0m - \u001b[0;36mgradient_accumulation_steps converted: 2, type: <class 'int'>\u001b[0m\n",
      "[\u001b[0;36mDEBUG\u001b[0m | src.tasks.clm_training.fabric.base]: \u001b[0;36mgradient_accumulation_steps converted: 2, type: <class 'int'>\u001b[0m\n",
      "2025-06-16 13:54:44 - src.tasks.clm_training.fabric.base - \u001b[0;36mDEBUG\u001b[0m - \u001b[0;36miter_num value: 42, type: <class 'int'>\u001b[0m\n",
      "[\u001b[0;36mDEBUG\u001b[0m | src.tasks.clm_training.fabric.base]: \u001b[0;36miter_num value: 42, type: <class 'int'>\u001b[0m\n",
      "2025-06-16 13:54:44 - src.tasks.clm_training.fabric.base - \u001b[0;36mDEBUG\u001b[0m - \u001b[0;36mAbout to check: (iter_num + 1) % gradient_accumulation_steps != 0 -> (42 + 1) % 2 != 0\u001b[0m\n",
      "[\u001b[0;36mDEBUG\u001b[0m | src.tasks.clm_training.fabric.base]: \u001b[0;36mAbout to check: (iter_num + 1) % gradient_accumulation_steps != 0 -> (42 + 1) % 2 != 0\u001b[0m\n",
      "2025-06-16 13:54:44 - src.tasks.clm_training.fabric.base - \u001b[0;36mDEBUG\u001b[0m - \u001b[0;36miter 43 step 21: loss 5.9375, iter time: 121.74ms remaining time: \u001b[0m\n",
      "[\u001b[0;36mDEBUG\u001b[0m | src.tasks.clm_training.fabric.base]: \u001b[0;36miter 43 step 21: loss 5.9375, iter time: 121.74ms remaining time: \u001b[0m\n",
      "  1%|\u001b[34m▍                                        \u001b[0m| 43/4682 [00:18<32:34,  2.37it/s]\u001b[0m2025-06-16 13:54:44 - src.tasks.clm_training.fabric.base - \u001b[0;36mDEBUG\u001b[0m - \u001b[0;36mgradient_accumulation_steps raw value: 2, type: <class 'int'>\u001b[0m\n",
      "[\u001b[0;36mDEBUG\u001b[0m | src.tasks.clm_training.fabric.base]: \u001b[0;36mgradient_accumulation_steps raw value: 2, type: <class 'int'>\u001b[0m\n",
      "2025-06-16 13:54:44 - src.tasks.clm_training.fabric.base - \u001b[0;36mDEBUG\u001b[0m - \u001b[0;36mgradient_accumulation_steps converted: 2, type: <class 'int'>\u001b[0m\n",
      "[\u001b[0;36mDEBUG\u001b[0m | src.tasks.clm_training.fabric.base]: \u001b[0;36mgradient_accumulation_steps converted: 2, type: <class 'int'>\u001b[0m\n",
      "2025-06-16 13:54:44 - src.tasks.clm_training.fabric.base - \u001b[0;36mDEBUG\u001b[0m - \u001b[0;36miter_num value: 43, type: <class 'int'>\u001b[0m\n",
      "[\u001b[0;36mDEBUG\u001b[0m | src.tasks.clm_training.fabric.base]: \u001b[0;36miter_num value: 43, type: <class 'int'>\u001b[0m\n",
      "2025-06-16 13:54:44 - src.tasks.clm_training.fabric.base - \u001b[0;36mDEBUG\u001b[0m - \u001b[0;36mAbout to check: (iter_num + 1) % gradient_accumulation_steps != 0 -> (43 + 1) % 2 != 0\u001b[0m\n",
      "[\u001b[0;36mDEBUG\u001b[0m | src.tasks.clm_training.fabric.base]: \u001b[0;36mAbout to check: (iter_num + 1) % gradient_accumulation_steps != 0 -> (43 + 1) % 2 != 0\u001b[0m\n",
      "2025-06-16 13:54:44 - src.tasks.clm_training.fabric.base - \u001b[0;36mDEBUG\u001b[0m - \u001b[0;36mGradient norm before clipping: 22.8750\u001b[0m\n",
      "[\u001b[0;36mDEBUG\u001b[0m | src.tasks.clm_training.fabric.base]: \u001b[0;36mGradient norm before clipping: 22.8750\u001b[0m\n",
      "2025-06-16 13:54:44 - src.tasks.clm_training.fabric.base - \u001b[0;36mDEBUG\u001b[0m - \u001b[0;36mvalidate_after_k_steps value: 1000, type: <class 'int'>\u001b[0m\n",
      "[\u001b[0;36mDEBUG\u001b[0m | src.tasks.clm_training.fabric.base]: \u001b[0;36mvalidate_after_k_steps value: 1000, type: <class 'int'>\u001b[0m\n",
      "2025-06-16 13:54:44 - src.tasks.clm_training.fabric.base - \u001b[0;36mDEBUG\u001b[0m - \u001b[0;36mstep_count value: 22, type: <class 'int'>\u001b[0m\n",
      "[\u001b[0;36mDEBUG\u001b[0m | src.tasks.clm_training.fabric.base]: \u001b[0;36mstep_count value: 22, type: <class 'int'>\u001b[0m\n",
      "2025-06-16 13:54:44 - src.tasks.clm_training.fabric.base - \u001b[0;36mDEBUG\u001b[0m - \u001b[0;36mValidation condition check - epochFinished: False, trainingFinished: False\u001b[0m\n",
      "[\u001b[0;36mDEBUG\u001b[0m | src.tasks.clm_training.fabric.base]: \u001b[0;36mValidation condition check - epochFinished: False, trainingFinished: False\u001b[0m\n",
      "2025-06-16 13:54:44 - src.tasks.clm_training.fabric.base - \u001b[0;36mDEBUG\u001b[0m - \u001b[0;36mAbout to check: step_count % validate_after_k_steps == 0 -> 22 % 1000 == 0\u001b[0m\n",
      "[\u001b[0;36mDEBUG\u001b[0m | src.tasks.clm_training.fabric.base]: \u001b[0;36mAbout to check: step_count % validate_after_k_steps == 0 -> 22 % 1000 == 0\u001b[0m\n",
      "2025-06-16 13:54:44 - src.tasks.clm_training.fabric.base - \u001b[0;36mDEBUG\u001b[0m - \u001b[0;36miter 44 step 22: loss 5.6250, iter time: 419.96ms remaining time: \u001b[0m\n",
      "[\u001b[0;36mDEBUG\u001b[0m | src.tasks.clm_training.fabric.base]: \u001b[0;36miter 44 step 22: loss 5.6250, iter time: 419.96ms remaining time: \u001b[0m\n",
      "  1%|\u001b[34m▍                                        \u001b[0m| 44/4682 [00:18<32:46,  2.36it/s]\u001b[0m2025-06-16 13:54:44 - src.tasks.clm_training.fabric.base - \u001b[0;36mDEBUG\u001b[0m - \u001b[0;36mgradient_accumulation_steps raw value: 2, type: <class 'int'>\u001b[0m\n",
      "[\u001b[0;36mDEBUG\u001b[0m | src.tasks.clm_training.fabric.base]: \u001b[0;36mgradient_accumulation_steps raw value: 2, type: <class 'int'>\u001b[0m\n",
      "2025-06-16 13:54:44 - src.tasks.clm_training.fabric.base - \u001b[0;36mDEBUG\u001b[0m - \u001b[0;36mgradient_accumulation_steps converted: 2, type: <class 'int'>\u001b[0m\n",
      "[\u001b[0;36mDEBUG\u001b[0m | src.tasks.clm_training.fabric.base]: \u001b[0;36mgradient_accumulation_steps converted: 2, type: <class 'int'>\u001b[0m\n",
      "2025-06-16 13:54:44 - src.tasks.clm_training.fabric.base - \u001b[0;36mDEBUG\u001b[0m - \u001b[0;36miter_num value: 44, type: <class 'int'>\u001b[0m\n",
      "[\u001b[0;36mDEBUG\u001b[0m | src.tasks.clm_training.fabric.base]: \u001b[0;36miter_num value: 44, type: <class 'int'>\u001b[0m\n",
      "2025-06-16 13:54:44 - src.tasks.clm_training.fabric.base - \u001b[0;36mDEBUG\u001b[0m - \u001b[0;36mAbout to check: (iter_num + 1) % gradient_accumulation_steps != 0 -> (44 + 1) % 2 != 0\u001b[0m\n",
      "[\u001b[0;36mDEBUG\u001b[0m | src.tasks.clm_training.fabric.base]: \u001b[0;36mAbout to check: (iter_num + 1) % gradient_accumulation_steps != 0 -> (44 + 1) % 2 != 0\u001b[0m\n",
      "2025-06-16 13:54:45 - src.tasks.clm_training.fabric.base - \u001b[0;36mDEBUG\u001b[0m - \u001b[0;36miter 45 step 22: loss 5.6875, iter time: 121.73ms remaining time: \u001b[0m\n",
      "[\u001b[0;36mDEBUG\u001b[0m | src.tasks.clm_training.fabric.base]: \u001b[0;36miter 45 step 22: loss 5.6875, iter time: 121.73ms remaining time: \u001b[0m\n",
      "  1%|\u001b[34m▍                                        \u001b[0m| 45/4682 [00:19<32:32,  2.37it/s]\u001b[0m2025-06-16 13:54:45 - src.tasks.clm_training.fabric.base - \u001b[0;36mDEBUG\u001b[0m - \u001b[0;36mgradient_accumulation_steps raw value: 2, type: <class 'int'>\u001b[0m\n",
      "[\u001b[0;36mDEBUG\u001b[0m | src.tasks.clm_training.fabric.base]: \u001b[0;36mgradient_accumulation_steps raw value: 2, type: <class 'int'>\u001b[0m\n",
      "2025-06-16 13:54:45 - src.tasks.clm_training.fabric.base - \u001b[0;36mDEBUG\u001b[0m - \u001b[0;36mgradient_accumulation_steps converted: 2, type: <class 'int'>\u001b[0m\n",
      "[\u001b[0;36mDEBUG\u001b[0m | src.tasks.clm_training.fabric.base]: \u001b[0;36mgradient_accumulation_steps converted: 2, type: <class 'int'>\u001b[0m\n",
      "2025-06-16 13:54:45 - src.tasks.clm_training.fabric.base - \u001b[0;36mDEBUG\u001b[0m - \u001b[0;36miter_num value: 45, type: <class 'int'>\u001b[0m\n",
      "[\u001b[0;36mDEBUG\u001b[0m | src.tasks.clm_training.fabric.base]: \u001b[0;36miter_num value: 45, type: <class 'int'>\u001b[0m\n",
      "2025-06-16 13:54:45 - src.tasks.clm_training.fabric.base - \u001b[0;36mDEBUG\u001b[0m - \u001b[0;36mAbout to check: (iter_num + 1) % gradient_accumulation_steps != 0 -> (45 + 1) % 2 != 0\u001b[0m\n",
      "[\u001b[0;36mDEBUG\u001b[0m | src.tasks.clm_training.fabric.base]: \u001b[0;36mAbout to check: (iter_num + 1) % gradient_accumulation_steps != 0 -> (45 + 1) % 2 != 0\u001b[0m\n",
      "2025-06-16 13:54:45 - src.tasks.clm_training.fabric.base - \u001b[0;36mDEBUG\u001b[0m - \u001b[0;36mGradient norm before clipping: 21.8750\u001b[0m\n",
      "[\u001b[0;36mDEBUG\u001b[0m | src.tasks.clm_training.fabric.base]: \u001b[0;36mGradient norm before clipping: 21.8750\u001b[0m\n",
      "2025-06-16 13:54:45 - src.tasks.clm_training.fabric.base - \u001b[0;36mDEBUG\u001b[0m - \u001b[0;36mvalidate_after_k_steps value: 1000, type: <class 'int'>\u001b[0m\n",
      "[\u001b[0;36mDEBUG\u001b[0m | src.tasks.clm_training.fabric.base]: \u001b[0;36mvalidate_after_k_steps value: 1000, type: <class 'int'>\u001b[0m\n",
      "2025-06-16 13:54:45 - src.tasks.clm_training.fabric.base - \u001b[0;36mDEBUG\u001b[0m - \u001b[0;36mstep_count value: 23, type: <class 'int'>\u001b[0m\n",
      "[\u001b[0;36mDEBUG\u001b[0m | src.tasks.clm_training.fabric.base]: \u001b[0;36mstep_count value: 23, type: <class 'int'>\u001b[0m\n",
      "2025-06-16 13:54:45 - src.tasks.clm_training.fabric.base - \u001b[0;36mDEBUG\u001b[0m - \u001b[0;36mValidation condition check - epochFinished: False, trainingFinished: False\u001b[0m\n",
      "[\u001b[0;36mDEBUG\u001b[0m | src.tasks.clm_training.fabric.base]: \u001b[0;36mValidation condition check - epochFinished: False, trainingFinished: False\u001b[0m\n",
      "2025-06-16 13:54:45 - src.tasks.clm_training.fabric.base - \u001b[0;36mDEBUG\u001b[0m - \u001b[0;36mAbout to check: step_count % validate_after_k_steps == 0 -> 23 % 1000 == 0\u001b[0m\n",
      "[\u001b[0;36mDEBUG\u001b[0m | src.tasks.clm_training.fabric.base]: \u001b[0;36mAbout to check: step_count % validate_after_k_steps == 0 -> 23 % 1000 == 0\u001b[0m\n",
      "2025-06-16 13:54:45 - src.tasks.clm_training.fabric.base - \u001b[0;36mDEBUG\u001b[0m - \u001b[0;36miter 46 step 23: loss 5.8750, iter time: 419.68ms remaining time: \u001b[0m\n",
      "[\u001b[0;36mDEBUG\u001b[0m | src.tasks.clm_training.fabric.base]: \u001b[0;36miter 46 step 23: loss 5.8750, iter time: 419.68ms remaining time: \u001b[0m\n",
      "  1%|\u001b[34m▍                                        \u001b[0m| 46/4682 [00:19<32:43,  2.36it/s]\u001b[0m2025-06-16 13:54:45 - src.tasks.clm_training.fabric.base - \u001b[0;36mDEBUG\u001b[0m - \u001b[0;36mgradient_accumulation_steps raw value: 2, type: <class 'int'>\u001b[0m\n",
      "[\u001b[0;36mDEBUG\u001b[0m | src.tasks.clm_training.fabric.base]: \u001b[0;36mgradient_accumulation_steps raw value: 2, type: <class 'int'>\u001b[0m\n",
      "2025-06-16 13:54:45 - src.tasks.clm_training.fabric.base - \u001b[0;36mDEBUG\u001b[0m - \u001b[0;36mgradient_accumulation_steps converted: 2, type: <class 'int'>\u001b[0m\n",
      "[\u001b[0;36mDEBUG\u001b[0m | src.tasks.clm_training.fabric.base]: \u001b[0;36mgradient_accumulation_steps converted: 2, type: <class 'int'>\u001b[0m\n",
      "2025-06-16 13:54:45 - src.tasks.clm_training.fabric.base - \u001b[0;36mDEBUG\u001b[0m - \u001b[0;36miter_num value: 46, type: <class 'int'>\u001b[0m\n",
      "[\u001b[0;36mDEBUG\u001b[0m | src.tasks.clm_training.fabric.base]: \u001b[0;36miter_num value: 46, type: <class 'int'>\u001b[0m\n",
      "2025-06-16 13:54:45 - src.tasks.clm_training.fabric.base - \u001b[0;36mDEBUG\u001b[0m - \u001b[0;36mAbout to check: (iter_num + 1) % gradient_accumulation_steps != 0 -> (46 + 1) % 2 != 0\u001b[0m\n",
      "[\u001b[0;36mDEBUG\u001b[0m | src.tasks.clm_training.fabric.base]: \u001b[0;36mAbout to check: (iter_num + 1) % gradient_accumulation_steps != 0 -> (46 + 1) % 2 != 0\u001b[0m\n",
      "2025-06-16 13:54:46 - src.tasks.clm_training.fabric.base - \u001b[0;36mDEBUG\u001b[0m - \u001b[0;36miter 47 step 23: loss 5.5000, iter time: 122.00ms remaining time: \u001b[0m\n",
      "[\u001b[0;36mDEBUG\u001b[0m | src.tasks.clm_training.fabric.base]: \u001b[0;36miter 47 step 23: loss 5.5000, iter time: 122.00ms remaining time: \u001b[0m\n",
      "  1%|\u001b[34m▍                                        \u001b[0m| 47/4682 [00:20<32:31,  2.37it/s]\u001b[0m2025-06-16 13:54:46 - src.tasks.clm_training.fabric.base - \u001b[0;36mDEBUG\u001b[0m - \u001b[0;36mgradient_accumulation_steps raw value: 2, type: <class 'int'>\u001b[0m\n",
      "[\u001b[0;36mDEBUG\u001b[0m | src.tasks.clm_training.fabric.base]: \u001b[0;36mgradient_accumulation_steps raw value: 2, type: <class 'int'>\u001b[0m\n",
      "2025-06-16 13:54:46 - src.tasks.clm_training.fabric.base - \u001b[0;36mDEBUG\u001b[0m - \u001b[0;36mgradient_accumulation_steps converted: 2, type: <class 'int'>\u001b[0m\n",
      "[\u001b[0;36mDEBUG\u001b[0m | src.tasks.clm_training.fabric.base]: \u001b[0;36mgradient_accumulation_steps converted: 2, type: <class 'int'>\u001b[0m\n",
      "2025-06-16 13:54:46 - src.tasks.clm_training.fabric.base - \u001b[0;36mDEBUG\u001b[0m - \u001b[0;36miter_num value: 47, type: <class 'int'>\u001b[0m\n",
      "[\u001b[0;36mDEBUG\u001b[0m | src.tasks.clm_training.fabric.base]: \u001b[0;36miter_num value: 47, type: <class 'int'>\u001b[0m\n",
      "2025-06-16 13:54:46 - src.tasks.clm_training.fabric.base - \u001b[0;36mDEBUG\u001b[0m - \u001b[0;36mAbout to check: (iter_num + 1) % gradient_accumulation_steps != 0 -> (47 + 1) % 2 != 0\u001b[0m\n",
      "[\u001b[0;36mDEBUG\u001b[0m | src.tasks.clm_training.fabric.base]: \u001b[0;36mAbout to check: (iter_num + 1) % gradient_accumulation_steps != 0 -> (47 + 1) % 2 != 0\u001b[0m\n",
      "2025-06-16 13:54:46 - src.tasks.clm_training.fabric.base - \u001b[0;36mDEBUG\u001b[0m - \u001b[0;36mGradient norm before clipping: 25.6250\u001b[0m\n",
      "[\u001b[0;36mDEBUG\u001b[0m | src.tasks.clm_training.fabric.base]: \u001b[0;36mGradient norm before clipping: 25.6250\u001b[0m\n",
      "2025-06-16 13:54:46 - src.tasks.clm_training.fabric.base - \u001b[0;36mDEBUG\u001b[0m - \u001b[0;36mvalidate_after_k_steps value: 1000, type: <class 'int'>\u001b[0m\n",
      "[\u001b[0;36mDEBUG\u001b[0m | src.tasks.clm_training.fabric.base]: \u001b[0;36mvalidate_after_k_steps value: 1000, type: <class 'int'>\u001b[0m\n",
      "2025-06-16 13:54:46 - src.tasks.clm_training.fabric.base - \u001b[0;36mDEBUG\u001b[0m - \u001b[0;36mstep_count value: 24, type: <class 'int'>\u001b[0m\n",
      "[\u001b[0;36mDEBUG\u001b[0m | src.tasks.clm_training.fabric.base]: \u001b[0;36mstep_count value: 24, type: <class 'int'>\u001b[0m\n",
      "2025-06-16 13:54:46 - src.tasks.clm_training.fabric.base - \u001b[0;36mDEBUG\u001b[0m - \u001b[0;36mValidation condition check - epochFinished: False, trainingFinished: False\u001b[0m\n",
      "[\u001b[0;36mDEBUG\u001b[0m | src.tasks.clm_training.fabric.base]: \u001b[0;36mValidation condition check - epochFinished: False, trainingFinished: False\u001b[0m\n",
      "2025-06-16 13:54:46 - src.tasks.clm_training.fabric.base - \u001b[0;36mDEBUG\u001b[0m - \u001b[0;36mAbout to check: step_count % validate_after_k_steps == 0 -> 24 % 1000 == 0\u001b[0m\n",
      "[\u001b[0;36mDEBUG\u001b[0m | src.tasks.clm_training.fabric.base]: \u001b[0;36mAbout to check: step_count % validate_after_k_steps == 0 -> 24 % 1000 == 0\u001b[0m\n",
      "2025-06-16 13:54:46 - src.tasks.clm_training.fabric.base - \u001b[0;36mDEBUG\u001b[0m - \u001b[0;36miter 48 step 24: loss 6.0625, iter time: 419.19ms remaining time: \u001b[0m\n",
      "[\u001b[0;36mDEBUG\u001b[0m | src.tasks.clm_training.fabric.base]: \u001b[0;36miter 48 step 24: loss 6.0625, iter time: 419.19ms remaining time: \u001b[0m\n",
      "  1%|\u001b[34m▍                                        \u001b[0m| 48/4682 [00:20<32:42,  2.36it/s]\u001b[0m2025-06-16 13:54:46 - src.tasks.clm_training.fabric.base - \u001b[0;36mDEBUG\u001b[0m - \u001b[0;36mgradient_accumulation_steps raw value: 2, type: <class 'int'>\u001b[0m\n",
      "[\u001b[0;36mDEBUG\u001b[0m | src.tasks.clm_training.fabric.base]: \u001b[0;36mgradient_accumulation_steps raw value: 2, type: <class 'int'>\u001b[0m\n",
      "2025-06-16 13:54:46 - src.tasks.clm_training.fabric.base - \u001b[0;36mDEBUG\u001b[0m - \u001b[0;36mgradient_accumulation_steps converted: 2, type: <class 'int'>\u001b[0m\n",
      "[\u001b[0;36mDEBUG\u001b[0m | src.tasks.clm_training.fabric.base]: \u001b[0;36mgradient_accumulation_steps converted: 2, type: <class 'int'>\u001b[0m\n",
      "2025-06-16 13:54:46 - src.tasks.clm_training.fabric.base - \u001b[0;36mDEBUG\u001b[0m - \u001b[0;36miter_num value: 48, type: <class 'int'>\u001b[0m\n",
      "[\u001b[0;36mDEBUG\u001b[0m | src.tasks.clm_training.fabric.base]: \u001b[0;36miter_num value: 48, type: <class 'int'>\u001b[0m\n",
      "2025-06-16 13:54:46 - src.tasks.clm_training.fabric.base - \u001b[0;36mDEBUG\u001b[0m - \u001b[0;36mAbout to check: (iter_num + 1) % gradient_accumulation_steps != 0 -> (48 + 1) % 2 != 0\u001b[0m\n",
      "[\u001b[0;36mDEBUG\u001b[0m | src.tasks.clm_training.fabric.base]: \u001b[0;36mAbout to check: (iter_num + 1) % gradient_accumulation_steps != 0 -> (48 + 1) % 2 != 0\u001b[0m\n",
      "2025-06-16 13:54:47 - src.tasks.clm_training.fabric.base - \u001b[0;36mDEBUG\u001b[0m - \u001b[0;36miter 49 step 24: loss 6.0312, iter time: 122.00ms remaining time: \u001b[0m\n",
      "[\u001b[0;36mDEBUG\u001b[0m | src.tasks.clm_training.fabric.base]: \u001b[0;36miter 49 step 24: loss 6.0312, iter time: 122.00ms remaining time: \u001b[0m\n",
      "  1%|\u001b[34m▍                                        \u001b[0m| 49/4682 [00:20<32:31,  2.37it/s]\u001b[0m2025-06-16 13:54:47 - src.tasks.clm_training.fabric.base - \u001b[0;36mDEBUG\u001b[0m - \u001b[0;36mgradient_accumulation_steps raw value: 2, type: <class 'int'>\u001b[0m\n",
      "[\u001b[0;36mDEBUG\u001b[0m | src.tasks.clm_training.fabric.base]: \u001b[0;36mgradient_accumulation_steps raw value: 2, type: <class 'int'>\u001b[0m\n",
      "2025-06-16 13:54:47 - src.tasks.clm_training.fabric.base - \u001b[0;36mDEBUG\u001b[0m - \u001b[0;36mgradient_accumulation_steps converted: 2, type: <class 'int'>\u001b[0m\n",
      "[\u001b[0;36mDEBUG\u001b[0m | src.tasks.clm_training.fabric.base]: \u001b[0;36mgradient_accumulation_steps converted: 2, type: <class 'int'>\u001b[0m\n",
      "2025-06-16 13:54:47 - src.tasks.clm_training.fabric.base - \u001b[0;36mDEBUG\u001b[0m - \u001b[0;36miter_num value: 49, type: <class 'int'>\u001b[0m\n",
      "[\u001b[0;36mDEBUG\u001b[0m | src.tasks.clm_training.fabric.base]: \u001b[0;36miter_num value: 49, type: <class 'int'>\u001b[0m\n",
      "2025-06-16 13:54:47 - src.tasks.clm_training.fabric.base - \u001b[0;36mDEBUG\u001b[0m - \u001b[0;36mAbout to check: (iter_num + 1) % gradient_accumulation_steps != 0 -> (49 + 1) % 2 != 0\u001b[0m\n",
      "[\u001b[0;36mDEBUG\u001b[0m | src.tasks.clm_training.fabric.base]: \u001b[0;36mAbout to check: (iter_num + 1) % gradient_accumulation_steps != 0 -> (49 + 1) % 2 != 0\u001b[0m\n",
      "2025-06-16 13:54:47 - src.tasks.clm_training.fabric.base - \u001b[0;36mDEBUG\u001b[0m - \u001b[0;36mGradient norm before clipping: 25.0000\u001b[0m\n",
      "[\u001b[0;36mDEBUG\u001b[0m | src.tasks.clm_training.fabric.base]: \u001b[0;36mGradient norm before clipping: 25.0000\u001b[0m\n",
      "2025-06-16 13:54:47 - src.tasks.clm_training.fabric.base - \u001b[0;36mDEBUG\u001b[0m - \u001b[0;36mvalidate_after_k_steps value: 1000, type: <class 'int'>\u001b[0m\n",
      "[\u001b[0;36mDEBUG\u001b[0m | src.tasks.clm_training.fabric.base]: \u001b[0;36mvalidate_after_k_steps value: 1000, type: <class 'int'>\u001b[0m\n",
      "2025-06-16 13:54:47 - src.tasks.clm_training.fabric.base - \u001b[0;36mDEBUG\u001b[0m - \u001b[0;36mstep_count value: 25, type: <class 'int'>\u001b[0m\n",
      "[\u001b[0;36mDEBUG\u001b[0m | src.tasks.clm_training.fabric.base]: \u001b[0;36mstep_count value: 25, type: <class 'int'>\u001b[0m\n",
      "2025-06-16 13:54:47 - src.tasks.clm_training.fabric.base - \u001b[0;36mDEBUG\u001b[0m - \u001b[0;36mValidation condition check - epochFinished: False, trainingFinished: False\u001b[0m\n",
      "[\u001b[0;36mDEBUG\u001b[0m | src.tasks.clm_training.fabric.base]: \u001b[0;36mValidation condition check - epochFinished: False, trainingFinished: False\u001b[0m\n",
      "2025-06-16 13:54:47 - src.tasks.clm_training.fabric.base - \u001b[0;36mDEBUG\u001b[0m - \u001b[0;36mAbout to check: step_count % validate_after_k_steps == 0 -> 25 % 1000 == 0\u001b[0m\n",
      "[\u001b[0;36mDEBUG\u001b[0m | src.tasks.clm_training.fabric.base]: \u001b[0;36mAbout to check: step_count % validate_after_k_steps == 0 -> 25 % 1000 == 0\u001b[0m\n",
      "2025-06-16 13:54:47 - src.tasks.clm_training.fabric.base - \u001b[0;36mDEBUG\u001b[0m - \u001b[0;36miter 50 step 25: loss 5.5938, iter time: 419.83ms remaining time: \u001b[0m\n",
      "[\u001b[0;36mDEBUG\u001b[0m | src.tasks.clm_training.fabric.base]: \u001b[0;36miter 50 step 25: loss 5.5938, iter time: 419.83ms remaining time: \u001b[0m\n",
      "  1%|\u001b[34m▍                                        \u001b[0m| 50/4682 [00:21<32:43,  2.36it/s]\u001b[0m2025-06-16 13:54:47 - src.tasks.clm_training.fabric.base - \u001b[0;36mDEBUG\u001b[0m - \u001b[0;36mgradient_accumulation_steps raw value: 2, type: <class 'int'>\u001b[0m\n",
      "[\u001b[0;36mDEBUG\u001b[0m | src.tasks.clm_training.fabric.base]: \u001b[0;36mgradient_accumulation_steps raw value: 2, type: <class 'int'>\u001b[0m\n",
      "2025-06-16 13:54:47 - src.tasks.clm_training.fabric.base - \u001b[0;36mDEBUG\u001b[0m - \u001b[0;36mgradient_accumulation_steps converted: 2, type: <class 'int'>\u001b[0m\n",
      "[\u001b[0;36mDEBUG\u001b[0m | src.tasks.clm_training.fabric.base]: \u001b[0;36mgradient_accumulation_steps converted: 2, type: <class 'int'>\u001b[0m\n",
      "2025-06-16 13:54:47 - src.tasks.clm_training.fabric.base - \u001b[0;36mDEBUG\u001b[0m - \u001b[0;36miter_num value: 50, type: <class 'int'>\u001b[0m\n",
      "[\u001b[0;36mDEBUG\u001b[0m | src.tasks.clm_training.fabric.base]: \u001b[0;36miter_num value: 50, type: <class 'int'>\u001b[0m\n",
      "2025-06-16 13:54:47 - src.tasks.clm_training.fabric.base - \u001b[0;36mDEBUG\u001b[0m - \u001b[0;36mAbout to check: (iter_num + 1) % gradient_accumulation_steps != 0 -> (50 + 1) % 2 != 0\u001b[0m\n",
      "[\u001b[0;36mDEBUG\u001b[0m | src.tasks.clm_training.fabric.base]: \u001b[0;36mAbout to check: (iter_num + 1) % gradient_accumulation_steps != 0 -> (50 + 1) % 2 != 0\u001b[0m\n",
      "2025-06-16 13:54:47 - src.tasks.clm_training.fabric.base - \u001b[0;36mDEBUG\u001b[0m - \u001b[0;36miter 51 step 25: loss 5.5938, iter time: 121.65ms remaining time: \u001b[0m\n",
      "[\u001b[0;36mDEBUG\u001b[0m | src.tasks.clm_training.fabric.base]: \u001b[0;36miter 51 step 25: loss 5.5938, iter time: 121.65ms remaining time: \u001b[0m\n",
      "  1%|\u001b[34m▍                                        \u001b[0m| 51/4682 [00:21<32:29,  2.38it/s]\u001b[0m2025-06-16 13:54:47 - src.tasks.clm_training.fabric.base - \u001b[0;36mDEBUG\u001b[0m - \u001b[0;36mgradient_accumulation_steps raw value: 2, type: <class 'int'>\u001b[0m\n",
      "[\u001b[0;36mDEBUG\u001b[0m | src.tasks.clm_training.fabric.base]: \u001b[0;36mgradient_accumulation_steps raw value: 2, type: <class 'int'>\u001b[0m\n",
      "2025-06-16 13:54:47 - src.tasks.clm_training.fabric.base - \u001b[0;36mDEBUG\u001b[0m - \u001b[0;36mgradient_accumulation_steps converted: 2, type: <class 'int'>\u001b[0m\n",
      "[\u001b[0;36mDEBUG\u001b[0m | src.tasks.clm_training.fabric.base]: \u001b[0;36mgradient_accumulation_steps converted: 2, type: <class 'int'>\u001b[0m\n",
      "2025-06-16 13:54:47 - src.tasks.clm_training.fabric.base - \u001b[0;36mDEBUG\u001b[0m - \u001b[0;36miter_num value: 51, type: <class 'int'>\u001b[0m\n",
      "[\u001b[0;36mDEBUG\u001b[0m | src.tasks.clm_training.fabric.base]: \u001b[0;36miter_num value: 51, type: <class 'int'>\u001b[0m\n",
      "2025-06-16 13:54:47 - src.tasks.clm_training.fabric.base - \u001b[0;36mDEBUG\u001b[0m - \u001b[0;36mAbout to check: (iter_num + 1) % gradient_accumulation_steps != 0 -> (51 + 1) % 2 != 0\u001b[0m\n",
      "[\u001b[0;36mDEBUG\u001b[0m | src.tasks.clm_training.fabric.base]: \u001b[0;36mAbout to check: (iter_num + 1) % gradient_accumulation_steps != 0 -> (51 + 1) % 2 != 0\u001b[0m\n",
      "2025-06-16 13:54:48 - src.tasks.clm_training.fabric.base - \u001b[0;36mDEBUG\u001b[0m - \u001b[0;36mGradient norm before clipping: 22.5000\u001b[0m\n",
      "[\u001b[0;36mDEBUG\u001b[0m | src.tasks.clm_training.fabric.base]: \u001b[0;36mGradient norm before clipping: 22.5000\u001b[0m\n",
      "2025-06-16 13:54:48 - src.tasks.clm_training.fabric.base - \u001b[0;36mDEBUG\u001b[0m - \u001b[0;36mvalidate_after_k_steps value: 1000, type: <class 'int'>\u001b[0m\n",
      "[\u001b[0;36mDEBUG\u001b[0m | src.tasks.clm_training.fabric.base]: \u001b[0;36mvalidate_after_k_steps value: 1000, type: <class 'int'>\u001b[0m\n",
      "2025-06-16 13:54:48 - src.tasks.clm_training.fabric.base - \u001b[0;36mDEBUG\u001b[0m - \u001b[0;36mstep_count value: 26, type: <class 'int'>\u001b[0m\n",
      "[\u001b[0;36mDEBUG\u001b[0m | src.tasks.clm_training.fabric.base]: \u001b[0;36mstep_count value: 26, type: <class 'int'>\u001b[0m\n",
      "2025-06-16 13:54:48 - src.tasks.clm_training.fabric.base - \u001b[0;36mDEBUG\u001b[0m - \u001b[0;36mValidation condition check - epochFinished: False, trainingFinished: False\u001b[0m\n",
      "[\u001b[0;36mDEBUG\u001b[0m | src.tasks.clm_training.fabric.base]: \u001b[0;36mValidation condition check - epochFinished: False, trainingFinished: False\u001b[0m\n",
      "2025-06-16 13:54:48 - src.tasks.clm_training.fabric.base - \u001b[0;36mDEBUG\u001b[0m - \u001b[0;36mAbout to check: step_count % validate_after_k_steps == 0 -> 26 % 1000 == 0\u001b[0m\n",
      "[\u001b[0;36mDEBUG\u001b[0m | src.tasks.clm_training.fabric.base]: \u001b[0;36mAbout to check: step_count % validate_after_k_steps == 0 -> 26 % 1000 == 0\u001b[0m\n",
      "2025-06-16 13:54:48 - src.tasks.clm_training.fabric.base - \u001b[0;36mDEBUG\u001b[0m - \u001b[0;36miter 52 step 26: loss 5.7188, iter time: 419.79ms remaining time: \u001b[0m\n",
      "[\u001b[0;36mDEBUG\u001b[0m | src.tasks.clm_training.fabric.base]: \u001b[0;36miter 52 step 26: loss 5.7188, iter time: 419.79ms remaining time: \u001b[0m\n",
      "  1%|\u001b[34m▍                                        \u001b[0m| 52/4682 [00:22<32:41,  2.36it/s]\u001b[0m2025-06-16 13:54:48 - src.tasks.clm_training.fabric.base - \u001b[0;36mDEBUG\u001b[0m - \u001b[0;36mgradient_accumulation_steps raw value: 2, type: <class 'int'>\u001b[0m\n",
      "[\u001b[0;36mDEBUG\u001b[0m | src.tasks.clm_training.fabric.base]: \u001b[0;36mgradient_accumulation_steps raw value: 2, type: <class 'int'>\u001b[0m\n",
      "2025-06-16 13:54:48 - src.tasks.clm_training.fabric.base - \u001b[0;36mDEBUG\u001b[0m - \u001b[0;36mgradient_accumulation_steps converted: 2, type: <class 'int'>\u001b[0m\n",
      "[\u001b[0;36mDEBUG\u001b[0m | src.tasks.clm_training.fabric.base]: \u001b[0;36mgradient_accumulation_steps converted: 2, type: <class 'int'>\u001b[0m\n",
      "2025-06-16 13:54:48 - src.tasks.clm_training.fabric.base - \u001b[0;36mDEBUG\u001b[0m - \u001b[0;36miter_num value: 52, type: <class 'int'>\u001b[0m\n",
      "[\u001b[0;36mDEBUG\u001b[0m | src.tasks.clm_training.fabric.base]: \u001b[0;36miter_num value: 52, type: <class 'int'>\u001b[0m\n",
      "2025-06-16 13:54:48 - src.tasks.clm_training.fabric.base - \u001b[0;36mDEBUG\u001b[0m - \u001b[0;36mAbout to check: (iter_num + 1) % gradient_accumulation_steps != 0 -> (52 + 1) % 2 != 0\u001b[0m\n",
      "[\u001b[0;36mDEBUG\u001b[0m | src.tasks.clm_training.fabric.base]: \u001b[0;36mAbout to check: (iter_num + 1) % gradient_accumulation_steps != 0 -> (52 + 1) % 2 != 0\u001b[0m\n",
      "2025-06-16 13:54:48 - src.tasks.clm_training.fabric.base - \u001b[0;36mDEBUG\u001b[0m - \u001b[0;36miter 53 step 26: loss 5.5625, iter time: 122.43ms remaining time: \u001b[0m\n",
      "[\u001b[0;36mDEBUG\u001b[0m | src.tasks.clm_training.fabric.base]: \u001b[0;36miter 53 step 26: loss 5.5625, iter time: 122.43ms remaining time: \u001b[0m\n",
      "  1%|\u001b[34m▍                                        \u001b[0m| 53/4682 [00:22<32:29,  2.37it/s]\u001b[0m2025-06-16 13:54:48 - src.tasks.clm_training.fabric.base - \u001b[0;36mDEBUG\u001b[0m - \u001b[0;36mgradient_accumulation_steps raw value: 2, type: <class 'int'>\u001b[0m\n",
      "[\u001b[0;36mDEBUG\u001b[0m | src.tasks.clm_training.fabric.base]: \u001b[0;36mgradient_accumulation_steps raw value: 2, type: <class 'int'>\u001b[0m\n",
      "2025-06-16 13:54:48 - src.tasks.clm_training.fabric.base - \u001b[0;36mDEBUG\u001b[0m - \u001b[0;36mgradient_accumulation_steps converted: 2, type: <class 'int'>\u001b[0m\n",
      "[\u001b[0;36mDEBUG\u001b[0m | src.tasks.clm_training.fabric.base]: \u001b[0;36mgradient_accumulation_steps converted: 2, type: <class 'int'>\u001b[0m\n",
      "2025-06-16 13:54:48 - src.tasks.clm_training.fabric.base - \u001b[0;36mDEBUG\u001b[0m - \u001b[0;36miter_num value: 53, type: <class 'int'>\u001b[0m\n",
      "[\u001b[0;36mDEBUG\u001b[0m | src.tasks.clm_training.fabric.base]: \u001b[0;36miter_num value: 53, type: <class 'int'>\u001b[0m\n",
      "2025-06-16 13:54:48 - src.tasks.clm_training.fabric.base - \u001b[0;36mDEBUG\u001b[0m - \u001b[0;36mAbout to check: (iter_num + 1) % gradient_accumulation_steps != 0 -> (53 + 1) % 2 != 0\u001b[0m\n",
      "[\u001b[0;36mDEBUG\u001b[0m | src.tasks.clm_training.fabric.base]: \u001b[0;36mAbout to check: (iter_num + 1) % gradient_accumulation_steps != 0 -> (53 + 1) % 2 != 0\u001b[0m\n",
      "2025-06-16 13:54:49 - src.tasks.clm_training.fabric.base - \u001b[0;36mDEBUG\u001b[0m - \u001b[0;36mGradient norm before clipping: 24.0000\u001b[0m\n",
      "[\u001b[0;36mDEBUG\u001b[0m | src.tasks.clm_training.fabric.base]: \u001b[0;36mGradient norm before clipping: 24.0000\u001b[0m\n",
      "2025-06-16 13:54:49 - src.tasks.clm_training.fabric.base - \u001b[0;36mDEBUG\u001b[0m - \u001b[0;36mvalidate_after_k_steps value: 1000, type: <class 'int'>\u001b[0m\n",
      "[\u001b[0;36mDEBUG\u001b[0m | src.tasks.clm_training.fabric.base]: \u001b[0;36mvalidate_after_k_steps value: 1000, type: <class 'int'>\u001b[0m\n",
      "2025-06-16 13:54:49 - src.tasks.clm_training.fabric.base - \u001b[0;36mDEBUG\u001b[0m - \u001b[0;36mstep_count value: 27, type: <class 'int'>\u001b[0m\n",
      "[\u001b[0;36mDEBUG\u001b[0m | src.tasks.clm_training.fabric.base]: \u001b[0;36mstep_count value: 27, type: <class 'int'>\u001b[0m\n",
      "2025-06-16 13:54:49 - src.tasks.clm_training.fabric.base - \u001b[0;36mDEBUG\u001b[0m - \u001b[0;36mValidation condition check - epochFinished: False, trainingFinished: False\u001b[0m\n",
      "[\u001b[0;36mDEBUG\u001b[0m | src.tasks.clm_training.fabric.base]: \u001b[0;36mValidation condition check - epochFinished: False, trainingFinished: False\u001b[0m\n",
      "2025-06-16 13:54:49 - src.tasks.clm_training.fabric.base - \u001b[0;36mDEBUG\u001b[0m - \u001b[0;36mAbout to check: step_count % validate_after_k_steps == 0 -> 27 % 1000 == 0\u001b[0m\n",
      "[\u001b[0;36mDEBUG\u001b[0m | src.tasks.clm_training.fabric.base]: \u001b[0;36mAbout to check: step_count % validate_after_k_steps == 0 -> 27 % 1000 == 0\u001b[0m\n",
      "2025-06-16 13:54:49 - src.tasks.clm_training.fabric.base - \u001b[0;36mDEBUG\u001b[0m - \u001b[0;36miter 54 step 27: loss 6.1250, iter time: 420.04ms remaining time: \u001b[0m\n",
      "[\u001b[0;36mDEBUG\u001b[0m | src.tasks.clm_training.fabric.base]: \u001b[0;36miter 54 step 27: loss 6.1250, iter time: 420.04ms remaining time: \u001b[0m\n",
      "  1%|\u001b[34m▍                                        \u001b[0m| 54/4682 [00:23<32:41,  2.36it/s]\u001b[0m2025-06-16 13:54:49 - src.tasks.clm_training.fabric.base - \u001b[0;36mDEBUG\u001b[0m - \u001b[0;36mgradient_accumulation_steps raw value: 2, type: <class 'int'>\u001b[0m\n",
      "[\u001b[0;36mDEBUG\u001b[0m | src.tasks.clm_training.fabric.base]: \u001b[0;36mgradient_accumulation_steps raw value: 2, type: <class 'int'>\u001b[0m\n",
      "2025-06-16 13:54:49 - src.tasks.clm_training.fabric.base - \u001b[0;36mDEBUG\u001b[0m - \u001b[0;36mgradient_accumulation_steps converted: 2, type: <class 'int'>\u001b[0m\n",
      "[\u001b[0;36mDEBUG\u001b[0m | src.tasks.clm_training.fabric.base]: \u001b[0;36mgradient_accumulation_steps converted: 2, type: <class 'int'>\u001b[0m\n",
      "2025-06-16 13:54:49 - src.tasks.clm_training.fabric.base - \u001b[0;36mDEBUG\u001b[0m - \u001b[0;36miter_num value: 54, type: <class 'int'>\u001b[0m\n",
      "[\u001b[0;36mDEBUG\u001b[0m | src.tasks.clm_training.fabric.base]: \u001b[0;36miter_num value: 54, type: <class 'int'>\u001b[0m\n",
      "2025-06-16 13:54:49 - src.tasks.clm_training.fabric.base - \u001b[0;36mDEBUG\u001b[0m - \u001b[0;36mAbout to check: (iter_num + 1) % gradient_accumulation_steps != 0 -> (54 + 1) % 2 != 0\u001b[0m\n",
      "[\u001b[0;36mDEBUG\u001b[0m | src.tasks.clm_training.fabric.base]: \u001b[0;36mAbout to check: (iter_num + 1) % gradient_accumulation_steps != 0 -> (54 + 1) % 2 != 0\u001b[0m\n",
      "2025-06-16 13:54:49 - src.tasks.clm_training.fabric.base - \u001b[0;36mDEBUG\u001b[0m - \u001b[0;36miter 55 step 27: loss 5.7812, iter time: 121.74ms remaining time: \u001b[0m\n",
      "[\u001b[0;36mDEBUG\u001b[0m | src.tasks.clm_training.fabric.base]: \u001b[0;36miter 55 step 27: loss 5.7812, iter time: 121.74ms remaining time: \u001b[0m\n",
      "  1%|\u001b[34m▍                                        \u001b[0m| 55/4682 [00:23<32:28,  2.37it/s]\u001b[0m2025-06-16 13:54:49 - src.tasks.clm_training.fabric.base - \u001b[0;36mDEBUG\u001b[0m - \u001b[0;36mgradient_accumulation_steps raw value: 2, type: <class 'int'>\u001b[0m\n",
      "[\u001b[0;36mDEBUG\u001b[0m | src.tasks.clm_training.fabric.base]: \u001b[0;36mgradient_accumulation_steps raw value: 2, type: <class 'int'>\u001b[0m\n",
      "2025-06-16 13:54:49 - src.tasks.clm_training.fabric.base - \u001b[0;36mDEBUG\u001b[0m - \u001b[0;36mgradient_accumulation_steps converted: 2, type: <class 'int'>\u001b[0m\n",
      "[\u001b[0;36mDEBUG\u001b[0m | src.tasks.clm_training.fabric.base]: \u001b[0;36mgradient_accumulation_steps converted: 2, type: <class 'int'>\u001b[0m\n",
      "2025-06-16 13:54:49 - src.tasks.clm_training.fabric.base - \u001b[0;36mDEBUG\u001b[0m - \u001b[0;36miter_num value: 55, type: <class 'int'>\u001b[0m\n",
      "[\u001b[0;36mDEBUG\u001b[0m | src.tasks.clm_training.fabric.base]: \u001b[0;36miter_num value: 55, type: <class 'int'>\u001b[0m\n",
      "2025-06-16 13:54:49 - src.tasks.clm_training.fabric.base - \u001b[0;36mDEBUG\u001b[0m - \u001b[0;36mAbout to check: (iter_num + 1) % gradient_accumulation_steps != 0 -> (55 + 1) % 2 != 0\u001b[0m\n",
      "[\u001b[0;36mDEBUG\u001b[0m | src.tasks.clm_training.fabric.base]: \u001b[0;36mAbout to check: (iter_num + 1) % gradient_accumulation_steps != 0 -> (55 + 1) % 2 != 0\u001b[0m\n",
      "2025-06-16 13:54:50 - src.tasks.clm_training.fabric.base - \u001b[0;36mDEBUG\u001b[0m - \u001b[0;36mGradient norm before clipping: 25.7500\u001b[0m\n",
      "[\u001b[0;36mDEBUG\u001b[0m | src.tasks.clm_training.fabric.base]: \u001b[0;36mGradient norm before clipping: 25.7500\u001b[0m\n",
      "2025-06-16 13:54:50 - src.tasks.clm_training.fabric.base - \u001b[0;36mDEBUG\u001b[0m - \u001b[0;36mvalidate_after_k_steps value: 1000, type: <class 'int'>\u001b[0m\n",
      "[\u001b[0;36mDEBUG\u001b[0m | src.tasks.clm_training.fabric.base]: \u001b[0;36mvalidate_after_k_steps value: 1000, type: <class 'int'>\u001b[0m\n",
      "2025-06-16 13:54:50 - src.tasks.clm_training.fabric.base - \u001b[0;36mDEBUG\u001b[0m - \u001b[0;36mstep_count value: 28, type: <class 'int'>\u001b[0m\n",
      "[\u001b[0;36mDEBUG\u001b[0m | src.tasks.clm_training.fabric.base]: \u001b[0;36mstep_count value: 28, type: <class 'int'>\u001b[0m\n",
      "2025-06-16 13:54:50 - src.tasks.clm_training.fabric.base - \u001b[0;36mDEBUG\u001b[0m - \u001b[0;36mValidation condition check - epochFinished: False, trainingFinished: False\u001b[0m\n",
      "[\u001b[0;36mDEBUG\u001b[0m | src.tasks.clm_training.fabric.base]: \u001b[0;36mValidation condition check - epochFinished: False, trainingFinished: False\u001b[0m\n",
      "2025-06-16 13:54:50 - src.tasks.clm_training.fabric.base - \u001b[0;36mDEBUG\u001b[0m - \u001b[0;36mAbout to check: step_count % validate_after_k_steps == 0 -> 28 % 1000 == 0\u001b[0m\n",
      "[\u001b[0;36mDEBUG\u001b[0m | src.tasks.clm_training.fabric.base]: \u001b[0;36mAbout to check: step_count % validate_after_k_steps == 0 -> 28 % 1000 == 0\u001b[0m\n",
      "2025-06-16 13:54:50 - src.tasks.clm_training.fabric.base - \u001b[0;36mDEBUG\u001b[0m - \u001b[0;36miter 56 step 28: loss 5.9688, iter time: 420.15ms remaining time: \u001b[0m\n",
      "[\u001b[0;36mDEBUG\u001b[0m | src.tasks.clm_training.fabric.base]: \u001b[0;36miter 56 step 28: loss 5.9688, iter time: 420.15ms remaining time: \u001b[0m\n",
      "  1%|\u001b[34m▍                                        \u001b[0m| 56/4682 [00:23<32:40,  2.36it/s]\u001b[0m2025-06-16 13:54:50 - src.tasks.clm_training.fabric.base - \u001b[0;36mDEBUG\u001b[0m - \u001b[0;36mgradient_accumulation_steps raw value: 2, type: <class 'int'>\u001b[0m\n",
      "[\u001b[0;36mDEBUG\u001b[0m | src.tasks.clm_training.fabric.base]: \u001b[0;36mgradient_accumulation_steps raw value: 2, type: <class 'int'>\u001b[0m\n",
      "2025-06-16 13:54:50 - src.tasks.clm_training.fabric.base - \u001b[0;36mDEBUG\u001b[0m - \u001b[0;36mgradient_accumulation_steps converted: 2, type: <class 'int'>\u001b[0m\n",
      "[\u001b[0;36mDEBUG\u001b[0m | src.tasks.clm_training.fabric.base]: \u001b[0;36mgradient_accumulation_steps converted: 2, type: <class 'int'>\u001b[0m\n",
      "2025-06-16 13:54:50 - src.tasks.clm_training.fabric.base - \u001b[0;36mDEBUG\u001b[0m - \u001b[0;36miter_num value: 56, type: <class 'int'>\u001b[0m\n",
      "[\u001b[0;36mDEBUG\u001b[0m | src.tasks.clm_training.fabric.base]: \u001b[0;36miter_num value: 56, type: <class 'int'>\u001b[0m\n",
      "2025-06-16 13:54:50 - src.tasks.clm_training.fabric.base - \u001b[0;36mDEBUG\u001b[0m - \u001b[0;36mAbout to check: (iter_num + 1) % gradient_accumulation_steps != 0 -> (56 + 1) % 2 != 0\u001b[0m\n",
      "[\u001b[0;36mDEBUG\u001b[0m | src.tasks.clm_training.fabric.base]: \u001b[0;36mAbout to check: (iter_num + 1) % gradient_accumulation_steps != 0 -> (56 + 1) % 2 != 0\u001b[0m\n",
      "2025-06-16 13:54:50 - src.tasks.clm_training.fabric.base - \u001b[0;36mDEBUG\u001b[0m - \u001b[0;36miter 57 step 28: loss 5.6875, iter time: 122.10ms remaining time: \u001b[0m\n",
      "[\u001b[0;36mDEBUG\u001b[0m | src.tasks.clm_training.fabric.base]: \u001b[0;36miter 57 step 28: loss 5.6875, iter time: 122.10ms remaining time: \u001b[0m\n",
      "  1%|\u001b[34m▍                                        \u001b[0m| 57/4682 [00:24<32:28,  2.37it/s]\u001b[0m2025-06-16 13:54:50 - src.tasks.clm_training.fabric.base - \u001b[0;36mDEBUG\u001b[0m - \u001b[0;36mgradient_accumulation_steps raw value: 2, type: <class 'int'>\u001b[0m\n",
      "[\u001b[0;36mDEBUG\u001b[0m | src.tasks.clm_training.fabric.base]: \u001b[0;36mgradient_accumulation_steps raw value: 2, type: <class 'int'>\u001b[0m\n",
      "2025-06-16 13:54:50 - src.tasks.clm_training.fabric.base - \u001b[0;36mDEBUG\u001b[0m - \u001b[0;36mgradient_accumulation_steps converted: 2, type: <class 'int'>\u001b[0m\n",
      "[\u001b[0;36mDEBUG\u001b[0m | src.tasks.clm_training.fabric.base]: \u001b[0;36mgradient_accumulation_steps converted: 2, type: <class 'int'>\u001b[0m\n",
      "2025-06-16 13:54:50 - src.tasks.clm_training.fabric.base - \u001b[0;36mDEBUG\u001b[0m - \u001b[0;36miter_num value: 57, type: <class 'int'>\u001b[0m\n",
      "[\u001b[0;36mDEBUG\u001b[0m | src.tasks.clm_training.fabric.base]: \u001b[0;36miter_num value: 57, type: <class 'int'>\u001b[0m\n",
      "2025-06-16 13:54:50 - src.tasks.clm_training.fabric.base - \u001b[0;36mDEBUG\u001b[0m - \u001b[0;36mAbout to check: (iter_num + 1) % gradient_accumulation_steps != 0 -> (57 + 1) % 2 != 0\u001b[0m\n",
      "[\u001b[0;36mDEBUG\u001b[0m | src.tasks.clm_training.fabric.base]: \u001b[0;36mAbout to check: (iter_num + 1) % gradient_accumulation_steps != 0 -> (57 + 1) % 2 != 0\u001b[0m\n",
      "2025-06-16 13:54:50 - src.tasks.clm_training.fabric.base - \u001b[0;36mDEBUG\u001b[0m - \u001b[0;36mGradient norm before clipping: 24.6250\u001b[0m\n",
      "[\u001b[0;36mDEBUG\u001b[0m | src.tasks.clm_training.fabric.base]: \u001b[0;36mGradient norm before clipping: 24.6250\u001b[0m\n",
      "2025-06-16 13:54:50 - src.tasks.clm_training.fabric.base - \u001b[0;36mDEBUG\u001b[0m - \u001b[0;36mvalidate_after_k_steps value: 1000, type: <class 'int'>\u001b[0m\n",
      "[\u001b[0;36mDEBUG\u001b[0m | src.tasks.clm_training.fabric.base]: \u001b[0;36mvalidate_after_k_steps value: 1000, type: <class 'int'>\u001b[0m\n",
      "2025-06-16 13:54:50 - src.tasks.clm_training.fabric.base - \u001b[0;36mDEBUG\u001b[0m - \u001b[0;36mstep_count value: 29, type: <class 'int'>\u001b[0m\n",
      "[\u001b[0;36mDEBUG\u001b[0m | src.tasks.clm_training.fabric.base]: \u001b[0;36mstep_count value: 29, type: <class 'int'>\u001b[0m\n",
      "2025-06-16 13:54:50 - src.tasks.clm_training.fabric.base - \u001b[0;36mDEBUG\u001b[0m - \u001b[0;36mValidation condition check - epochFinished: False, trainingFinished: False\u001b[0m\n",
      "[\u001b[0;36mDEBUG\u001b[0m | src.tasks.clm_training.fabric.base]: \u001b[0;36mValidation condition check - epochFinished: False, trainingFinished: False\u001b[0m\n",
      "2025-06-16 13:54:50 - src.tasks.clm_training.fabric.base - \u001b[0;36mDEBUG\u001b[0m - \u001b[0;36mAbout to check: step_count % validate_after_k_steps == 0 -> 29 % 1000 == 0\u001b[0m\n",
      "[\u001b[0;36mDEBUG\u001b[0m | src.tasks.clm_training.fabric.base]: \u001b[0;36mAbout to check: step_count % validate_after_k_steps == 0 -> 29 % 1000 == 0\u001b[0m\n",
      "2025-06-16 13:54:50 - src.tasks.clm_training.fabric.base - \u001b[0;36mDEBUG\u001b[0m - \u001b[0;36miter 58 step 29: loss 5.5625, iter time: 419.41ms remaining time: \u001b[0m\n",
      "[\u001b[0;36mDEBUG\u001b[0m | src.tasks.clm_training.fabric.base]: \u001b[0;36miter 58 step 29: loss 5.5625, iter time: 419.41ms remaining time: \u001b[0m\n",
      "  1%|\u001b[34m▌                                        \u001b[0m| 58/4682 [00:24<32:39,  2.36it/s]\u001b[0m2025-06-16 13:54:50 - src.tasks.clm_training.fabric.base - \u001b[0;36mDEBUG\u001b[0m - \u001b[0;36mgradient_accumulation_steps raw value: 2, type: <class 'int'>\u001b[0m\n",
      "[\u001b[0;36mDEBUG\u001b[0m | src.tasks.clm_training.fabric.base]: \u001b[0;36mgradient_accumulation_steps raw value: 2, type: <class 'int'>\u001b[0m\n",
      "2025-06-16 13:54:50 - src.tasks.clm_training.fabric.base - \u001b[0;36mDEBUG\u001b[0m - \u001b[0;36mgradient_accumulation_steps converted: 2, type: <class 'int'>\u001b[0m\n",
      "[\u001b[0;36mDEBUG\u001b[0m | src.tasks.clm_training.fabric.base]: \u001b[0;36mgradient_accumulation_steps converted: 2, type: <class 'int'>\u001b[0m\n",
      "2025-06-16 13:54:50 - src.tasks.clm_training.fabric.base - \u001b[0;36mDEBUG\u001b[0m - \u001b[0;36miter_num value: 58, type: <class 'int'>\u001b[0m\n",
      "[\u001b[0;36mDEBUG\u001b[0m | src.tasks.clm_training.fabric.base]: \u001b[0;36miter_num value: 58, type: <class 'int'>\u001b[0m\n",
      "2025-06-16 13:54:50 - src.tasks.clm_training.fabric.base - \u001b[0;36mDEBUG\u001b[0m - \u001b[0;36mAbout to check: (iter_num + 1) % gradient_accumulation_steps != 0 -> (58 + 1) % 2 != 0\u001b[0m\n",
      "[\u001b[0;36mDEBUG\u001b[0m | src.tasks.clm_training.fabric.base]: \u001b[0;36mAbout to check: (iter_num + 1) % gradient_accumulation_steps != 0 -> (58 + 1) % 2 != 0\u001b[0m\n",
      "2025-06-16 13:54:51 - src.tasks.clm_training.fabric.base - \u001b[0;36mDEBUG\u001b[0m - \u001b[0;36miter 59 step 29: loss 6.0312, iter time: 121.99ms remaining time: \u001b[0m\n",
      "[\u001b[0;36mDEBUG\u001b[0m | src.tasks.clm_training.fabric.base]: \u001b[0;36miter 59 step 29: loss 6.0312, iter time: 121.99ms remaining time: \u001b[0m\n",
      "  1%|\u001b[34m▌                                        \u001b[0m| 59/4682 [00:25<32:27,  2.37it/s]\u001b[0m2025-06-16 13:54:51 - src.tasks.clm_training.fabric.base - \u001b[0;36mDEBUG\u001b[0m - \u001b[0;36mgradient_accumulation_steps raw value: 2, type: <class 'int'>\u001b[0m\n",
      "[\u001b[0;36mDEBUG\u001b[0m | src.tasks.clm_training.fabric.base]: \u001b[0;36mgradient_accumulation_steps raw value: 2, type: <class 'int'>\u001b[0m\n",
      "2025-06-16 13:54:51 - src.tasks.clm_training.fabric.base - \u001b[0;36mDEBUG\u001b[0m - \u001b[0;36mgradient_accumulation_steps converted: 2, type: <class 'int'>\u001b[0m\n",
      "[\u001b[0;36mDEBUG\u001b[0m | src.tasks.clm_training.fabric.base]: \u001b[0;36mgradient_accumulation_steps converted: 2, type: <class 'int'>\u001b[0m\n",
      "2025-06-16 13:54:51 - src.tasks.clm_training.fabric.base - \u001b[0;36mDEBUG\u001b[0m - \u001b[0;36miter_num value: 59, type: <class 'int'>\u001b[0m\n",
      "[\u001b[0;36mDEBUG\u001b[0m | src.tasks.clm_training.fabric.base]: \u001b[0;36miter_num value: 59, type: <class 'int'>\u001b[0m\n",
      "2025-06-16 13:54:51 - src.tasks.clm_training.fabric.base - \u001b[0;36mDEBUG\u001b[0m - \u001b[0;36mAbout to check: (iter_num + 1) % gradient_accumulation_steps != 0 -> (59 + 1) % 2 != 0\u001b[0m\n",
      "[\u001b[0;36mDEBUG\u001b[0m | src.tasks.clm_training.fabric.base]: \u001b[0;36mAbout to check: (iter_num + 1) % gradient_accumulation_steps != 0 -> (59 + 1) % 2 != 0\u001b[0m\n",
      "2025-06-16 13:54:51 - src.tasks.clm_training.fabric.base - \u001b[0;36mDEBUG\u001b[0m - \u001b[0;36mGradient norm before clipping: 24.5000\u001b[0m\n",
      "[\u001b[0;36mDEBUG\u001b[0m | src.tasks.clm_training.fabric.base]: \u001b[0;36mGradient norm before clipping: 24.5000\u001b[0m\n",
      "2025-06-16 13:54:51 - src.tasks.clm_training.fabric.base - \u001b[0;36mDEBUG\u001b[0m - \u001b[0;36mvalidate_after_k_steps value: 1000, type: <class 'int'>\u001b[0m\n",
      "[\u001b[0;36mDEBUG\u001b[0m | src.tasks.clm_training.fabric.base]: \u001b[0;36mvalidate_after_k_steps value: 1000, type: <class 'int'>\u001b[0m\n",
      "2025-06-16 13:54:51 - src.tasks.clm_training.fabric.base - \u001b[0;36mDEBUG\u001b[0m - \u001b[0;36mstep_count value: 30, type: <class 'int'>\u001b[0m\n",
      "[\u001b[0;36mDEBUG\u001b[0m | src.tasks.clm_training.fabric.base]: \u001b[0;36mstep_count value: 30, type: <class 'int'>\u001b[0m\n",
      "2025-06-16 13:54:51 - src.tasks.clm_training.fabric.base - \u001b[0;36mDEBUG\u001b[0m - \u001b[0;36mValidation condition check - epochFinished: False, trainingFinished: False\u001b[0m\n",
      "[\u001b[0;36mDEBUG\u001b[0m | src.tasks.clm_training.fabric.base]: \u001b[0;36mValidation condition check - epochFinished: False, trainingFinished: False\u001b[0m\n",
      "2025-06-16 13:54:51 - src.tasks.clm_training.fabric.base - \u001b[0;36mDEBUG\u001b[0m - \u001b[0;36mAbout to check: step_count % validate_after_k_steps == 0 -> 30 % 1000 == 0\u001b[0m\n",
      "[\u001b[0;36mDEBUG\u001b[0m | src.tasks.clm_training.fabric.base]: \u001b[0;36mAbout to check: step_count % validate_after_k_steps == 0 -> 30 % 1000 == 0\u001b[0m\n",
      "2025-06-16 13:54:51 - src.tasks.clm_training.fabric.base - \u001b[0;36mDEBUG\u001b[0m - \u001b[0;36miter 60 step 30: loss 5.7188, iter time: 419.93ms remaining time: \u001b[0m\n",
      "[\u001b[0;36mDEBUG\u001b[0m | src.tasks.clm_training.fabric.base]: \u001b[0;36miter 60 step 30: loss 5.7188, iter time: 419.93ms remaining time: \u001b[0m\n",
      "  1%|\u001b[34m▌                                        \u001b[0m| 60/4682 [00:25<32:38,  2.36it/s]\u001b[0m2025-06-16 13:54:51 - src.tasks.clm_training.fabric.base - \u001b[0;36mDEBUG\u001b[0m - \u001b[0;36mgradient_accumulation_steps raw value: 2, type: <class 'int'>\u001b[0m\n",
      "[\u001b[0;36mDEBUG\u001b[0m | src.tasks.clm_training.fabric.base]: \u001b[0;36mgradient_accumulation_steps raw value: 2, type: <class 'int'>\u001b[0m\n",
      "2025-06-16 13:54:51 - src.tasks.clm_training.fabric.base - \u001b[0;36mDEBUG\u001b[0m - \u001b[0;36mgradient_accumulation_steps converted: 2, type: <class 'int'>\u001b[0m\n",
      "[\u001b[0;36mDEBUG\u001b[0m | src.tasks.clm_training.fabric.base]: \u001b[0;36mgradient_accumulation_steps converted: 2, type: <class 'int'>\u001b[0m\n",
      "2025-06-16 13:54:51 - src.tasks.clm_training.fabric.base - \u001b[0;36mDEBUG\u001b[0m - \u001b[0;36miter_num value: 60, type: <class 'int'>\u001b[0m\n",
      "[\u001b[0;36mDEBUG\u001b[0m | src.tasks.clm_training.fabric.base]: \u001b[0;36miter_num value: 60, type: <class 'int'>\u001b[0m\n",
      "2025-06-16 13:54:51 - src.tasks.clm_training.fabric.base - \u001b[0;36mDEBUG\u001b[0m - \u001b[0;36mAbout to check: (iter_num + 1) % gradient_accumulation_steps != 0 -> (60 + 1) % 2 != 0\u001b[0m\n",
      "[\u001b[0;36mDEBUG\u001b[0m | src.tasks.clm_training.fabric.base]: \u001b[0;36mAbout to check: (iter_num + 1) % gradient_accumulation_steps != 0 -> (60 + 1) % 2 != 0\u001b[0m\n",
      "2025-06-16 13:54:52 - src.tasks.clm_training.fabric.base - \u001b[0;36mDEBUG\u001b[0m - \u001b[0;36miter 61 step 30: loss 6.1250, iter time: 121.94ms remaining time: \u001b[0m\n",
      "[\u001b[0;36mDEBUG\u001b[0m | src.tasks.clm_training.fabric.base]: \u001b[0;36miter 61 step 30: loss 6.1250, iter time: 121.94ms remaining time: \u001b[0m\n",
      "  1%|\u001b[34m▌                                        \u001b[0m| 61/4682 [00:26<32:26,  2.37it/s]\u001b[0m2025-06-16 13:54:52 - src.tasks.clm_training.fabric.base - \u001b[0;36mDEBUG\u001b[0m - \u001b[0;36mgradient_accumulation_steps raw value: 2, type: <class 'int'>\u001b[0m\n",
      "[\u001b[0;36mDEBUG\u001b[0m | src.tasks.clm_training.fabric.base]: \u001b[0;36mgradient_accumulation_steps raw value: 2, type: <class 'int'>\u001b[0m\n",
      "2025-06-16 13:54:52 - src.tasks.clm_training.fabric.base - \u001b[0;36mDEBUG\u001b[0m - \u001b[0;36mgradient_accumulation_steps converted: 2, type: <class 'int'>\u001b[0m\n",
      "[\u001b[0;36mDEBUG\u001b[0m | src.tasks.clm_training.fabric.base]: \u001b[0;36mgradient_accumulation_steps converted: 2, type: <class 'int'>\u001b[0m\n",
      "2025-06-16 13:54:52 - src.tasks.clm_training.fabric.base - \u001b[0;36mDEBUG\u001b[0m - \u001b[0;36miter_num value: 61, type: <class 'int'>\u001b[0m\n",
      "[\u001b[0;36mDEBUG\u001b[0m | src.tasks.clm_training.fabric.base]: \u001b[0;36miter_num value: 61, type: <class 'int'>\u001b[0m\n",
      "2025-06-16 13:54:52 - src.tasks.clm_training.fabric.base - \u001b[0;36mDEBUG\u001b[0m - \u001b[0;36mAbout to check: (iter_num + 1) % gradient_accumulation_steps != 0 -> (61 + 1) % 2 != 0\u001b[0m\n",
      "[\u001b[0;36mDEBUG\u001b[0m | src.tasks.clm_training.fabric.base]: \u001b[0;36mAbout to check: (iter_num + 1) % gradient_accumulation_steps != 0 -> (61 + 1) % 2 != 0\u001b[0m\n",
      "2025-06-16 13:54:52 - src.tasks.clm_training.fabric.base - \u001b[0;36mDEBUG\u001b[0m - \u001b[0;36mGradient norm before clipping: 24.1250\u001b[0m\n",
      "[\u001b[0;36mDEBUG\u001b[0m | src.tasks.clm_training.fabric.base]: \u001b[0;36mGradient norm before clipping: 24.1250\u001b[0m\n",
      "2025-06-16 13:54:52 - src.tasks.clm_training.fabric.base - \u001b[0;36mDEBUG\u001b[0m - \u001b[0;36mvalidate_after_k_steps value: 1000, type: <class 'int'>\u001b[0m\n",
      "[\u001b[0;36mDEBUG\u001b[0m | src.tasks.clm_training.fabric.base]: \u001b[0;36mvalidate_after_k_steps value: 1000, type: <class 'int'>\u001b[0m\n",
      "2025-06-16 13:54:52 - src.tasks.clm_training.fabric.base - \u001b[0;36mDEBUG\u001b[0m - \u001b[0;36mstep_count value: 31, type: <class 'int'>\u001b[0m\n",
      "[\u001b[0;36mDEBUG\u001b[0m | src.tasks.clm_training.fabric.base]: \u001b[0;36mstep_count value: 31, type: <class 'int'>\u001b[0m\n",
      "2025-06-16 13:54:52 - src.tasks.clm_training.fabric.base - \u001b[0;36mDEBUG\u001b[0m - \u001b[0;36mValidation condition check - epochFinished: False, trainingFinished: False\u001b[0m\n",
      "[\u001b[0;36mDEBUG\u001b[0m | src.tasks.clm_training.fabric.base]: \u001b[0;36mValidation condition check - epochFinished: False, trainingFinished: False\u001b[0m\n",
      "2025-06-16 13:54:52 - src.tasks.clm_training.fabric.base - \u001b[0;36mDEBUG\u001b[0m - \u001b[0;36mAbout to check: step_count % validate_after_k_steps == 0 -> 31 % 1000 == 0\u001b[0m\n",
      "[\u001b[0;36mDEBUG\u001b[0m | src.tasks.clm_training.fabric.base]: \u001b[0;36mAbout to check: step_count % validate_after_k_steps == 0 -> 31 % 1000 == 0\u001b[0m\n",
      "2025-06-16 13:54:52 - src.tasks.clm_training.fabric.base - \u001b[0;36mDEBUG\u001b[0m - \u001b[0;36miter 62 step 31: loss 5.6875, iter time: 419.91ms remaining time: \u001b[0m\n",
      "[\u001b[0;36mDEBUG\u001b[0m | src.tasks.clm_training.fabric.base]: \u001b[0;36miter 62 step 31: loss 5.6875, iter time: 419.91ms remaining time: \u001b[0m\n",
      "  1%|\u001b[34m▌                                        \u001b[0m| 62/4682 [00:26<32:38,  2.36it/s]\u001b[0m2025-06-16 13:54:52 - src.tasks.clm_training.fabric.base - \u001b[0;36mDEBUG\u001b[0m - \u001b[0;36mgradient_accumulation_steps raw value: 2, type: <class 'int'>\u001b[0m\n",
      "[\u001b[0;36mDEBUG\u001b[0m | src.tasks.clm_training.fabric.base]: \u001b[0;36mgradient_accumulation_steps raw value: 2, type: <class 'int'>\u001b[0m\n",
      "2025-06-16 13:54:52 - src.tasks.clm_training.fabric.base - \u001b[0;36mDEBUG\u001b[0m - \u001b[0;36mgradient_accumulation_steps converted: 2, type: <class 'int'>\u001b[0m\n",
      "[\u001b[0;36mDEBUG\u001b[0m | src.tasks.clm_training.fabric.base]: \u001b[0;36mgradient_accumulation_steps converted: 2, type: <class 'int'>\u001b[0m\n",
      "2025-06-16 13:54:52 - src.tasks.clm_training.fabric.base - \u001b[0;36mDEBUG\u001b[0m - \u001b[0;36miter_num value: 62, type: <class 'int'>\u001b[0m\n",
      "[\u001b[0;36mDEBUG\u001b[0m | src.tasks.clm_training.fabric.base]: \u001b[0;36miter_num value: 62, type: <class 'int'>\u001b[0m\n",
      "2025-06-16 13:54:52 - src.tasks.clm_training.fabric.base - \u001b[0;36mDEBUG\u001b[0m - \u001b[0;36mAbout to check: (iter_num + 1) % gradient_accumulation_steps != 0 -> (62 + 1) % 2 != 0\u001b[0m\n",
      "[\u001b[0;36mDEBUG\u001b[0m | src.tasks.clm_training.fabric.base]: \u001b[0;36mAbout to check: (iter_num + 1) % gradient_accumulation_steps != 0 -> (62 + 1) % 2 != 0\u001b[0m\n",
      "2025-06-16 13:54:52 - src.tasks.clm_training.fabric.base - \u001b[0;36mDEBUG\u001b[0m - \u001b[0;36miter 63 step 31: loss 5.7812, iter time: 121.78ms remaining time: \u001b[0m\n",
      "[\u001b[0;36mDEBUG\u001b[0m | src.tasks.clm_training.fabric.base]: \u001b[0;36miter 63 step 31: loss 5.7812, iter time: 121.78ms remaining time: \u001b[0m\n",
      "  1%|\u001b[34m▌                                        \u001b[0m| 63/4682 [00:26<32:25,  2.37it/s]\u001b[0m2025-06-16 13:54:52 - src.tasks.clm_training.fabric.base - \u001b[0;36mDEBUG\u001b[0m - \u001b[0;36mgradient_accumulation_steps raw value: 2, type: <class 'int'>\u001b[0m\n",
      "[\u001b[0;36mDEBUG\u001b[0m | src.tasks.clm_training.fabric.base]: \u001b[0;36mgradient_accumulation_steps raw value: 2, type: <class 'int'>\u001b[0m\n",
      "2025-06-16 13:54:52 - src.tasks.clm_training.fabric.base - \u001b[0;36mDEBUG\u001b[0m - \u001b[0;36mgradient_accumulation_steps converted: 2, type: <class 'int'>\u001b[0m\n",
      "[\u001b[0;36mDEBUG\u001b[0m | src.tasks.clm_training.fabric.base]: \u001b[0;36mgradient_accumulation_steps converted: 2, type: <class 'int'>\u001b[0m\n",
      "2025-06-16 13:54:52 - src.tasks.clm_training.fabric.base - \u001b[0;36mDEBUG\u001b[0m - \u001b[0;36miter_num value: 63, type: <class 'int'>\u001b[0m\n",
      "[\u001b[0;36mDEBUG\u001b[0m | src.tasks.clm_training.fabric.base]: \u001b[0;36miter_num value: 63, type: <class 'int'>\u001b[0m\n",
      "2025-06-16 13:54:52 - src.tasks.clm_training.fabric.base - \u001b[0;36mDEBUG\u001b[0m - \u001b[0;36mAbout to check: (iter_num + 1) % gradient_accumulation_steps != 0 -> (63 + 1) % 2 != 0\u001b[0m\n",
      "[\u001b[0;36mDEBUG\u001b[0m | src.tasks.clm_training.fabric.base]: \u001b[0;36mAbout to check: (iter_num + 1) % gradient_accumulation_steps != 0 -> (63 + 1) % 2 != 0\u001b[0m\n",
      "2025-06-16 13:54:53 - src.tasks.clm_training.fabric.base - \u001b[0;36mDEBUG\u001b[0m - \u001b[0;36mGradient norm before clipping: 25.5000\u001b[0m\n",
      "[\u001b[0;36mDEBUG\u001b[0m | src.tasks.clm_training.fabric.base]: \u001b[0;36mGradient norm before clipping: 25.5000\u001b[0m\n",
      "2025-06-16 13:54:53 - src.tasks.clm_training.fabric.base - \u001b[0;36mDEBUG\u001b[0m - \u001b[0;36mvalidate_after_k_steps value: 1000, type: <class 'int'>\u001b[0m\n",
      "[\u001b[0;36mDEBUG\u001b[0m | src.tasks.clm_training.fabric.base]: \u001b[0;36mvalidate_after_k_steps value: 1000, type: <class 'int'>\u001b[0m\n",
      "2025-06-16 13:54:53 - src.tasks.clm_training.fabric.base - \u001b[0;36mDEBUG\u001b[0m - \u001b[0;36mstep_count value: 32, type: <class 'int'>\u001b[0m\n",
      "[\u001b[0;36mDEBUG\u001b[0m | src.tasks.clm_training.fabric.base]: \u001b[0;36mstep_count value: 32, type: <class 'int'>\u001b[0m\n",
      "2025-06-16 13:54:53 - src.tasks.clm_training.fabric.base - \u001b[0;36mDEBUG\u001b[0m - \u001b[0;36mValidation condition check - epochFinished: False, trainingFinished: False\u001b[0m\n",
      "[\u001b[0;36mDEBUG\u001b[0m | src.tasks.clm_training.fabric.base]: \u001b[0;36mValidation condition check - epochFinished: False, trainingFinished: False\u001b[0m\n",
      "2025-06-16 13:54:53 - src.tasks.clm_training.fabric.base - \u001b[0;36mDEBUG\u001b[0m - \u001b[0;36mAbout to check: step_count % validate_after_k_steps == 0 -> 32 % 1000 == 0\u001b[0m\n",
      "[\u001b[0;36mDEBUG\u001b[0m | src.tasks.clm_training.fabric.base]: \u001b[0;36mAbout to check: step_count % validate_after_k_steps == 0 -> 32 % 1000 == 0\u001b[0m\n",
      "2025-06-16 13:54:53 - src.tasks.clm_training.fabric.base - \u001b[0;36mDEBUG\u001b[0m - \u001b[0;36miter 64 step 32: loss 6.1562, iter time: 419.40ms remaining time: \u001b[0m\n",
      "[\u001b[0;36mDEBUG\u001b[0m | src.tasks.clm_training.fabric.base]: \u001b[0;36miter 64 step 32: loss 6.1562, iter time: 419.40ms remaining time: \u001b[0m\n",
      "  1%|\u001b[34m▌                                        \u001b[0m| 64/4682 [00:27<32:36,  2.36it/s]\u001b[0m2025-06-16 13:54:53 - src.tasks.clm_training.fabric.base - \u001b[0;36mDEBUG\u001b[0m - \u001b[0;36mgradient_accumulation_steps raw value: 2, type: <class 'int'>\u001b[0m\n",
      "[\u001b[0;36mDEBUG\u001b[0m | src.tasks.clm_training.fabric.base]: \u001b[0;36mgradient_accumulation_steps raw value: 2, type: <class 'int'>\u001b[0m\n",
      "2025-06-16 13:54:53 - src.tasks.clm_training.fabric.base - \u001b[0;36mDEBUG\u001b[0m - \u001b[0;36mgradient_accumulation_steps converted: 2, type: <class 'int'>\u001b[0m\n",
      "[\u001b[0;36mDEBUG\u001b[0m | src.tasks.clm_training.fabric.base]: \u001b[0;36mgradient_accumulation_steps converted: 2, type: <class 'int'>\u001b[0m\n",
      "2025-06-16 13:54:53 - src.tasks.clm_training.fabric.base - \u001b[0;36mDEBUG\u001b[0m - \u001b[0;36miter_num value: 64, type: <class 'int'>\u001b[0m\n",
      "[\u001b[0;36mDEBUG\u001b[0m | src.tasks.clm_training.fabric.base]: \u001b[0;36miter_num value: 64, type: <class 'int'>\u001b[0m\n",
      "2025-06-16 13:54:53 - src.tasks.clm_training.fabric.base - \u001b[0;36mDEBUG\u001b[0m - \u001b[0;36mAbout to check: (iter_num + 1) % gradient_accumulation_steps != 0 -> (64 + 1) % 2 != 0\u001b[0m\n",
      "[\u001b[0;36mDEBUG\u001b[0m | src.tasks.clm_training.fabric.base]: \u001b[0;36mAbout to check: (iter_num + 1) % gradient_accumulation_steps != 0 -> (64 + 1) % 2 != 0\u001b[0m\n",
      "2025-06-16 13:54:53 - src.tasks.clm_training.fabric.base - \u001b[0;36mDEBUG\u001b[0m - \u001b[0;36miter 65 step 32: loss 5.7812, iter time: 122.09ms remaining time: \u001b[0m\n",
      "[\u001b[0;36mDEBUG\u001b[0m | src.tasks.clm_training.fabric.base]: \u001b[0;36miter 65 step 32: loss 5.7812, iter time: 122.09ms remaining time: \u001b[0m\n",
      "  1%|\u001b[34m▌                                        \u001b[0m| 65/4682 [00:27<32:24,  2.37it/s]\u001b[0m2025-06-16 13:54:53 - src.tasks.clm_training.fabric.base - \u001b[0;36mDEBUG\u001b[0m - \u001b[0;36mgradient_accumulation_steps raw value: 2, type: <class 'int'>\u001b[0m\n",
      "[\u001b[0;36mDEBUG\u001b[0m | src.tasks.clm_training.fabric.base]: \u001b[0;36mgradient_accumulation_steps raw value: 2, type: <class 'int'>\u001b[0m\n",
      "2025-06-16 13:54:53 - src.tasks.clm_training.fabric.base - \u001b[0;36mDEBUG\u001b[0m - \u001b[0;36mgradient_accumulation_steps converted: 2, type: <class 'int'>\u001b[0m\n",
      "[\u001b[0;36mDEBUG\u001b[0m | src.tasks.clm_training.fabric.base]: \u001b[0;36mgradient_accumulation_steps converted: 2, type: <class 'int'>\u001b[0m\n",
      "2025-06-16 13:54:53 - src.tasks.clm_training.fabric.base - \u001b[0;36mDEBUG\u001b[0m - \u001b[0;36miter_num value: 65, type: <class 'int'>\u001b[0m\n",
      "[\u001b[0;36mDEBUG\u001b[0m | src.tasks.clm_training.fabric.base]: \u001b[0;36miter_num value: 65, type: <class 'int'>\u001b[0m\n",
      "2025-06-16 13:54:53 - src.tasks.clm_training.fabric.base - \u001b[0;36mDEBUG\u001b[0m - \u001b[0;36mAbout to check: (iter_num + 1) % gradient_accumulation_steps != 0 -> (65 + 1) % 2 != 0\u001b[0m\n",
      "[\u001b[0;36mDEBUG\u001b[0m | src.tasks.clm_training.fabric.base]: \u001b[0;36mAbout to check: (iter_num + 1) % gradient_accumulation_steps != 0 -> (65 + 1) % 2 != 0\u001b[0m\n",
      "2025-06-16 13:54:54 - src.tasks.clm_training.fabric.base - \u001b[0;36mDEBUG\u001b[0m - \u001b[0;36mGradient norm before clipping: 25.3750\u001b[0m\n",
      "[\u001b[0;36mDEBUG\u001b[0m | src.tasks.clm_training.fabric.base]: \u001b[0;36mGradient norm before clipping: 25.3750\u001b[0m\n",
      "2025-06-16 13:54:54 - src.tasks.clm_training.fabric.base - \u001b[0;36mDEBUG\u001b[0m - \u001b[0;36mvalidate_after_k_steps value: 1000, type: <class 'int'>\u001b[0m\n",
      "[\u001b[0;36mDEBUG\u001b[0m | src.tasks.clm_training.fabric.base]: \u001b[0;36mvalidate_after_k_steps value: 1000, type: <class 'int'>\u001b[0m\n",
      "2025-06-16 13:54:54 - src.tasks.clm_training.fabric.base - \u001b[0;36mDEBUG\u001b[0m - \u001b[0;36mstep_count value: 33, type: <class 'int'>\u001b[0m\n",
      "[\u001b[0;36mDEBUG\u001b[0m | src.tasks.clm_training.fabric.base]: \u001b[0;36mstep_count value: 33, type: <class 'int'>\u001b[0m\n",
      "2025-06-16 13:54:54 - src.tasks.clm_training.fabric.base - \u001b[0;36mDEBUG\u001b[0m - \u001b[0;36mValidation condition check - epochFinished: False, trainingFinished: False\u001b[0m\n",
      "[\u001b[0;36mDEBUG\u001b[0m | src.tasks.clm_training.fabric.base]: \u001b[0;36mValidation condition check - epochFinished: False, trainingFinished: False\u001b[0m\n",
      "2025-06-16 13:54:54 - src.tasks.clm_training.fabric.base - \u001b[0;36mDEBUG\u001b[0m - \u001b[0;36mAbout to check: step_count % validate_after_k_steps == 0 -> 33 % 1000 == 0\u001b[0m\n",
      "[\u001b[0;36mDEBUG\u001b[0m | src.tasks.clm_training.fabric.base]: \u001b[0;36mAbout to check: step_count % validate_after_k_steps == 0 -> 33 % 1000 == 0\u001b[0m\n",
      "2025-06-16 13:54:54 - src.tasks.clm_training.fabric.base - \u001b[0;36mDEBUG\u001b[0m - \u001b[0;36miter 66 step 33: loss 6.0312, iter time: 419.81ms remaining time: \u001b[0m\n",
      "[\u001b[0;36mDEBUG\u001b[0m | src.tasks.clm_training.fabric.base]: \u001b[0;36miter 66 step 33: loss 6.0312, iter time: 419.81ms remaining time: \u001b[0m\n",
      "  1%|\u001b[34m▌                                        \u001b[0m| 66/4682 [00:28<32:35,  2.36it/s]\u001b[0m2025-06-16 13:54:54 - src.tasks.clm_training.fabric.base - \u001b[0;36mDEBUG\u001b[0m - \u001b[0;36mgradient_accumulation_steps raw value: 2, type: <class 'int'>\u001b[0m\n",
      "[\u001b[0;36mDEBUG\u001b[0m | src.tasks.clm_training.fabric.base]: \u001b[0;36mgradient_accumulation_steps raw value: 2, type: <class 'int'>\u001b[0m\n",
      "2025-06-16 13:54:54 - src.tasks.clm_training.fabric.base - \u001b[0;36mDEBUG\u001b[0m - \u001b[0;36mgradient_accumulation_steps converted: 2, type: <class 'int'>\u001b[0m\n",
      "[\u001b[0;36mDEBUG\u001b[0m | src.tasks.clm_training.fabric.base]: \u001b[0;36mgradient_accumulation_steps converted: 2, type: <class 'int'>\u001b[0m\n",
      "2025-06-16 13:54:54 - src.tasks.clm_training.fabric.base - \u001b[0;36mDEBUG\u001b[0m - \u001b[0;36miter_num value: 66, type: <class 'int'>\u001b[0m\n",
      "[\u001b[0;36mDEBUG\u001b[0m | src.tasks.clm_training.fabric.base]: \u001b[0;36miter_num value: 66, type: <class 'int'>\u001b[0m\n",
      "2025-06-16 13:54:54 - src.tasks.clm_training.fabric.base - \u001b[0;36mDEBUG\u001b[0m - \u001b[0;36mAbout to check: (iter_num + 1) % gradient_accumulation_steps != 0 -> (66 + 1) % 2 != 0\u001b[0m\n",
      "[\u001b[0;36mDEBUG\u001b[0m | src.tasks.clm_training.fabric.base]: \u001b[0;36mAbout to check: (iter_num + 1) % gradient_accumulation_steps != 0 -> (66 + 1) % 2 != 0\u001b[0m\n",
      "2025-06-16 13:54:54 - src.tasks.clm_training.fabric.base - \u001b[0;36mDEBUG\u001b[0m - \u001b[0;36miter 67 step 33: loss 6.0625, iter time: 122.17ms remaining time: \u001b[0m\n",
      "[\u001b[0;36mDEBUG\u001b[0m | src.tasks.clm_training.fabric.base]: \u001b[0;36miter 67 step 33: loss 6.0625, iter time: 122.17ms remaining time: \u001b[0m\n",
      "  1%|\u001b[34m▌                                        \u001b[0m| 67/4682 [00:28<32:24,  2.37it/s]\u001b[0m2025-06-16 13:54:54 - src.tasks.clm_training.fabric.base - \u001b[0;36mDEBUG\u001b[0m - \u001b[0;36mgradient_accumulation_steps raw value: 2, type: <class 'int'>\u001b[0m\n",
      "[\u001b[0;36mDEBUG\u001b[0m | src.tasks.clm_training.fabric.base]: \u001b[0;36mgradient_accumulation_steps raw value: 2, type: <class 'int'>\u001b[0m\n",
      "2025-06-16 13:54:54 - src.tasks.clm_training.fabric.base - \u001b[0;36mDEBUG\u001b[0m - \u001b[0;36mgradient_accumulation_steps converted: 2, type: <class 'int'>\u001b[0m\n",
      "[\u001b[0;36mDEBUG\u001b[0m | src.tasks.clm_training.fabric.base]: \u001b[0;36mgradient_accumulation_steps converted: 2, type: <class 'int'>\u001b[0m\n",
      "2025-06-16 13:54:54 - src.tasks.clm_training.fabric.base - \u001b[0;36mDEBUG\u001b[0m - \u001b[0;36miter_num value: 67, type: <class 'int'>\u001b[0m\n",
      "[\u001b[0;36mDEBUG\u001b[0m | src.tasks.clm_training.fabric.base]: \u001b[0;36miter_num value: 67, type: <class 'int'>\u001b[0m\n",
      "2025-06-16 13:54:54 - src.tasks.clm_training.fabric.base - \u001b[0;36mDEBUG\u001b[0m - \u001b[0;36mAbout to check: (iter_num + 1) % gradient_accumulation_steps != 0 -> (67 + 1) % 2 != 0\u001b[0m\n",
      "[\u001b[0;36mDEBUG\u001b[0m | src.tasks.clm_training.fabric.base]: \u001b[0;36mAbout to check: (iter_num + 1) % gradient_accumulation_steps != 0 -> (67 + 1) % 2 != 0\u001b[0m\n",
      "2025-06-16 13:54:55 - src.tasks.clm_training.fabric.base - \u001b[0;36mDEBUG\u001b[0m - \u001b[0;36mGradient norm before clipping: 26.7500\u001b[0m\n",
      "[\u001b[0;36mDEBUG\u001b[0m | src.tasks.clm_training.fabric.base]: \u001b[0;36mGradient norm before clipping: 26.7500\u001b[0m\n",
      "2025-06-16 13:54:55 - src.tasks.clm_training.fabric.base - \u001b[0;36mDEBUG\u001b[0m - \u001b[0;36mvalidate_after_k_steps value: 1000, type: <class 'int'>\u001b[0m\n",
      "[\u001b[0;36mDEBUG\u001b[0m | src.tasks.clm_training.fabric.base]: \u001b[0;36mvalidate_after_k_steps value: 1000, type: <class 'int'>\u001b[0m\n",
      "2025-06-16 13:54:55 - src.tasks.clm_training.fabric.base - \u001b[0;36mDEBUG\u001b[0m - \u001b[0;36mstep_count value: 34, type: <class 'int'>\u001b[0m\n",
      "[\u001b[0;36mDEBUG\u001b[0m | src.tasks.clm_training.fabric.base]: \u001b[0;36mstep_count value: 34, type: <class 'int'>\u001b[0m\n",
      "2025-06-16 13:54:55 - src.tasks.clm_training.fabric.base - \u001b[0;36mDEBUG\u001b[0m - \u001b[0;36mValidation condition check - epochFinished: False, trainingFinished: False\u001b[0m\n",
      "[\u001b[0;36mDEBUG\u001b[0m | src.tasks.clm_training.fabric.base]: \u001b[0;36mValidation condition check - epochFinished: False, trainingFinished: False\u001b[0m\n",
      "2025-06-16 13:54:55 - src.tasks.clm_training.fabric.base - \u001b[0;36mDEBUG\u001b[0m - \u001b[0;36mAbout to check: step_count % validate_after_k_steps == 0 -> 34 % 1000 == 0\u001b[0m\n",
      "[\u001b[0;36mDEBUG\u001b[0m | src.tasks.clm_training.fabric.base]: \u001b[0;36mAbout to check: step_count % validate_after_k_steps == 0 -> 34 % 1000 == 0\u001b[0m\n",
      "2025-06-16 13:54:55 - src.tasks.clm_training.fabric.base - \u001b[0;36mDEBUG\u001b[0m - \u001b[0;36miter 68 step 34: loss 5.6875, iter time: 419.86ms remaining time: \u001b[0m\n",
      "[\u001b[0;36mDEBUG\u001b[0m | src.tasks.clm_training.fabric.base]: \u001b[0;36miter 68 step 34: loss 5.6875, iter time: 419.86ms remaining time: \u001b[0m\n",
      "  1%|\u001b[34m▌                                        \u001b[0m| 68/4682 [00:29<32:35,  2.36it/s]\u001b[0m2025-06-16 13:54:55 - src.tasks.clm_training.fabric.base - \u001b[0;36mDEBUG\u001b[0m - \u001b[0;36mgradient_accumulation_steps raw value: 2, type: <class 'int'>\u001b[0m\n",
      "[\u001b[0;36mDEBUG\u001b[0m | src.tasks.clm_training.fabric.base]: \u001b[0;36mgradient_accumulation_steps raw value: 2, type: <class 'int'>\u001b[0m\n",
      "2025-06-16 13:54:55 - src.tasks.clm_training.fabric.base - \u001b[0;36mDEBUG\u001b[0m - \u001b[0;36mgradient_accumulation_steps converted: 2, type: <class 'int'>\u001b[0m\n",
      "[\u001b[0;36mDEBUG\u001b[0m | src.tasks.clm_training.fabric.base]: \u001b[0;36mgradient_accumulation_steps converted: 2, type: <class 'int'>\u001b[0m\n",
      "2025-06-16 13:54:55 - src.tasks.clm_training.fabric.base - \u001b[0;36mDEBUG\u001b[0m - \u001b[0;36miter_num value: 68, type: <class 'int'>\u001b[0m\n",
      "[\u001b[0;36mDEBUG\u001b[0m | src.tasks.clm_training.fabric.base]: \u001b[0;36miter_num value: 68, type: <class 'int'>\u001b[0m\n",
      "2025-06-16 13:54:55 - src.tasks.clm_training.fabric.base - \u001b[0;36mDEBUG\u001b[0m - \u001b[0;36mAbout to check: (iter_num + 1) % gradient_accumulation_steps != 0 -> (68 + 1) % 2 != 0\u001b[0m\n",
      "[\u001b[0;36mDEBUG\u001b[0m | src.tasks.clm_training.fabric.base]: \u001b[0;36mAbout to check: (iter_num + 1) % gradient_accumulation_steps != 0 -> (68 + 1) % 2 != 0\u001b[0m\n",
      "2025-06-16 13:54:55 - src.tasks.clm_training.fabric.base - \u001b[0;36mDEBUG\u001b[0m - \u001b[0;36miter 69 step 34: loss 6.5000, iter time: 122.07ms remaining time: \u001b[0m\n",
      "[\u001b[0;36mDEBUG\u001b[0m | src.tasks.clm_training.fabric.base]: \u001b[0;36miter 69 step 34: loss 6.5000, iter time: 122.07ms remaining time: \u001b[0m\n",
      "  1%|\u001b[34m▌                                        \u001b[0m| 69/4682 [00:29<32:22,  2.37it/s]\u001b[0m2025-06-16 13:54:55 - src.tasks.clm_training.fabric.base - \u001b[0;36mDEBUG\u001b[0m - \u001b[0;36mgradient_accumulation_steps raw value: 2, type: <class 'int'>\u001b[0m\n",
      "[\u001b[0;36mDEBUG\u001b[0m | src.tasks.clm_training.fabric.base]: \u001b[0;36mgradient_accumulation_steps raw value: 2, type: <class 'int'>\u001b[0m\n",
      "2025-06-16 13:54:55 - src.tasks.clm_training.fabric.base - \u001b[0;36mDEBUG\u001b[0m - \u001b[0;36mgradient_accumulation_steps converted: 2, type: <class 'int'>\u001b[0m\n",
      "[\u001b[0;36mDEBUG\u001b[0m | src.tasks.clm_training.fabric.base]: \u001b[0;36mgradient_accumulation_steps converted: 2, type: <class 'int'>\u001b[0m\n",
      "2025-06-16 13:54:55 - src.tasks.clm_training.fabric.base - \u001b[0;36mDEBUG\u001b[0m - \u001b[0;36miter_num value: 69, type: <class 'int'>\u001b[0m\n",
      "[\u001b[0;36mDEBUG\u001b[0m | src.tasks.clm_training.fabric.base]: \u001b[0;36miter_num value: 69, type: <class 'int'>\u001b[0m\n",
      "2025-06-16 13:54:55 - src.tasks.clm_training.fabric.base - \u001b[0;36mDEBUG\u001b[0m - \u001b[0;36mAbout to check: (iter_num + 1) % gradient_accumulation_steps != 0 -> (69 + 1) % 2 != 0\u001b[0m\n",
      "[\u001b[0;36mDEBUG\u001b[0m | src.tasks.clm_training.fabric.base]: \u001b[0;36mAbout to check: (iter_num + 1) % gradient_accumulation_steps != 0 -> (69 + 1) % 2 != 0\u001b[0m\n",
      "2025-06-16 13:54:55 - src.tasks.clm_training.fabric.base - \u001b[0;36mDEBUG\u001b[0m - \u001b[0;36mGradient norm before clipping: 24.8750\u001b[0m\n",
      "[\u001b[0;36mDEBUG\u001b[0m | src.tasks.clm_training.fabric.base]: \u001b[0;36mGradient norm before clipping: 24.8750\u001b[0m\n",
      "2025-06-16 13:54:55 - src.tasks.clm_training.fabric.base - \u001b[0;36mDEBUG\u001b[0m - \u001b[0;36mvalidate_after_k_steps value: 1000, type: <class 'int'>\u001b[0m\n",
      "[\u001b[0;36mDEBUG\u001b[0m | src.tasks.clm_training.fabric.base]: \u001b[0;36mvalidate_after_k_steps value: 1000, type: <class 'int'>\u001b[0m\n",
      "2025-06-16 13:54:55 - src.tasks.clm_training.fabric.base - \u001b[0;36mDEBUG\u001b[0m - \u001b[0;36mstep_count value: 35, type: <class 'int'>\u001b[0m\n",
      "[\u001b[0;36mDEBUG\u001b[0m | src.tasks.clm_training.fabric.base]: \u001b[0;36mstep_count value: 35, type: <class 'int'>\u001b[0m\n",
      "2025-06-16 13:54:55 - src.tasks.clm_training.fabric.base - \u001b[0;36mDEBUG\u001b[0m - \u001b[0;36mValidation condition check - epochFinished: False, trainingFinished: False\u001b[0m\n",
      "[\u001b[0;36mDEBUG\u001b[0m | src.tasks.clm_training.fabric.base]: \u001b[0;36mValidation condition check - epochFinished: False, trainingFinished: False\u001b[0m\n",
      "2025-06-16 13:54:55 - src.tasks.clm_training.fabric.base - \u001b[0;36mDEBUG\u001b[0m - \u001b[0;36mAbout to check: step_count % validate_after_k_steps == 0 -> 35 % 1000 == 0\u001b[0m\n",
      "[\u001b[0;36mDEBUG\u001b[0m | src.tasks.clm_training.fabric.base]: \u001b[0;36mAbout to check: step_count % validate_after_k_steps == 0 -> 35 % 1000 == 0\u001b[0m\n",
      "2025-06-16 13:54:55 - src.tasks.clm_training.fabric.base - \u001b[0;36mDEBUG\u001b[0m - \u001b[0;36miter 70 step 35: loss 5.8750, iter time: 419.95ms remaining time: \u001b[0m\n",
      "[\u001b[0;36mDEBUG\u001b[0m | src.tasks.clm_training.fabric.base]: \u001b[0;36miter 70 step 35: loss 5.8750, iter time: 419.95ms remaining time: \u001b[0m\n",
      "  1%|\u001b[34m▌                                        \u001b[0m| 70/4682 [00:29<32:34,  2.36it/s]\u001b[0m2025-06-16 13:54:55 - src.tasks.clm_training.fabric.base - \u001b[0;36mDEBUG\u001b[0m - \u001b[0;36mgradient_accumulation_steps raw value: 2, type: <class 'int'>\u001b[0m\n",
      "[\u001b[0;36mDEBUG\u001b[0m | src.tasks.clm_training.fabric.base]: \u001b[0;36mgradient_accumulation_steps raw value: 2, type: <class 'int'>\u001b[0m\n",
      "2025-06-16 13:54:55 - src.tasks.clm_training.fabric.base - \u001b[0;36mDEBUG\u001b[0m - \u001b[0;36mgradient_accumulation_steps converted: 2, type: <class 'int'>\u001b[0m\n",
      "[\u001b[0;36mDEBUG\u001b[0m | src.tasks.clm_training.fabric.base]: \u001b[0;36mgradient_accumulation_steps converted: 2, type: <class 'int'>\u001b[0m\n",
      "2025-06-16 13:54:55 - src.tasks.clm_training.fabric.base - \u001b[0;36mDEBUG\u001b[0m - \u001b[0;36miter_num value: 70, type: <class 'int'>\u001b[0m\n",
      "[\u001b[0;36mDEBUG\u001b[0m | src.tasks.clm_training.fabric.base]: \u001b[0;36miter_num value: 70, type: <class 'int'>\u001b[0m\n",
      "2025-06-16 13:54:55 - src.tasks.clm_training.fabric.base - \u001b[0;36mDEBUG\u001b[0m - \u001b[0;36mAbout to check: (iter_num + 1) % gradient_accumulation_steps != 0 -> (70 + 1) % 2 != 0\u001b[0m\n",
      "[\u001b[0;36mDEBUG\u001b[0m | src.tasks.clm_training.fabric.base]: \u001b[0;36mAbout to check: (iter_num + 1) % gradient_accumulation_steps != 0 -> (70 + 1) % 2 != 0\u001b[0m\n",
      "2025-06-16 13:54:56 - src.tasks.clm_training.fabric.base - \u001b[0;36mDEBUG\u001b[0m - \u001b[0;36miter 71 step 35: loss 5.1250, iter time: 121.81ms remaining time: \u001b[0m\n",
      "[\u001b[0;36mDEBUG\u001b[0m | src.tasks.clm_training.fabric.base]: \u001b[0;36miter 71 step 35: loss 5.1250, iter time: 121.81ms remaining time: \u001b[0m\n",
      "  2%|\u001b[34m▌                                        \u001b[0m| 71/4682 [00:30<32:21,  2.37it/s]\u001b[0m2025-06-16 13:54:56 - src.tasks.clm_training.fabric.base - \u001b[0;36mDEBUG\u001b[0m - \u001b[0;36mgradient_accumulation_steps raw value: 2, type: <class 'int'>\u001b[0m\n",
      "[\u001b[0;36mDEBUG\u001b[0m | src.tasks.clm_training.fabric.base]: \u001b[0;36mgradient_accumulation_steps raw value: 2, type: <class 'int'>\u001b[0m\n",
      "2025-06-16 13:54:56 - src.tasks.clm_training.fabric.base - \u001b[0;36mDEBUG\u001b[0m - \u001b[0;36mgradient_accumulation_steps converted: 2, type: <class 'int'>\u001b[0m\n",
      "[\u001b[0;36mDEBUG\u001b[0m | src.tasks.clm_training.fabric.base]: \u001b[0;36mgradient_accumulation_steps converted: 2, type: <class 'int'>\u001b[0m\n",
      "2025-06-16 13:54:56 - src.tasks.clm_training.fabric.base - \u001b[0;36mDEBUG\u001b[0m - \u001b[0;36miter_num value: 71, type: <class 'int'>\u001b[0m\n",
      "[\u001b[0;36mDEBUG\u001b[0m | src.tasks.clm_training.fabric.base]: \u001b[0;36miter_num value: 71, type: <class 'int'>\u001b[0m\n",
      "2025-06-16 13:54:56 - src.tasks.clm_training.fabric.base - \u001b[0;36mDEBUG\u001b[0m - \u001b[0;36mAbout to check: (iter_num + 1) % gradient_accumulation_steps != 0 -> (71 + 1) % 2 != 0\u001b[0m\n",
      "[\u001b[0;36mDEBUG\u001b[0m | src.tasks.clm_training.fabric.base]: \u001b[0;36mAbout to check: (iter_num + 1) % gradient_accumulation_steps != 0 -> (71 + 1) % 2 != 0\u001b[0m\n",
      "2025-06-16 13:54:56 - src.tasks.clm_training.fabric.base - \u001b[0;36mDEBUG\u001b[0m - \u001b[0;36mGradient norm before clipping: 24.1250\u001b[0m\n",
      "[\u001b[0;36mDEBUG\u001b[0m | src.tasks.clm_training.fabric.base]: \u001b[0;36mGradient norm before clipping: 24.1250\u001b[0m\n",
      "2025-06-16 13:54:56 - src.tasks.clm_training.fabric.base - \u001b[0;36mDEBUG\u001b[0m - \u001b[0;36mvalidate_after_k_steps value: 1000, type: <class 'int'>\u001b[0m\n",
      "[\u001b[0;36mDEBUG\u001b[0m | src.tasks.clm_training.fabric.base]: \u001b[0;36mvalidate_after_k_steps value: 1000, type: <class 'int'>\u001b[0m\n",
      "2025-06-16 13:54:56 - src.tasks.clm_training.fabric.base - \u001b[0;36mDEBUG\u001b[0m - \u001b[0;36mstep_count value: 36, type: <class 'int'>\u001b[0m\n",
      "[\u001b[0;36mDEBUG\u001b[0m | src.tasks.clm_training.fabric.base]: \u001b[0;36mstep_count value: 36, type: <class 'int'>\u001b[0m\n",
      "2025-06-16 13:54:56 - src.tasks.clm_training.fabric.base - \u001b[0;36mDEBUG\u001b[0m - \u001b[0;36mValidation condition check - epochFinished: False, trainingFinished: False\u001b[0m\n",
      "[\u001b[0;36mDEBUG\u001b[0m | src.tasks.clm_training.fabric.base]: \u001b[0;36mValidation condition check - epochFinished: False, trainingFinished: False\u001b[0m\n",
      "2025-06-16 13:54:56 - src.tasks.clm_training.fabric.base - \u001b[0;36mDEBUG\u001b[0m - \u001b[0;36mAbout to check: step_count % validate_after_k_steps == 0 -> 36 % 1000 == 0\u001b[0m\n",
      "[\u001b[0;36mDEBUG\u001b[0m | src.tasks.clm_training.fabric.base]: \u001b[0;36mAbout to check: step_count % validate_after_k_steps == 0 -> 36 % 1000 == 0\u001b[0m\n",
      "2025-06-16 13:54:56 - src.tasks.clm_training.fabric.base - \u001b[0;36mDEBUG\u001b[0m - \u001b[0;36miter 72 step 36: loss 5.4062, iter time: 419.32ms remaining time: \u001b[0m\n",
      "[\u001b[0;36mDEBUG\u001b[0m | src.tasks.clm_training.fabric.base]: \u001b[0;36miter 72 step 36: loss 5.4062, iter time: 419.32ms remaining time: \u001b[0m\n",
      "  2%|\u001b[34m▋                                        \u001b[0m| 72/4682 [00:30<32:32,  2.36it/s]\u001b[0m2025-06-16 13:54:56 - src.tasks.clm_training.fabric.base - \u001b[0;36mDEBUG\u001b[0m - \u001b[0;36mgradient_accumulation_steps raw value: 2, type: <class 'int'>\u001b[0m\n",
      "[\u001b[0;36mDEBUG\u001b[0m | src.tasks.clm_training.fabric.base]: \u001b[0;36mgradient_accumulation_steps raw value: 2, type: <class 'int'>\u001b[0m\n",
      "2025-06-16 13:54:56 - src.tasks.clm_training.fabric.base - \u001b[0;36mDEBUG\u001b[0m - \u001b[0;36mgradient_accumulation_steps converted: 2, type: <class 'int'>\u001b[0m\n",
      "[\u001b[0;36mDEBUG\u001b[0m | src.tasks.clm_training.fabric.base]: \u001b[0;36mgradient_accumulation_steps converted: 2, type: <class 'int'>\u001b[0m\n",
      "2025-06-16 13:54:56 - src.tasks.clm_training.fabric.base - \u001b[0;36mDEBUG\u001b[0m - \u001b[0;36miter_num value: 72, type: <class 'int'>\u001b[0m\n",
      "[\u001b[0;36mDEBUG\u001b[0m | src.tasks.clm_training.fabric.base]: \u001b[0;36miter_num value: 72, type: <class 'int'>\u001b[0m\n",
      "2025-06-16 13:54:56 - src.tasks.clm_training.fabric.base - \u001b[0;36mDEBUG\u001b[0m - \u001b[0;36mAbout to check: (iter_num + 1) % gradient_accumulation_steps != 0 -> (72 + 1) % 2 != 0\u001b[0m\n",
      "[\u001b[0;36mDEBUG\u001b[0m | src.tasks.clm_training.fabric.base]: \u001b[0;36mAbout to check: (iter_num + 1) % gradient_accumulation_steps != 0 -> (72 + 1) % 2 != 0\u001b[0m\n",
      "2025-06-16 13:54:57 - src.tasks.clm_training.fabric.base - \u001b[0;36mDEBUG\u001b[0m - \u001b[0;36miter 73 step 36: loss 5.6562, iter time: 122.26ms remaining time: \u001b[0m\n",
      "[\u001b[0;36mDEBUG\u001b[0m | src.tasks.clm_training.fabric.base]: \u001b[0;36miter 73 step 36: loss 5.6562, iter time: 122.26ms remaining time: \u001b[0m\n",
      "  2%|\u001b[34m▋                                        \u001b[0m| 73/4682 [00:31<32:22,  2.37it/s]\u001b[0m2025-06-16 13:54:57 - src.tasks.clm_training.fabric.base - \u001b[0;36mDEBUG\u001b[0m - \u001b[0;36mgradient_accumulation_steps raw value: 2, type: <class 'int'>\u001b[0m\n",
      "[\u001b[0;36mDEBUG\u001b[0m | src.tasks.clm_training.fabric.base]: \u001b[0;36mgradient_accumulation_steps raw value: 2, type: <class 'int'>\u001b[0m\n",
      "2025-06-16 13:54:57 - src.tasks.clm_training.fabric.base - \u001b[0;36mDEBUG\u001b[0m - \u001b[0;36mgradient_accumulation_steps converted: 2, type: <class 'int'>\u001b[0m\n",
      "[\u001b[0;36mDEBUG\u001b[0m | src.tasks.clm_training.fabric.base]: \u001b[0;36mgradient_accumulation_steps converted: 2, type: <class 'int'>\u001b[0m\n",
      "2025-06-16 13:54:57 - src.tasks.clm_training.fabric.base - \u001b[0;36mDEBUG\u001b[0m - \u001b[0;36miter_num value: 73, type: <class 'int'>\u001b[0m\n",
      "[\u001b[0;36mDEBUG\u001b[0m | src.tasks.clm_training.fabric.base]: \u001b[0;36miter_num value: 73, type: <class 'int'>\u001b[0m\n",
      "2025-06-16 13:54:57 - src.tasks.clm_training.fabric.base - \u001b[0;36mDEBUG\u001b[0m - \u001b[0;36mAbout to check: (iter_num + 1) % gradient_accumulation_steps != 0 -> (73 + 1) % 2 != 0\u001b[0m\n",
      "[\u001b[0;36mDEBUG\u001b[0m | src.tasks.clm_training.fabric.base]: \u001b[0;36mAbout to check: (iter_num + 1) % gradient_accumulation_steps != 0 -> (73 + 1) % 2 != 0\u001b[0m\n",
      "2025-06-16 13:54:57 - src.tasks.clm_training.fabric.base - \u001b[0;36mDEBUG\u001b[0m - \u001b[0;36mGradient norm before clipping: 24.5000\u001b[0m\n",
      "[\u001b[0;36mDEBUG\u001b[0m | src.tasks.clm_training.fabric.base]: \u001b[0;36mGradient norm before clipping: 24.5000\u001b[0m\n",
      "2025-06-16 13:54:57 - src.tasks.clm_training.fabric.base - \u001b[0;36mDEBUG\u001b[0m - \u001b[0;36mvalidate_after_k_steps value: 1000, type: <class 'int'>\u001b[0m\n",
      "[\u001b[0;36mDEBUG\u001b[0m | src.tasks.clm_training.fabric.base]: \u001b[0;36mvalidate_after_k_steps value: 1000, type: <class 'int'>\u001b[0m\n",
      "2025-06-16 13:54:57 - src.tasks.clm_training.fabric.base - \u001b[0;36mDEBUG\u001b[0m - \u001b[0;36mstep_count value: 37, type: <class 'int'>\u001b[0m\n",
      "[\u001b[0;36mDEBUG\u001b[0m | src.tasks.clm_training.fabric.base]: \u001b[0;36mstep_count value: 37, type: <class 'int'>\u001b[0m\n",
      "2025-06-16 13:54:57 - src.tasks.clm_training.fabric.base - \u001b[0;36mDEBUG\u001b[0m - \u001b[0;36mValidation condition check - epochFinished: False, trainingFinished: False\u001b[0m\n",
      "[\u001b[0;36mDEBUG\u001b[0m | src.tasks.clm_training.fabric.base]: \u001b[0;36mValidation condition check - epochFinished: False, trainingFinished: False\u001b[0m\n",
      "2025-06-16 13:54:57 - src.tasks.clm_training.fabric.base - \u001b[0;36mDEBUG\u001b[0m - \u001b[0;36mAbout to check: step_count % validate_after_k_steps == 0 -> 37 % 1000 == 0\u001b[0m\n",
      "[\u001b[0;36mDEBUG\u001b[0m | src.tasks.clm_training.fabric.base]: \u001b[0;36mAbout to check: step_count % validate_after_k_steps == 0 -> 37 % 1000 == 0\u001b[0m\n",
      "2025-06-16 13:54:57 - src.tasks.clm_training.fabric.base - \u001b[0;36mDEBUG\u001b[0m - \u001b[0;36miter 74 step 37: loss 5.5625, iter time: 420.51ms remaining time: \u001b[0m\n",
      "[\u001b[0;36mDEBUG\u001b[0m | src.tasks.clm_training.fabric.base]: \u001b[0;36miter 74 step 37: loss 5.5625, iter time: 420.51ms remaining time: \u001b[0m\n",
      "  2%|\u001b[34m▋                                        \u001b[0m| 74/4682 [00:31<32:34,  2.36it/s]\u001b[0m2025-06-16 13:54:57 - src.tasks.clm_training.fabric.base - \u001b[0;36mDEBUG\u001b[0m - \u001b[0;36mgradient_accumulation_steps raw value: 2, type: <class 'int'>\u001b[0m\n",
      "[\u001b[0;36mDEBUG\u001b[0m | src.tasks.clm_training.fabric.base]: \u001b[0;36mgradient_accumulation_steps raw value: 2, type: <class 'int'>\u001b[0m\n",
      "2025-06-16 13:54:57 - src.tasks.clm_training.fabric.base - \u001b[0;36mDEBUG\u001b[0m - \u001b[0;36mgradient_accumulation_steps converted: 2, type: <class 'int'>\u001b[0m\n",
      "[\u001b[0;36mDEBUG\u001b[0m | src.tasks.clm_training.fabric.base]: \u001b[0;36mgradient_accumulation_steps converted: 2, type: <class 'int'>\u001b[0m\n",
      "2025-06-16 13:54:57 - src.tasks.clm_training.fabric.base - \u001b[0;36mDEBUG\u001b[0m - \u001b[0;36miter_num value: 74, type: <class 'int'>\u001b[0m\n",
      "[\u001b[0;36mDEBUG\u001b[0m | src.tasks.clm_training.fabric.base]: \u001b[0;36miter_num value: 74, type: <class 'int'>\u001b[0m\n",
      "2025-06-16 13:54:57 - src.tasks.clm_training.fabric.base - \u001b[0;36mDEBUG\u001b[0m - \u001b[0;36mAbout to check: (iter_num + 1) % gradient_accumulation_steps != 0 -> (74 + 1) % 2 != 0\u001b[0m\n",
      "[\u001b[0;36mDEBUG\u001b[0m | src.tasks.clm_training.fabric.base]: \u001b[0;36mAbout to check: (iter_num + 1) % gradient_accumulation_steps != 0 -> (74 + 1) % 2 != 0\u001b[0m\n",
      "2025-06-16 13:54:58 - src.tasks.clm_training.fabric.base - \u001b[0;36mDEBUG\u001b[0m - \u001b[0;36miter 75 step 37: loss 5.7188, iter time: 121.80ms remaining time: \u001b[0m\n",
      "[\u001b[0;36mDEBUG\u001b[0m | src.tasks.clm_training.fabric.base]: \u001b[0;36miter 75 step 37: loss 5.7188, iter time: 121.80ms remaining time: \u001b[0m\n",
      "  2%|\u001b[34m▋                                        \u001b[0m| 75/4682 [00:31<32:20,  2.37it/s]\u001b[0m2025-06-16 13:54:58 - src.tasks.clm_training.fabric.base - \u001b[0;36mDEBUG\u001b[0m - \u001b[0;36mgradient_accumulation_steps raw value: 2, type: <class 'int'>\u001b[0m\n",
      "[\u001b[0;36mDEBUG\u001b[0m | src.tasks.clm_training.fabric.base]: \u001b[0;36mgradient_accumulation_steps raw value: 2, type: <class 'int'>\u001b[0m\n",
      "2025-06-16 13:54:58 - src.tasks.clm_training.fabric.base - \u001b[0;36mDEBUG\u001b[0m - \u001b[0;36mgradient_accumulation_steps converted: 2, type: <class 'int'>\u001b[0m\n",
      "[\u001b[0;36mDEBUG\u001b[0m | src.tasks.clm_training.fabric.base]: \u001b[0;36mgradient_accumulation_steps converted: 2, type: <class 'int'>\u001b[0m\n",
      "2025-06-16 13:54:58 - src.tasks.clm_training.fabric.base - \u001b[0;36mDEBUG\u001b[0m - \u001b[0;36miter_num value: 75, type: <class 'int'>\u001b[0m\n",
      "[\u001b[0;36mDEBUG\u001b[0m | src.tasks.clm_training.fabric.base]: \u001b[0;36miter_num value: 75, type: <class 'int'>\u001b[0m\n",
      "2025-06-16 13:54:58 - src.tasks.clm_training.fabric.base - \u001b[0;36mDEBUG\u001b[0m - \u001b[0;36mAbout to check: (iter_num + 1) % gradient_accumulation_steps != 0 -> (75 + 1) % 2 != 0\u001b[0m\n",
      "[\u001b[0;36mDEBUG\u001b[0m | src.tasks.clm_training.fabric.base]: \u001b[0;36mAbout to check: (iter_num + 1) % gradient_accumulation_steps != 0 -> (75 + 1) % 2 != 0\u001b[0m\n",
      "2025-06-16 13:54:58 - src.tasks.clm_training.fabric.base - \u001b[0;36mDEBUG\u001b[0m - \u001b[0;36mGradient norm before clipping: 27.2500\u001b[0m\n",
      "[\u001b[0;36mDEBUG\u001b[0m | src.tasks.clm_training.fabric.base]: \u001b[0;36mGradient norm before clipping: 27.2500\u001b[0m\n",
      "2025-06-16 13:54:58 - src.tasks.clm_training.fabric.base - \u001b[0;36mDEBUG\u001b[0m - \u001b[0;36mvalidate_after_k_steps value: 1000, type: <class 'int'>\u001b[0m\n",
      "[\u001b[0;36mDEBUG\u001b[0m | src.tasks.clm_training.fabric.base]: \u001b[0;36mvalidate_after_k_steps value: 1000, type: <class 'int'>\u001b[0m\n",
      "2025-06-16 13:54:58 - src.tasks.clm_training.fabric.base - \u001b[0;36mDEBUG\u001b[0m - \u001b[0;36mstep_count value: 38, type: <class 'int'>\u001b[0m\n",
      "[\u001b[0;36mDEBUG\u001b[0m | src.tasks.clm_training.fabric.base]: \u001b[0;36mstep_count value: 38, type: <class 'int'>\u001b[0m\n",
      "2025-06-16 13:54:58 - src.tasks.clm_training.fabric.base - \u001b[0;36mDEBUG\u001b[0m - \u001b[0;36mValidation condition check - epochFinished: False, trainingFinished: False\u001b[0m\n",
      "[\u001b[0;36mDEBUG\u001b[0m | src.tasks.clm_training.fabric.base]: \u001b[0;36mValidation condition check - epochFinished: False, trainingFinished: False\u001b[0m\n",
      "2025-06-16 13:54:58 - src.tasks.clm_training.fabric.base - \u001b[0;36mDEBUG\u001b[0m - \u001b[0;36mAbout to check: step_count % validate_after_k_steps == 0 -> 38 % 1000 == 0\u001b[0m\n",
      "[\u001b[0;36mDEBUG\u001b[0m | src.tasks.clm_training.fabric.base]: \u001b[0;36mAbout to check: step_count % validate_after_k_steps == 0 -> 38 % 1000 == 0\u001b[0m\n",
      "2025-06-16 13:54:58 - src.tasks.clm_training.fabric.base - \u001b[0;36mDEBUG\u001b[0m - \u001b[0;36miter 76 step 38: loss 6.4062, iter time: 420.20ms remaining time: \u001b[0m\n",
      "[\u001b[0;36mDEBUG\u001b[0m | src.tasks.clm_training.fabric.base]: \u001b[0;36miter 76 step 38: loss 6.4062, iter time: 420.20ms remaining time: \u001b[0m\n",
      "  2%|\u001b[34m▋                                        \u001b[0m| 76/4682 [00:32<32:33,  2.36it/s]\u001b[0m2025-06-16 13:54:58 - src.tasks.clm_training.fabric.base - \u001b[0;36mDEBUG\u001b[0m - \u001b[0;36mgradient_accumulation_steps raw value: 2, type: <class 'int'>\u001b[0m\n",
      "[\u001b[0;36mDEBUG\u001b[0m | src.tasks.clm_training.fabric.base]: \u001b[0;36mgradient_accumulation_steps raw value: 2, type: <class 'int'>\u001b[0m\n",
      "2025-06-16 13:54:58 - src.tasks.clm_training.fabric.base - \u001b[0;36mDEBUG\u001b[0m - \u001b[0;36mgradient_accumulation_steps converted: 2, type: <class 'int'>\u001b[0m\n",
      "[\u001b[0;36mDEBUG\u001b[0m | src.tasks.clm_training.fabric.base]: \u001b[0;36mgradient_accumulation_steps converted: 2, type: <class 'int'>\u001b[0m\n",
      "2025-06-16 13:54:58 - src.tasks.clm_training.fabric.base - \u001b[0;36mDEBUG\u001b[0m - \u001b[0;36miter_num value: 76, type: <class 'int'>\u001b[0m\n",
      "[\u001b[0;36mDEBUG\u001b[0m | src.tasks.clm_training.fabric.base]: \u001b[0;36miter_num value: 76, type: <class 'int'>\u001b[0m\n",
      "2025-06-16 13:54:58 - src.tasks.clm_training.fabric.base - \u001b[0;36mDEBUG\u001b[0m - \u001b[0;36mAbout to check: (iter_num + 1) % gradient_accumulation_steps != 0 -> (76 + 1) % 2 != 0\u001b[0m\n",
      "[\u001b[0;36mDEBUG\u001b[0m | src.tasks.clm_training.fabric.base]: \u001b[0;36mAbout to check: (iter_num + 1) % gradient_accumulation_steps != 0 -> (76 + 1) % 2 != 0\u001b[0m\n",
      "2025-06-16 13:54:58 - src.tasks.clm_training.fabric.base - \u001b[0;36mDEBUG\u001b[0m - \u001b[0;36miter 77 step 38: loss 5.7500, iter time: 121.85ms remaining time: \u001b[0m\n",
      "[\u001b[0;36mDEBUG\u001b[0m | src.tasks.clm_training.fabric.base]: \u001b[0;36miter 77 step 38: loss 5.7500, iter time: 121.85ms remaining time: \u001b[0m\n",
      "  2%|\u001b[34m▋                                        \u001b[0m| 77/4682 [00:32<32:21,  2.37it/s]\u001b[0m2025-06-16 13:54:58 - src.tasks.clm_training.fabric.base - \u001b[0;36mDEBUG\u001b[0m - \u001b[0;36mgradient_accumulation_steps raw value: 2, type: <class 'int'>\u001b[0m\n",
      "[\u001b[0;36mDEBUG\u001b[0m | src.tasks.clm_training.fabric.base]: \u001b[0;36mgradient_accumulation_steps raw value: 2, type: <class 'int'>\u001b[0m\n",
      "2025-06-16 13:54:58 - src.tasks.clm_training.fabric.base - \u001b[0;36mDEBUG\u001b[0m - \u001b[0;36mgradient_accumulation_steps converted: 2, type: <class 'int'>\u001b[0m\n",
      "[\u001b[0;36mDEBUG\u001b[0m | src.tasks.clm_training.fabric.base]: \u001b[0;36mgradient_accumulation_steps converted: 2, type: <class 'int'>\u001b[0m\n",
      "2025-06-16 13:54:58 - src.tasks.clm_training.fabric.base - \u001b[0;36mDEBUG\u001b[0m - \u001b[0;36miter_num value: 77, type: <class 'int'>\u001b[0m\n",
      "[\u001b[0;36mDEBUG\u001b[0m | src.tasks.clm_training.fabric.base]: \u001b[0;36miter_num value: 77, type: <class 'int'>\u001b[0m\n",
      "2025-06-16 13:54:58 - src.tasks.clm_training.fabric.base - \u001b[0;36mDEBUG\u001b[0m - \u001b[0;36mAbout to check: (iter_num + 1) % gradient_accumulation_steps != 0 -> (77 + 1) % 2 != 0\u001b[0m\n",
      "[\u001b[0;36mDEBUG\u001b[0m | src.tasks.clm_training.fabric.base]: \u001b[0;36mAbout to check: (iter_num + 1) % gradient_accumulation_steps != 0 -> (77 + 1) % 2 != 0\u001b[0m\n",
      "2025-06-16 13:54:59 - src.tasks.clm_training.fabric.base - \u001b[0;36mDEBUG\u001b[0m - \u001b[0;36mGradient norm before clipping: 24.3750\u001b[0m\n",
      "[\u001b[0;36mDEBUG\u001b[0m | src.tasks.clm_training.fabric.base]: \u001b[0;36mGradient norm before clipping: 24.3750\u001b[0m\n",
      "2025-06-16 13:54:59 - src.tasks.clm_training.fabric.base - \u001b[0;36mDEBUG\u001b[0m - \u001b[0;36mvalidate_after_k_steps value: 1000, type: <class 'int'>\u001b[0m\n",
      "[\u001b[0;36mDEBUG\u001b[0m | src.tasks.clm_training.fabric.base]: \u001b[0;36mvalidate_after_k_steps value: 1000, type: <class 'int'>\u001b[0m\n",
      "2025-06-16 13:54:59 - src.tasks.clm_training.fabric.base - \u001b[0;36mDEBUG\u001b[0m - \u001b[0;36mstep_count value: 39, type: <class 'int'>\u001b[0m\n",
      "[\u001b[0;36mDEBUG\u001b[0m | src.tasks.clm_training.fabric.base]: \u001b[0;36mstep_count value: 39, type: <class 'int'>\u001b[0m\n",
      "2025-06-16 13:54:59 - src.tasks.clm_training.fabric.base - \u001b[0;36mDEBUG\u001b[0m - \u001b[0;36mValidation condition check - epochFinished: False, trainingFinished: False\u001b[0m\n",
      "[\u001b[0;36mDEBUG\u001b[0m | src.tasks.clm_training.fabric.base]: \u001b[0;36mValidation condition check - epochFinished: False, trainingFinished: False\u001b[0m\n",
      "2025-06-16 13:54:59 - src.tasks.clm_training.fabric.base - \u001b[0;36mDEBUG\u001b[0m - \u001b[0;36mAbout to check: step_count % validate_after_k_steps == 0 -> 39 % 1000 == 0\u001b[0m\n",
      "[\u001b[0;36mDEBUG\u001b[0m | src.tasks.clm_training.fabric.base]: \u001b[0;36mAbout to check: step_count % validate_after_k_steps == 0 -> 39 % 1000 == 0\u001b[0m\n",
      "2025-06-16 13:54:59 - src.tasks.clm_training.fabric.base - \u001b[0;36mDEBUG\u001b[0m - \u001b[0;36miter 78 step 39: loss 6.3125, iter time: 419.46ms remaining time: \u001b[0m\n",
      "[\u001b[0;36mDEBUG\u001b[0m | src.tasks.clm_training.fabric.base]: \u001b[0;36miter 78 step 39: loss 6.3125, iter time: 419.46ms remaining time: \u001b[0m\n",
      "  2%|\u001b[34m▋                                        \u001b[0m| 78/4682 [00:33<32:31,  2.36it/s]\u001b[0m2025-06-16 13:54:59 - src.tasks.clm_training.fabric.base - \u001b[0;36mDEBUG\u001b[0m - \u001b[0;36mgradient_accumulation_steps raw value: 2, type: <class 'int'>\u001b[0m\n",
      "[\u001b[0;36mDEBUG\u001b[0m | src.tasks.clm_training.fabric.base]: \u001b[0;36mgradient_accumulation_steps raw value: 2, type: <class 'int'>\u001b[0m\n",
      "2025-06-16 13:54:59 - src.tasks.clm_training.fabric.base - \u001b[0;36mDEBUG\u001b[0m - \u001b[0;36mgradient_accumulation_steps converted: 2, type: <class 'int'>\u001b[0m\n",
      "[\u001b[0;36mDEBUG\u001b[0m | src.tasks.clm_training.fabric.base]: \u001b[0;36mgradient_accumulation_steps converted: 2, type: <class 'int'>\u001b[0m\n",
      "2025-06-16 13:54:59 - src.tasks.clm_training.fabric.base - \u001b[0;36mDEBUG\u001b[0m - \u001b[0;36miter_num value: 78, type: <class 'int'>\u001b[0m\n",
      "[\u001b[0;36mDEBUG\u001b[0m | src.tasks.clm_training.fabric.base]: \u001b[0;36miter_num value: 78, type: <class 'int'>\u001b[0m\n",
      "2025-06-16 13:54:59 - src.tasks.clm_training.fabric.base - \u001b[0;36mDEBUG\u001b[0m - \u001b[0;36mAbout to check: (iter_num + 1) % gradient_accumulation_steps != 0 -> (78 + 1) % 2 != 0\u001b[0m\n",
      "[\u001b[0;36mDEBUG\u001b[0m | src.tasks.clm_training.fabric.base]: \u001b[0;36mAbout to check: (iter_num + 1) % gradient_accumulation_steps != 0 -> (78 + 1) % 2 != 0\u001b[0m\n",
      "2025-06-16 13:54:59 - src.tasks.clm_training.fabric.base - \u001b[0;36mDEBUG\u001b[0m - \u001b[0;36miter 79 step 39: loss 5.6562, iter time: 121.99ms remaining time: \u001b[0m\n",
      "[\u001b[0;36mDEBUG\u001b[0m | src.tasks.clm_training.fabric.base]: \u001b[0;36miter 79 step 39: loss 5.6562, iter time: 121.99ms remaining time: \u001b[0m\n",
      "  2%|\u001b[34m▋                                        \u001b[0m| 79/4682 [00:33<32:18,  2.37it/s]\u001b[0m2025-06-16 13:54:59 - src.tasks.clm_training.fabric.base - \u001b[0;36mDEBUG\u001b[0m - \u001b[0;36mgradient_accumulation_steps raw value: 2, type: <class 'int'>\u001b[0m\n",
      "[\u001b[0;36mDEBUG\u001b[0m | src.tasks.clm_training.fabric.base]: \u001b[0;36mgradient_accumulation_steps raw value: 2, type: <class 'int'>\u001b[0m\n",
      "2025-06-16 13:54:59 - src.tasks.clm_training.fabric.base - \u001b[0;36mDEBUG\u001b[0m - \u001b[0;36mgradient_accumulation_steps converted: 2, type: <class 'int'>\u001b[0m\n",
      "[\u001b[0;36mDEBUG\u001b[0m | src.tasks.clm_training.fabric.base]: \u001b[0;36mgradient_accumulation_steps converted: 2, type: <class 'int'>\u001b[0m\n",
      "2025-06-16 13:54:59 - src.tasks.clm_training.fabric.base - \u001b[0;36mDEBUG\u001b[0m - \u001b[0;36miter_num value: 79, type: <class 'int'>\u001b[0m\n",
      "[\u001b[0;36mDEBUG\u001b[0m | src.tasks.clm_training.fabric.base]: \u001b[0;36miter_num value: 79, type: <class 'int'>\u001b[0m\n",
      "2025-06-16 13:54:59 - src.tasks.clm_training.fabric.base - \u001b[0;36mDEBUG\u001b[0m - \u001b[0;36mAbout to check: (iter_num + 1) % gradient_accumulation_steps != 0 -> (79 + 1) % 2 != 0\u001b[0m\n",
      "[\u001b[0;36mDEBUG\u001b[0m | src.tasks.clm_training.fabric.base]: \u001b[0;36mAbout to check: (iter_num + 1) % gradient_accumulation_steps != 0 -> (79 + 1) % 2 != 0\u001b[0m\n",
      "2025-06-16 13:55:00 - src.tasks.clm_training.fabric.base - \u001b[0;36mDEBUG\u001b[0m - \u001b[0;36mGradient norm before clipping: 25.5000\u001b[0m\n",
      "[\u001b[0;36mDEBUG\u001b[0m | src.tasks.clm_training.fabric.base]: \u001b[0;36mGradient norm before clipping: 25.5000\u001b[0m\n",
      "2025-06-16 13:55:00 - src.tasks.clm_training.fabric.base - \u001b[0;36mDEBUG\u001b[0m - \u001b[0;36mvalidate_after_k_steps value: 1000, type: <class 'int'>\u001b[0m\n",
      "[\u001b[0;36mDEBUG\u001b[0m | src.tasks.clm_training.fabric.base]: \u001b[0;36mvalidate_after_k_steps value: 1000, type: <class 'int'>\u001b[0m\n",
      "2025-06-16 13:55:00 - src.tasks.clm_training.fabric.base - \u001b[0;36mDEBUG\u001b[0m - \u001b[0;36mstep_count value: 40, type: <class 'int'>\u001b[0m\n",
      "[\u001b[0;36mDEBUG\u001b[0m | src.tasks.clm_training.fabric.base]: \u001b[0;36mstep_count value: 40, type: <class 'int'>\u001b[0m\n",
      "2025-06-16 13:55:00 - src.tasks.clm_training.fabric.base - \u001b[0;36mDEBUG\u001b[0m - \u001b[0;36mValidation condition check - epochFinished: False, trainingFinished: False\u001b[0m\n",
      "[\u001b[0;36mDEBUG\u001b[0m | src.tasks.clm_training.fabric.base]: \u001b[0;36mValidation condition check - epochFinished: False, trainingFinished: False\u001b[0m\n",
      "2025-06-16 13:55:00 - src.tasks.clm_training.fabric.base - \u001b[0;36mDEBUG\u001b[0m - \u001b[0;36mAbout to check: step_count % validate_after_k_steps == 0 -> 40 % 1000 == 0\u001b[0m\n",
      "[\u001b[0;36mDEBUG\u001b[0m | src.tasks.clm_training.fabric.base]: \u001b[0;36mAbout to check: step_count % validate_after_k_steps == 0 -> 40 % 1000 == 0\u001b[0m\n",
      "2025-06-16 13:55:00 - src.tasks.clm_training.fabric.base - \u001b[0;36mDEBUG\u001b[0m - \u001b[0;36miter 80 step 40: loss 5.4688, iter time: 419.60ms remaining time: \u001b[0m\n",
      "[\u001b[0;36mDEBUG\u001b[0m | src.tasks.clm_training.fabric.base]: \u001b[0;36miter 80 step 40: loss 5.4688, iter time: 419.60ms remaining time: \u001b[0m\n",
      "  2%|\u001b[34m▋                                        \u001b[0m| 80/4682 [00:34<32:29,  2.36it/s]\u001b[0m2025-06-16 13:55:00 - src.tasks.clm_training.fabric.base - \u001b[0;36mDEBUG\u001b[0m - \u001b[0;36mgradient_accumulation_steps raw value: 2, type: <class 'int'>\u001b[0m\n",
      "[\u001b[0;36mDEBUG\u001b[0m | src.tasks.clm_training.fabric.base]: \u001b[0;36mgradient_accumulation_steps raw value: 2, type: <class 'int'>\u001b[0m\n",
      "2025-06-16 13:55:00 - src.tasks.clm_training.fabric.base - \u001b[0;36mDEBUG\u001b[0m - \u001b[0;36mgradient_accumulation_steps converted: 2, type: <class 'int'>\u001b[0m\n",
      "[\u001b[0;36mDEBUG\u001b[0m | src.tasks.clm_training.fabric.base]: \u001b[0;36mgradient_accumulation_steps converted: 2, type: <class 'int'>\u001b[0m\n",
      "2025-06-16 13:55:00 - src.tasks.clm_training.fabric.base - \u001b[0;36mDEBUG\u001b[0m - \u001b[0;36miter_num value: 80, type: <class 'int'>\u001b[0m\n",
      "[\u001b[0;36mDEBUG\u001b[0m | src.tasks.clm_training.fabric.base]: \u001b[0;36miter_num value: 80, type: <class 'int'>\u001b[0m\n",
      "2025-06-16 13:55:00 - src.tasks.clm_training.fabric.base - \u001b[0;36mDEBUG\u001b[0m - \u001b[0;36mAbout to check: (iter_num + 1) % gradient_accumulation_steps != 0 -> (80 + 1) % 2 != 0\u001b[0m\n",
      "[\u001b[0;36mDEBUG\u001b[0m | src.tasks.clm_training.fabric.base]: \u001b[0;36mAbout to check: (iter_num + 1) % gradient_accumulation_steps != 0 -> (80 + 1) % 2 != 0\u001b[0m\n",
      "2025-06-16 13:55:00 - src.tasks.clm_training.fabric.base - \u001b[0;36mDEBUG\u001b[0m - \u001b[0;36miter 81 step 40: loss 5.5000, iter time: 121.96ms remaining time: \u001b[0m\n",
      "[\u001b[0;36mDEBUG\u001b[0m | src.tasks.clm_training.fabric.base]: \u001b[0;36miter 81 step 40: loss 5.5000, iter time: 121.96ms remaining time: \u001b[0m\n",
      "  2%|\u001b[34m▋                                        \u001b[0m| 81/4682 [00:34<32:17,  2.37it/s]\u001b[0m2025-06-16 13:55:00 - src.tasks.clm_training.fabric.base - \u001b[0;36mDEBUG\u001b[0m - \u001b[0;36mgradient_accumulation_steps raw value: 2, type: <class 'int'>\u001b[0m\n",
      "[\u001b[0;36mDEBUG\u001b[0m | src.tasks.clm_training.fabric.base]: \u001b[0;36mgradient_accumulation_steps raw value: 2, type: <class 'int'>\u001b[0m\n",
      "2025-06-16 13:55:00 - src.tasks.clm_training.fabric.base - \u001b[0;36mDEBUG\u001b[0m - \u001b[0;36mgradient_accumulation_steps converted: 2, type: <class 'int'>\u001b[0m\n",
      "[\u001b[0;36mDEBUG\u001b[0m | src.tasks.clm_training.fabric.base]: \u001b[0;36mgradient_accumulation_steps converted: 2, type: <class 'int'>\u001b[0m\n",
      "2025-06-16 13:55:00 - src.tasks.clm_training.fabric.base - \u001b[0;36mDEBUG\u001b[0m - \u001b[0;36miter_num value: 81, type: <class 'int'>\u001b[0m\n",
      "[\u001b[0;36mDEBUG\u001b[0m | src.tasks.clm_training.fabric.base]: \u001b[0;36miter_num value: 81, type: <class 'int'>\u001b[0m\n",
      "2025-06-16 13:55:00 - src.tasks.clm_training.fabric.base - \u001b[0;36mDEBUG\u001b[0m - \u001b[0;36mAbout to check: (iter_num + 1) % gradient_accumulation_steps != 0 -> (81 + 1) % 2 != 0\u001b[0m\n",
      "[\u001b[0;36mDEBUG\u001b[0m | src.tasks.clm_training.fabric.base]: \u001b[0;36mAbout to check: (iter_num + 1) % gradient_accumulation_steps != 0 -> (81 + 1) % 2 != 0\u001b[0m\n",
      "2025-06-16 13:55:00 - src.tasks.clm_training.fabric.base - \u001b[0;36mDEBUG\u001b[0m - \u001b[0;36mGradient norm before clipping: 25.3750\u001b[0m\n",
      "[\u001b[0;36mDEBUG\u001b[0m | src.tasks.clm_training.fabric.base]: \u001b[0;36mGradient norm before clipping: 25.3750\u001b[0m\n",
      "2025-06-16 13:55:00 - src.tasks.clm_training.fabric.base - \u001b[0;36mDEBUG\u001b[0m - \u001b[0;36mvalidate_after_k_steps value: 1000, type: <class 'int'>\u001b[0m\n",
      "[\u001b[0;36mDEBUG\u001b[0m | src.tasks.clm_training.fabric.base]: \u001b[0;36mvalidate_after_k_steps value: 1000, type: <class 'int'>\u001b[0m\n",
      "2025-06-16 13:55:00 - src.tasks.clm_training.fabric.base - \u001b[0;36mDEBUG\u001b[0m - \u001b[0;36mstep_count value: 41, type: <class 'int'>\u001b[0m\n",
      "[\u001b[0;36mDEBUG\u001b[0m | src.tasks.clm_training.fabric.base]: \u001b[0;36mstep_count value: 41, type: <class 'int'>\u001b[0m\n",
      "2025-06-16 13:55:00 - src.tasks.clm_training.fabric.base - \u001b[0;36mDEBUG\u001b[0m - \u001b[0;36mValidation condition check - epochFinished: False, trainingFinished: False\u001b[0m\n",
      "[\u001b[0;36mDEBUG\u001b[0m | src.tasks.clm_training.fabric.base]: \u001b[0;36mValidation condition check - epochFinished: False, trainingFinished: False\u001b[0m\n",
      "2025-06-16 13:55:00 - src.tasks.clm_training.fabric.base - \u001b[0;36mDEBUG\u001b[0m - \u001b[0;36mAbout to check: step_count % validate_after_k_steps == 0 -> 41 % 1000 == 0\u001b[0m\n",
      "[\u001b[0;36mDEBUG\u001b[0m | src.tasks.clm_training.fabric.base]: \u001b[0;36mAbout to check: step_count % validate_after_k_steps == 0 -> 41 % 1000 == 0\u001b[0m\n",
      "2025-06-16 13:55:00 - src.tasks.clm_training.fabric.base - \u001b[0;36mDEBUG\u001b[0m - \u001b[0;36miter 82 step 41: loss 5.7812, iter time: 419.87ms remaining time: \u001b[0m\n",
      "[\u001b[0;36mDEBUG\u001b[0m | src.tasks.clm_training.fabric.base]: \u001b[0;36miter 82 step 41: loss 5.7812, iter time: 419.87ms remaining time: \u001b[0m\n",
      "  2%|\u001b[34m▋                                        \u001b[0m| 82/4682 [00:34<32:29,  2.36it/s]\u001b[0m2025-06-16 13:55:01 - src.tasks.clm_training.fabric.base - \u001b[0;36mDEBUG\u001b[0m - \u001b[0;36mgradient_accumulation_steps raw value: 2, type: <class 'int'>\u001b[0m\n",
      "[\u001b[0;36mDEBUG\u001b[0m | src.tasks.clm_training.fabric.base]: \u001b[0;36mgradient_accumulation_steps raw value: 2, type: <class 'int'>\u001b[0m\n",
      "2025-06-16 13:55:01 - src.tasks.clm_training.fabric.base - \u001b[0;36mDEBUG\u001b[0m - \u001b[0;36mgradient_accumulation_steps converted: 2, type: <class 'int'>\u001b[0m\n",
      "[\u001b[0;36mDEBUG\u001b[0m | src.tasks.clm_training.fabric.base]: \u001b[0;36mgradient_accumulation_steps converted: 2, type: <class 'int'>\u001b[0m\n",
      "2025-06-16 13:55:01 - src.tasks.clm_training.fabric.base - \u001b[0;36mDEBUG\u001b[0m - \u001b[0;36miter_num value: 82, type: <class 'int'>\u001b[0m\n",
      "[\u001b[0;36mDEBUG\u001b[0m | src.tasks.clm_training.fabric.base]: \u001b[0;36miter_num value: 82, type: <class 'int'>\u001b[0m\n",
      "2025-06-16 13:55:01 - src.tasks.clm_training.fabric.base - \u001b[0;36mDEBUG\u001b[0m - \u001b[0;36mAbout to check: (iter_num + 1) % gradient_accumulation_steps != 0 -> (82 + 1) % 2 != 0\u001b[0m\n",
      "[\u001b[0;36mDEBUG\u001b[0m | src.tasks.clm_training.fabric.base]: \u001b[0;36mAbout to check: (iter_num + 1) % gradient_accumulation_steps != 0 -> (82 + 1) % 2 != 0\u001b[0m\n",
      "2025-06-16 13:55:01 - src.tasks.clm_training.fabric.base - \u001b[0;36mDEBUG\u001b[0m - \u001b[0;36miter 83 step 41: loss 5.9375, iter time: 122.11ms remaining time: \u001b[0m\n",
      "[\u001b[0;36mDEBUG\u001b[0m | src.tasks.clm_training.fabric.base]: \u001b[0;36miter 83 step 41: loss 5.9375, iter time: 122.11ms remaining time: \u001b[0m\n",
      "  2%|\u001b[34m▋                                        \u001b[0m| 83/4682 [00:35<32:18,  2.37it/s]\u001b[0m2025-06-16 13:55:01 - src.tasks.clm_training.fabric.base - \u001b[0;36mDEBUG\u001b[0m - \u001b[0;36mgradient_accumulation_steps raw value: 2, type: <class 'int'>\u001b[0m\n",
      "[\u001b[0;36mDEBUG\u001b[0m | src.tasks.clm_training.fabric.base]: \u001b[0;36mgradient_accumulation_steps raw value: 2, type: <class 'int'>\u001b[0m\n",
      "2025-06-16 13:55:01 - src.tasks.clm_training.fabric.base - \u001b[0;36mDEBUG\u001b[0m - \u001b[0;36mgradient_accumulation_steps converted: 2, type: <class 'int'>\u001b[0m\n",
      "[\u001b[0;36mDEBUG\u001b[0m | src.tasks.clm_training.fabric.base]: \u001b[0;36mgradient_accumulation_steps converted: 2, type: <class 'int'>\u001b[0m\n",
      "2025-06-16 13:55:01 - src.tasks.clm_training.fabric.base - \u001b[0;36mDEBUG\u001b[0m - \u001b[0;36miter_num value: 83, type: <class 'int'>\u001b[0m\n",
      "[\u001b[0;36mDEBUG\u001b[0m | src.tasks.clm_training.fabric.base]: \u001b[0;36miter_num value: 83, type: <class 'int'>\u001b[0m\n",
      "2025-06-16 13:55:01 - src.tasks.clm_training.fabric.base - \u001b[0;36mDEBUG\u001b[0m - \u001b[0;36mAbout to check: (iter_num + 1) % gradient_accumulation_steps != 0 -> (83 + 1) % 2 != 0\u001b[0m\n",
      "[\u001b[0;36mDEBUG\u001b[0m | src.tasks.clm_training.fabric.base]: \u001b[0;36mAbout to check: (iter_num + 1) % gradient_accumulation_steps != 0 -> (83 + 1) % 2 != 0\u001b[0m\n",
      "2025-06-16 13:55:01 - src.tasks.clm_training.fabric.base - \u001b[0;36mDEBUG\u001b[0m - \u001b[0;36mGradient norm before clipping: 22.1250\u001b[0m\n",
      "[\u001b[0;36mDEBUG\u001b[0m | src.tasks.clm_training.fabric.base]: \u001b[0;36mGradient norm before clipping: 22.1250\u001b[0m\n",
      "2025-06-16 13:55:01 - src.tasks.clm_training.fabric.base - \u001b[0;36mDEBUG\u001b[0m - \u001b[0;36mvalidate_after_k_steps value: 1000, type: <class 'int'>\u001b[0m\n",
      "[\u001b[0;36mDEBUG\u001b[0m | src.tasks.clm_training.fabric.base]: \u001b[0;36mvalidate_after_k_steps value: 1000, type: <class 'int'>\u001b[0m\n",
      "2025-06-16 13:55:01 - src.tasks.clm_training.fabric.base - \u001b[0;36mDEBUG\u001b[0m - \u001b[0;36mstep_count value: 42, type: <class 'int'>\u001b[0m\n",
      "[\u001b[0;36mDEBUG\u001b[0m | src.tasks.clm_training.fabric.base]: \u001b[0;36mstep_count value: 42, type: <class 'int'>\u001b[0m\n",
      "2025-06-16 13:55:01 - src.tasks.clm_training.fabric.base - \u001b[0;36mDEBUG\u001b[0m - \u001b[0;36mValidation condition check - epochFinished: False, trainingFinished: False\u001b[0m\n",
      "[\u001b[0;36mDEBUG\u001b[0m | src.tasks.clm_training.fabric.base]: \u001b[0;36mValidation condition check - epochFinished: False, trainingFinished: False\u001b[0m\n",
      "2025-06-16 13:55:01 - src.tasks.clm_training.fabric.base - \u001b[0;36mDEBUG\u001b[0m - \u001b[0;36mAbout to check: step_count % validate_after_k_steps == 0 -> 42 % 1000 == 0\u001b[0m\n",
      "[\u001b[0;36mDEBUG\u001b[0m | src.tasks.clm_training.fabric.base]: \u001b[0;36mAbout to check: step_count % validate_after_k_steps == 0 -> 42 % 1000 == 0\u001b[0m\n",
      "2025-06-16 13:55:01 - src.tasks.clm_training.fabric.base - \u001b[0;36mDEBUG\u001b[0m - \u001b[0;36miter 84 step 42: loss 5.5312, iter time: 420.25ms remaining time: \u001b[0m\n",
      "[\u001b[0;36mDEBUG\u001b[0m | src.tasks.clm_training.fabric.base]: \u001b[0;36miter 84 step 42: loss 5.5312, iter time: 420.25ms remaining time: \u001b[0m\n",
      "  2%|\u001b[34m▋                                        \u001b[0m| 84/4682 [00:35<32:29,  2.36it/s]\u001b[0m2025-06-16 13:55:01 - src.tasks.clm_training.fabric.base - \u001b[0;36mDEBUG\u001b[0m - \u001b[0;36mgradient_accumulation_steps raw value: 2, type: <class 'int'>\u001b[0m\n",
      "[\u001b[0;36mDEBUG\u001b[0m | src.tasks.clm_training.fabric.base]: \u001b[0;36mgradient_accumulation_steps raw value: 2, type: <class 'int'>\u001b[0m\n",
      "2025-06-16 13:55:01 - src.tasks.clm_training.fabric.base - \u001b[0;36mDEBUG\u001b[0m - \u001b[0;36mgradient_accumulation_steps converted: 2, type: <class 'int'>\u001b[0m\n",
      "[\u001b[0;36mDEBUG\u001b[0m | src.tasks.clm_training.fabric.base]: \u001b[0;36mgradient_accumulation_steps converted: 2, type: <class 'int'>\u001b[0m\n",
      "2025-06-16 13:55:01 - src.tasks.clm_training.fabric.base - \u001b[0;36mDEBUG\u001b[0m - \u001b[0;36miter_num value: 84, type: <class 'int'>\u001b[0m\n",
      "[\u001b[0;36mDEBUG\u001b[0m | src.tasks.clm_training.fabric.base]: \u001b[0;36miter_num value: 84, type: <class 'int'>\u001b[0m\n",
      "2025-06-16 13:55:01 - src.tasks.clm_training.fabric.base - \u001b[0;36mDEBUG\u001b[0m - \u001b[0;36mAbout to check: (iter_num + 1) % gradient_accumulation_steps != 0 -> (84 + 1) % 2 != 0\u001b[0m\n",
      "[\u001b[0;36mDEBUG\u001b[0m | src.tasks.clm_training.fabric.base]: \u001b[0;36mAbout to check: (iter_num + 1) % gradient_accumulation_steps != 0 -> (84 + 1) % 2 != 0\u001b[0m\n",
      "2025-06-16 13:55:02 - src.tasks.clm_training.fabric.base - \u001b[0;36mDEBUG\u001b[0m - \u001b[0;36miter 85 step 42: loss 5.6250, iter time: 122.13ms remaining time: \u001b[0m\n",
      "[\u001b[0;36mDEBUG\u001b[0m | src.tasks.clm_training.fabric.base]: \u001b[0;36miter 85 step 42: loss 5.6250, iter time: 122.13ms remaining time: \u001b[0m\n",
      "  2%|\u001b[34m▋                                        \u001b[0m| 85/4682 [00:36<32:17,  2.37it/s]\u001b[0m2025-06-16 13:55:02 - src.tasks.clm_training.fabric.base - \u001b[0;36mDEBUG\u001b[0m - \u001b[0;36mgradient_accumulation_steps raw value: 2, type: <class 'int'>\u001b[0m\n",
      "[\u001b[0;36mDEBUG\u001b[0m | src.tasks.clm_training.fabric.base]: \u001b[0;36mgradient_accumulation_steps raw value: 2, type: <class 'int'>\u001b[0m\n",
      "2025-06-16 13:55:02 - src.tasks.clm_training.fabric.base - \u001b[0;36mDEBUG\u001b[0m - \u001b[0;36mgradient_accumulation_steps converted: 2, type: <class 'int'>\u001b[0m\n",
      "[\u001b[0;36mDEBUG\u001b[0m | src.tasks.clm_training.fabric.base]: \u001b[0;36mgradient_accumulation_steps converted: 2, type: <class 'int'>\u001b[0m\n",
      "2025-06-16 13:55:02 - src.tasks.clm_training.fabric.base - \u001b[0;36mDEBUG\u001b[0m - \u001b[0;36miter_num value: 85, type: <class 'int'>\u001b[0m\n",
      "[\u001b[0;36mDEBUG\u001b[0m | src.tasks.clm_training.fabric.base]: \u001b[0;36miter_num value: 85, type: <class 'int'>\u001b[0m\n",
      "2025-06-16 13:55:02 - src.tasks.clm_training.fabric.base - \u001b[0;36mDEBUG\u001b[0m - \u001b[0;36mAbout to check: (iter_num + 1) % gradient_accumulation_steps != 0 -> (85 + 1) % 2 != 0\u001b[0m\n",
      "[\u001b[0;36mDEBUG\u001b[0m | src.tasks.clm_training.fabric.base]: \u001b[0;36mAbout to check: (iter_num + 1) % gradient_accumulation_steps != 0 -> (85 + 1) % 2 != 0\u001b[0m\n",
      "2025-06-16 13:55:02 - src.tasks.clm_training.fabric.base - \u001b[0;36mDEBUG\u001b[0m - \u001b[0;36mGradient norm before clipping: 22.2500\u001b[0m\n",
      "[\u001b[0;36mDEBUG\u001b[0m | src.tasks.clm_training.fabric.base]: \u001b[0;36mGradient norm before clipping: 22.2500\u001b[0m\n",
      "2025-06-16 13:55:02 - src.tasks.clm_training.fabric.base - \u001b[0;36mDEBUG\u001b[0m - \u001b[0;36mvalidate_after_k_steps value: 1000, type: <class 'int'>\u001b[0m\n",
      "[\u001b[0;36mDEBUG\u001b[0m | src.tasks.clm_training.fabric.base]: \u001b[0;36mvalidate_after_k_steps value: 1000, type: <class 'int'>\u001b[0m\n",
      "2025-06-16 13:55:02 - src.tasks.clm_training.fabric.base - \u001b[0;36mDEBUG\u001b[0m - \u001b[0;36mstep_count value: 43, type: <class 'int'>\u001b[0m\n",
      "[\u001b[0;36mDEBUG\u001b[0m | src.tasks.clm_training.fabric.base]: \u001b[0;36mstep_count value: 43, type: <class 'int'>\u001b[0m\n",
      "2025-06-16 13:55:02 - src.tasks.clm_training.fabric.base - \u001b[0;36mDEBUG\u001b[0m - \u001b[0;36mValidation condition check - epochFinished: False, trainingFinished: False\u001b[0m\n",
      "[\u001b[0;36mDEBUG\u001b[0m | src.tasks.clm_training.fabric.base]: \u001b[0;36mValidation condition check - epochFinished: False, trainingFinished: False\u001b[0m\n",
      "2025-06-16 13:55:02 - src.tasks.clm_training.fabric.base - \u001b[0;36mDEBUG\u001b[0m - \u001b[0;36mAbout to check: step_count % validate_after_k_steps == 0 -> 43 % 1000 == 0\u001b[0m\n",
      "[\u001b[0;36mDEBUG\u001b[0m | src.tasks.clm_training.fabric.base]: \u001b[0;36mAbout to check: step_count % validate_after_k_steps == 0 -> 43 % 1000 == 0\u001b[0m\n",
      "2025-06-16 13:55:02 - src.tasks.clm_training.fabric.base - \u001b[0;36mDEBUG\u001b[0m - \u001b[0;36miter 86 step 43: loss 5.6562, iter time: 421.18ms remaining time: \u001b[0m\n",
      "[\u001b[0;36mDEBUG\u001b[0m | src.tasks.clm_training.fabric.base]: \u001b[0;36miter 86 step 43: loss 5.6562, iter time: 421.18ms remaining time: \u001b[0m\n",
      "  2%|\u001b[34m▊                                        \u001b[0m| 86/4682 [00:36<32:30,  2.36it/s]\u001b[0m2025-06-16 13:55:02 - src.tasks.clm_training.fabric.base - \u001b[0;36mDEBUG\u001b[0m - \u001b[0;36mgradient_accumulation_steps raw value: 2, type: <class 'int'>\u001b[0m\n",
      "[\u001b[0;36mDEBUG\u001b[0m | src.tasks.clm_training.fabric.base]: \u001b[0;36mgradient_accumulation_steps raw value: 2, type: <class 'int'>\u001b[0m\n",
      "2025-06-16 13:55:02 - src.tasks.clm_training.fabric.base - \u001b[0;36mDEBUG\u001b[0m - \u001b[0;36mgradient_accumulation_steps converted: 2, type: <class 'int'>\u001b[0m\n",
      "[\u001b[0;36mDEBUG\u001b[0m | src.tasks.clm_training.fabric.base]: \u001b[0;36mgradient_accumulation_steps converted: 2, type: <class 'int'>\u001b[0m\n",
      "2025-06-16 13:55:02 - src.tasks.clm_training.fabric.base - \u001b[0;36mDEBUG\u001b[0m - \u001b[0;36miter_num value: 86, type: <class 'int'>\u001b[0m\n",
      "[\u001b[0;36mDEBUG\u001b[0m | src.tasks.clm_training.fabric.base]: \u001b[0;36miter_num value: 86, type: <class 'int'>\u001b[0m\n",
      "2025-06-16 13:55:02 - src.tasks.clm_training.fabric.base - \u001b[0;36mDEBUG\u001b[0m - \u001b[0;36mAbout to check: (iter_num + 1) % gradient_accumulation_steps != 0 -> (86 + 1) % 2 != 0\u001b[0m\n",
      "[\u001b[0;36mDEBUG\u001b[0m | src.tasks.clm_training.fabric.base]: \u001b[0;36mAbout to check: (iter_num + 1) % gradient_accumulation_steps != 0 -> (86 + 1) % 2 != 0\u001b[0m\n",
      "2025-06-16 13:55:03 - src.tasks.clm_training.fabric.base - \u001b[0;36mDEBUG\u001b[0m - \u001b[0;36miter 87 step 43: loss 5.3438, iter time: 122.69ms remaining time: \u001b[0m\n",
      "[\u001b[0;36mDEBUG\u001b[0m | src.tasks.clm_training.fabric.base]: \u001b[0;36miter 87 step 43: loss 5.3438, iter time: 122.69ms remaining time: \u001b[0m\n",
      "  2%|\u001b[34m▊                                        \u001b[0m| 87/4682 [00:37<32:18,  2.37it/s]\u001b[0m2025-06-16 13:55:03 - src.tasks.clm_training.fabric.base - \u001b[0;36mDEBUG\u001b[0m - \u001b[0;36mgradient_accumulation_steps raw value: 2, type: <class 'int'>\u001b[0m\n",
      "[\u001b[0;36mDEBUG\u001b[0m | src.tasks.clm_training.fabric.base]: \u001b[0;36mgradient_accumulation_steps raw value: 2, type: <class 'int'>\u001b[0m\n",
      "2025-06-16 13:55:03 - src.tasks.clm_training.fabric.base - \u001b[0;36mDEBUG\u001b[0m - \u001b[0;36mgradient_accumulation_steps converted: 2, type: <class 'int'>\u001b[0m\n",
      "[\u001b[0;36mDEBUG\u001b[0m | src.tasks.clm_training.fabric.base]: \u001b[0;36mgradient_accumulation_steps converted: 2, type: <class 'int'>\u001b[0m\n",
      "2025-06-16 13:55:03 - src.tasks.clm_training.fabric.base - \u001b[0;36mDEBUG\u001b[0m - \u001b[0;36miter_num value: 87, type: <class 'int'>\u001b[0m\n",
      "[\u001b[0;36mDEBUG\u001b[0m | src.tasks.clm_training.fabric.base]: \u001b[0;36miter_num value: 87, type: <class 'int'>\u001b[0m\n",
      "2025-06-16 13:55:03 - src.tasks.clm_training.fabric.base - \u001b[0;36mDEBUG\u001b[0m - \u001b[0;36mAbout to check: (iter_num + 1) % gradient_accumulation_steps != 0 -> (87 + 1) % 2 != 0\u001b[0m\n",
      "[\u001b[0;36mDEBUG\u001b[0m | src.tasks.clm_training.fabric.base]: \u001b[0;36mAbout to check: (iter_num + 1) % gradient_accumulation_steps != 0 -> (87 + 1) % 2 != 0\u001b[0m\n",
      "2025-06-16 13:55:03 - src.tasks.clm_training.fabric.base - \u001b[0;36mDEBUG\u001b[0m - \u001b[0;36mGradient norm before clipping: 25.1250\u001b[0m\n",
      "[\u001b[0;36mDEBUG\u001b[0m | src.tasks.clm_training.fabric.base]: \u001b[0;36mGradient norm before clipping: 25.1250\u001b[0m\n",
      "2025-06-16 13:55:03 - src.tasks.clm_training.fabric.base - \u001b[0;36mDEBUG\u001b[0m - \u001b[0;36mvalidate_after_k_steps value: 1000, type: <class 'int'>\u001b[0m\n",
      "[\u001b[0;36mDEBUG\u001b[0m | src.tasks.clm_training.fabric.base]: \u001b[0;36mvalidate_after_k_steps value: 1000, type: <class 'int'>\u001b[0m\n",
      "2025-06-16 13:55:03 - src.tasks.clm_training.fabric.base - \u001b[0;36mDEBUG\u001b[0m - \u001b[0;36mstep_count value: 44, type: <class 'int'>\u001b[0m\n",
      "[\u001b[0;36mDEBUG\u001b[0m | src.tasks.clm_training.fabric.base]: \u001b[0;36mstep_count value: 44, type: <class 'int'>\u001b[0m\n",
      "2025-06-16 13:55:03 - src.tasks.clm_training.fabric.base - \u001b[0;36mDEBUG\u001b[0m - \u001b[0;36mValidation condition check - epochFinished: False, trainingFinished: False\u001b[0m\n",
      "[\u001b[0;36mDEBUG\u001b[0m | src.tasks.clm_training.fabric.base]: \u001b[0;36mValidation condition check - epochFinished: False, trainingFinished: False\u001b[0m\n",
      "2025-06-16 13:55:03 - src.tasks.clm_training.fabric.base - \u001b[0;36mDEBUG\u001b[0m - \u001b[0;36mAbout to check: step_count % validate_after_k_steps == 0 -> 44 % 1000 == 0\u001b[0m\n",
      "[\u001b[0;36mDEBUG\u001b[0m | src.tasks.clm_training.fabric.base]: \u001b[0;36mAbout to check: step_count % validate_after_k_steps == 0 -> 44 % 1000 == 0\u001b[0m\n",
      "2025-06-16 13:55:03 - src.tasks.clm_training.fabric.base - \u001b[0;36mDEBUG\u001b[0m - \u001b[0;36miter 88 step 44: loss 6.3750, iter time: 419.68ms remaining time: \u001b[0m\n",
      "[\u001b[0;36mDEBUG\u001b[0m | src.tasks.clm_training.fabric.base]: \u001b[0;36miter 88 step 44: loss 6.3750, iter time: 419.68ms remaining time: \u001b[0m\n",
      "  2%|\u001b[34m▊                                        \u001b[0m| 88/4682 [00:37<32:28,  2.36it/s]\u001b[0m2025-06-16 13:55:03 - src.tasks.clm_training.fabric.base - \u001b[0;36mDEBUG\u001b[0m - \u001b[0;36mgradient_accumulation_steps raw value: 2, type: <class 'int'>\u001b[0m\n",
      "[\u001b[0;36mDEBUG\u001b[0m | src.tasks.clm_training.fabric.base]: \u001b[0;36mgradient_accumulation_steps raw value: 2, type: <class 'int'>\u001b[0m\n",
      "2025-06-16 13:55:03 - src.tasks.clm_training.fabric.base - \u001b[0;36mDEBUG\u001b[0m - \u001b[0;36mgradient_accumulation_steps converted: 2, type: <class 'int'>\u001b[0m\n",
      "[\u001b[0;36mDEBUG\u001b[0m | src.tasks.clm_training.fabric.base]: \u001b[0;36mgradient_accumulation_steps converted: 2, type: <class 'int'>\u001b[0m\n",
      "2025-06-16 13:55:03 - src.tasks.clm_training.fabric.base - \u001b[0;36mDEBUG\u001b[0m - \u001b[0;36miter_num value: 88, type: <class 'int'>\u001b[0m\n",
      "[\u001b[0;36mDEBUG\u001b[0m | src.tasks.clm_training.fabric.base]: \u001b[0;36miter_num value: 88, type: <class 'int'>\u001b[0m\n",
      "2025-06-16 13:55:03 - src.tasks.clm_training.fabric.base - \u001b[0;36mDEBUG\u001b[0m - \u001b[0;36mAbout to check: (iter_num + 1) % gradient_accumulation_steps != 0 -> (88 + 1) % 2 != 0\u001b[0m\n",
      "[\u001b[0;36mDEBUG\u001b[0m | src.tasks.clm_training.fabric.base]: \u001b[0;36mAbout to check: (iter_num + 1) % gradient_accumulation_steps != 0 -> (88 + 1) % 2 != 0\u001b[0m\n",
      "2025-06-16 13:55:03 - src.tasks.clm_training.fabric.base - \u001b[0;36mDEBUG\u001b[0m - \u001b[0;36miter 89 step 44: loss 5.3750, iter time: 122.04ms remaining time: \u001b[0m\n",
      "[\u001b[0;36mDEBUG\u001b[0m | src.tasks.clm_training.fabric.base]: \u001b[0;36miter 89 step 44: loss 5.3750, iter time: 122.04ms remaining time: \u001b[0m\n",
      "  2%|\u001b[34m▊                                        \u001b[0m| 89/4682 [00:37<32:16,  2.37it/s]\u001b[0m2025-06-16 13:55:03 - src.tasks.clm_training.fabric.base - \u001b[0;36mDEBUG\u001b[0m - \u001b[0;36mgradient_accumulation_steps raw value: 2, type: <class 'int'>\u001b[0m\n",
      "[\u001b[0;36mDEBUG\u001b[0m | src.tasks.clm_training.fabric.base]: \u001b[0;36mgradient_accumulation_steps raw value: 2, type: <class 'int'>\u001b[0m\n",
      "2025-06-16 13:55:03 - src.tasks.clm_training.fabric.base - \u001b[0;36mDEBUG\u001b[0m - \u001b[0;36mgradient_accumulation_steps converted: 2, type: <class 'int'>\u001b[0m\n",
      "[\u001b[0;36mDEBUG\u001b[0m | src.tasks.clm_training.fabric.base]: \u001b[0;36mgradient_accumulation_steps converted: 2, type: <class 'int'>\u001b[0m\n",
      "2025-06-16 13:55:03 - src.tasks.clm_training.fabric.base - \u001b[0;36mDEBUG\u001b[0m - \u001b[0;36miter_num value: 89, type: <class 'int'>\u001b[0m\n",
      "[\u001b[0;36mDEBUG\u001b[0m | src.tasks.clm_training.fabric.base]: \u001b[0;36miter_num value: 89, type: <class 'int'>\u001b[0m\n",
      "2025-06-16 13:55:03 - src.tasks.clm_training.fabric.base - \u001b[0;36mDEBUG\u001b[0m - \u001b[0;36mAbout to check: (iter_num + 1) % gradient_accumulation_steps != 0 -> (89 + 1) % 2 != 0\u001b[0m\n",
      "[\u001b[0;36mDEBUG\u001b[0m | src.tasks.clm_training.fabric.base]: \u001b[0;36mAbout to check: (iter_num + 1) % gradient_accumulation_steps != 0 -> (89 + 1) % 2 != 0\u001b[0m\n",
      "2025-06-16 13:55:04 - src.tasks.clm_training.fabric.base - \u001b[0;36mDEBUG\u001b[0m - \u001b[0;36mGradient norm before clipping: 22.7500\u001b[0m\n",
      "[\u001b[0;36mDEBUG\u001b[0m | src.tasks.clm_training.fabric.base]: \u001b[0;36mGradient norm before clipping: 22.7500\u001b[0m\n",
      "2025-06-16 13:55:04 - src.tasks.clm_training.fabric.base - \u001b[0;36mDEBUG\u001b[0m - \u001b[0;36mvalidate_after_k_steps value: 1000, type: <class 'int'>\u001b[0m\n",
      "[\u001b[0;36mDEBUG\u001b[0m | src.tasks.clm_training.fabric.base]: \u001b[0;36mvalidate_after_k_steps value: 1000, type: <class 'int'>\u001b[0m\n",
      "2025-06-16 13:55:04 - src.tasks.clm_training.fabric.base - \u001b[0;36mDEBUG\u001b[0m - \u001b[0;36mstep_count value: 45, type: <class 'int'>\u001b[0m\n",
      "[\u001b[0;36mDEBUG\u001b[0m | src.tasks.clm_training.fabric.base]: \u001b[0;36mstep_count value: 45, type: <class 'int'>\u001b[0m\n",
      "2025-06-16 13:55:04 - src.tasks.clm_training.fabric.base - \u001b[0;36mDEBUG\u001b[0m - \u001b[0;36mValidation condition check - epochFinished: False, trainingFinished: False\u001b[0m\n",
      "[\u001b[0;36mDEBUG\u001b[0m | src.tasks.clm_training.fabric.base]: \u001b[0;36mValidation condition check - epochFinished: False, trainingFinished: False\u001b[0m\n",
      "2025-06-16 13:55:04 - src.tasks.clm_training.fabric.base - \u001b[0;36mDEBUG\u001b[0m - \u001b[0;36mAbout to check: step_count % validate_after_k_steps == 0 -> 45 % 1000 == 0\u001b[0m\n",
      "[\u001b[0;36mDEBUG\u001b[0m | src.tasks.clm_training.fabric.base]: \u001b[0;36mAbout to check: step_count % validate_after_k_steps == 0 -> 45 % 1000 == 0\u001b[0m\n",
      "2025-06-16 13:55:04 - src.tasks.clm_training.fabric.base - \u001b[0;36mDEBUG\u001b[0m - \u001b[0;36miter 90 step 45: loss 5.5312, iter time: 420.37ms remaining time: \u001b[0m\n",
      "[\u001b[0;36mDEBUG\u001b[0m | src.tasks.clm_training.fabric.base]: \u001b[0;36miter 90 step 45: loss 5.5312, iter time: 420.37ms remaining time: \u001b[0m\n",
      "  2%|\u001b[34m▊                                        \u001b[0m| 90/4682 [00:38<32:28,  2.36it/s]\u001b[0m2025-06-16 13:55:04 - src.tasks.clm_training.fabric.base - \u001b[0;36mDEBUG\u001b[0m - \u001b[0;36mgradient_accumulation_steps raw value: 2, type: <class 'int'>\u001b[0m\n",
      "[\u001b[0;36mDEBUG\u001b[0m | src.tasks.clm_training.fabric.base]: \u001b[0;36mgradient_accumulation_steps raw value: 2, type: <class 'int'>\u001b[0m\n",
      "2025-06-16 13:55:04 - src.tasks.clm_training.fabric.base - \u001b[0;36mDEBUG\u001b[0m - \u001b[0;36mgradient_accumulation_steps converted: 2, type: <class 'int'>\u001b[0m\n",
      "[\u001b[0;36mDEBUG\u001b[0m | src.tasks.clm_training.fabric.base]: \u001b[0;36mgradient_accumulation_steps converted: 2, type: <class 'int'>\u001b[0m\n",
      "2025-06-16 13:55:04 - src.tasks.clm_training.fabric.base - \u001b[0;36mDEBUG\u001b[0m - \u001b[0;36miter_num value: 90, type: <class 'int'>\u001b[0m\n",
      "[\u001b[0;36mDEBUG\u001b[0m | src.tasks.clm_training.fabric.base]: \u001b[0;36miter_num value: 90, type: <class 'int'>\u001b[0m\n",
      "2025-06-16 13:55:04 - src.tasks.clm_training.fabric.base - \u001b[0;36mDEBUG\u001b[0m - \u001b[0;36mAbout to check: (iter_num + 1) % gradient_accumulation_steps != 0 -> (90 + 1) % 2 != 0\u001b[0m\n",
      "[\u001b[0;36mDEBUG\u001b[0m | src.tasks.clm_training.fabric.base]: \u001b[0;36mAbout to check: (iter_num + 1) % gradient_accumulation_steps != 0 -> (90 + 1) % 2 != 0\u001b[0m\n",
      "2025-06-16 13:55:04 - src.tasks.clm_training.fabric.base - \u001b[0;36mDEBUG\u001b[0m - \u001b[0;36miter 91 step 45: loss 5.8438, iter time: 122.28ms remaining time: \u001b[0m\n",
      "[\u001b[0;36mDEBUG\u001b[0m | src.tasks.clm_training.fabric.base]: \u001b[0;36miter 91 step 45: loss 5.8438, iter time: 122.28ms remaining time: \u001b[0m\n",
      "  2%|\u001b[34m▊                                        \u001b[0m| 91/4682 [00:38<32:16,  2.37it/s]\u001b[0m2025-06-16 13:55:04 - src.tasks.clm_training.fabric.base - \u001b[0;36mDEBUG\u001b[0m - \u001b[0;36mgradient_accumulation_steps raw value: 2, type: <class 'int'>\u001b[0m\n",
      "[\u001b[0;36mDEBUG\u001b[0m | src.tasks.clm_training.fabric.base]: \u001b[0;36mgradient_accumulation_steps raw value: 2, type: <class 'int'>\u001b[0m\n",
      "2025-06-16 13:55:04 - src.tasks.clm_training.fabric.base - \u001b[0;36mDEBUG\u001b[0m - \u001b[0;36mgradient_accumulation_steps converted: 2, type: <class 'int'>\u001b[0m\n",
      "[\u001b[0;36mDEBUG\u001b[0m | src.tasks.clm_training.fabric.base]: \u001b[0;36mgradient_accumulation_steps converted: 2, type: <class 'int'>\u001b[0m\n",
      "2025-06-16 13:55:04 - src.tasks.clm_training.fabric.base - \u001b[0;36mDEBUG\u001b[0m - \u001b[0;36miter_num value: 91, type: <class 'int'>\u001b[0m\n",
      "[\u001b[0;36mDEBUG\u001b[0m | src.tasks.clm_training.fabric.base]: \u001b[0;36miter_num value: 91, type: <class 'int'>\u001b[0m\n",
      "2025-06-16 13:55:04 - src.tasks.clm_training.fabric.base - \u001b[0;36mDEBUG\u001b[0m - \u001b[0;36mAbout to check: (iter_num + 1) % gradient_accumulation_steps != 0 -> (91 + 1) % 2 != 0\u001b[0m\n",
      "[\u001b[0;36mDEBUG\u001b[0m | src.tasks.clm_training.fabric.base]: \u001b[0;36mAbout to check: (iter_num + 1) % gradient_accumulation_steps != 0 -> (91 + 1) % 2 != 0\u001b[0m\n",
      "2025-06-16 13:55:05 - src.tasks.clm_training.fabric.base - \u001b[0;36mDEBUG\u001b[0m - \u001b[0;36mGradient norm before clipping: 23.2500\u001b[0m\n",
      "[\u001b[0;36mDEBUG\u001b[0m | src.tasks.clm_training.fabric.base]: \u001b[0;36mGradient norm before clipping: 23.2500\u001b[0m\n",
      "2025-06-16 13:55:05 - src.tasks.clm_training.fabric.base - \u001b[0;36mDEBUG\u001b[0m - \u001b[0;36mvalidate_after_k_steps value: 1000, type: <class 'int'>\u001b[0m\n",
      "[\u001b[0;36mDEBUG\u001b[0m | src.tasks.clm_training.fabric.base]: \u001b[0;36mvalidate_after_k_steps value: 1000, type: <class 'int'>\u001b[0m\n",
      "2025-06-16 13:55:05 - src.tasks.clm_training.fabric.base - \u001b[0;36mDEBUG\u001b[0m - \u001b[0;36mstep_count value: 46, type: <class 'int'>\u001b[0m\n",
      "[\u001b[0;36mDEBUG\u001b[0m | src.tasks.clm_training.fabric.base]: \u001b[0;36mstep_count value: 46, type: <class 'int'>\u001b[0m\n",
      "2025-06-16 13:55:05 - src.tasks.clm_training.fabric.base - \u001b[0;36mDEBUG\u001b[0m - \u001b[0;36mValidation condition check - epochFinished: False, trainingFinished: False\u001b[0m\n",
      "[\u001b[0;36mDEBUG\u001b[0m | src.tasks.clm_training.fabric.base]: \u001b[0;36mValidation condition check - epochFinished: False, trainingFinished: False\u001b[0m\n",
      "2025-06-16 13:55:05 - src.tasks.clm_training.fabric.base - \u001b[0;36mDEBUG\u001b[0m - \u001b[0;36mAbout to check: step_count % validate_after_k_steps == 0 -> 46 % 1000 == 0\u001b[0m\n",
      "[\u001b[0;36mDEBUG\u001b[0m | src.tasks.clm_training.fabric.base]: \u001b[0;36mAbout to check: step_count % validate_after_k_steps == 0 -> 46 % 1000 == 0\u001b[0m\n",
      "2025-06-16 13:55:05 - src.tasks.clm_training.fabric.base - \u001b[0;36mDEBUG\u001b[0m - \u001b[0;36miter 92 step 46: loss 5.9375, iter time: 420.27ms remaining time: \u001b[0m\n",
      "[\u001b[0;36mDEBUG\u001b[0m | src.tasks.clm_training.fabric.base]: \u001b[0;36miter 92 step 46: loss 5.9375, iter time: 420.27ms remaining time: \u001b[0m\n",
      "  2%|\u001b[34m▊                                        \u001b[0m| 92/4682 [00:39<32:27,  2.36it/s]\u001b[0m2025-06-16 13:55:05 - src.tasks.clm_training.fabric.base - \u001b[0;36mDEBUG\u001b[0m - \u001b[0;36mgradient_accumulation_steps raw value: 2, type: <class 'int'>\u001b[0m\n",
      "[\u001b[0;36mDEBUG\u001b[0m | src.tasks.clm_training.fabric.base]: \u001b[0;36mgradient_accumulation_steps raw value: 2, type: <class 'int'>\u001b[0m\n",
      "2025-06-16 13:55:05 - src.tasks.clm_training.fabric.base - \u001b[0;36mDEBUG\u001b[0m - \u001b[0;36mgradient_accumulation_steps converted: 2, type: <class 'int'>\u001b[0m\n",
      "[\u001b[0;36mDEBUG\u001b[0m | src.tasks.clm_training.fabric.base]: \u001b[0;36mgradient_accumulation_steps converted: 2, type: <class 'int'>\u001b[0m\n",
      "2025-06-16 13:55:05 - src.tasks.clm_training.fabric.base - \u001b[0;36mDEBUG\u001b[0m - \u001b[0;36miter_num value: 92, type: <class 'int'>\u001b[0m\n",
      "[\u001b[0;36mDEBUG\u001b[0m | src.tasks.clm_training.fabric.base]: \u001b[0;36miter_num value: 92, type: <class 'int'>\u001b[0m\n",
      "2025-06-16 13:55:05 - src.tasks.clm_training.fabric.base - \u001b[0;36mDEBUG\u001b[0m - \u001b[0;36mAbout to check: (iter_num + 1) % gradient_accumulation_steps != 0 -> (92 + 1) % 2 != 0\u001b[0m\n",
      "[\u001b[0;36mDEBUG\u001b[0m | src.tasks.clm_training.fabric.base]: \u001b[0;36mAbout to check: (iter_num + 1) % gradient_accumulation_steps != 0 -> (92 + 1) % 2 != 0\u001b[0m\n",
      "2025-06-16 13:55:05 - src.tasks.clm_training.fabric.base - \u001b[0;36mDEBUG\u001b[0m - \u001b[0;36miter 93 step 46: loss 5.8125, iter time: 122.29ms remaining time: \u001b[0m\n",
      "[\u001b[0;36mDEBUG\u001b[0m | src.tasks.clm_training.fabric.base]: \u001b[0;36miter 93 step 46: loss 5.8125, iter time: 122.29ms remaining time: \u001b[0m\n",
      "  2%|\u001b[34m▊                                        \u001b[0m| 93/4682 [00:39<32:16,  2.37it/s]\u001b[0m2025-06-16 13:55:05 - src.tasks.clm_training.fabric.base - \u001b[0;36mDEBUG\u001b[0m - \u001b[0;36mgradient_accumulation_steps raw value: 2, type: <class 'int'>\u001b[0m\n",
      "[\u001b[0;36mDEBUG\u001b[0m | src.tasks.clm_training.fabric.base]: \u001b[0;36mgradient_accumulation_steps raw value: 2, type: <class 'int'>\u001b[0m\n",
      "2025-06-16 13:55:05 - src.tasks.clm_training.fabric.base - \u001b[0;36mDEBUG\u001b[0m - \u001b[0;36mgradient_accumulation_steps converted: 2, type: <class 'int'>\u001b[0m\n",
      "[\u001b[0;36mDEBUG\u001b[0m | src.tasks.clm_training.fabric.base]: \u001b[0;36mgradient_accumulation_steps converted: 2, type: <class 'int'>\u001b[0m\n",
      "2025-06-16 13:55:05 - src.tasks.clm_training.fabric.base - \u001b[0;36mDEBUG\u001b[0m - \u001b[0;36miter_num value: 93, type: <class 'int'>\u001b[0m\n",
      "[\u001b[0;36mDEBUG\u001b[0m | src.tasks.clm_training.fabric.base]: \u001b[0;36miter_num value: 93, type: <class 'int'>\u001b[0m\n",
      "2025-06-16 13:55:05 - src.tasks.clm_training.fabric.base - \u001b[0;36mDEBUG\u001b[0m - \u001b[0;36mAbout to check: (iter_num + 1) % gradient_accumulation_steps != 0 -> (93 + 1) % 2 != 0\u001b[0m\n",
      "[\u001b[0;36mDEBUG\u001b[0m | src.tasks.clm_training.fabric.base]: \u001b[0;36mAbout to check: (iter_num + 1) % gradient_accumulation_steps != 0 -> (93 + 1) % 2 != 0\u001b[0m\n",
      "2025-06-16 13:55:06 - src.tasks.clm_training.fabric.base - \u001b[0;36mDEBUG\u001b[0m - \u001b[0;36mGradient norm before clipping: 25.0000\u001b[0m\n",
      "[\u001b[0;36mDEBUG\u001b[0m | src.tasks.clm_training.fabric.base]: \u001b[0;36mGradient norm before clipping: 25.0000\u001b[0m\n",
      "2025-06-16 13:55:06 - src.tasks.clm_training.fabric.base - \u001b[0;36mDEBUG\u001b[0m - \u001b[0;36mvalidate_after_k_steps value: 1000, type: <class 'int'>\u001b[0m\n",
      "[\u001b[0;36mDEBUG\u001b[0m | src.tasks.clm_training.fabric.base]: \u001b[0;36mvalidate_after_k_steps value: 1000, type: <class 'int'>\u001b[0m\n",
      "2025-06-16 13:55:06 - src.tasks.clm_training.fabric.base - \u001b[0;36mDEBUG\u001b[0m - \u001b[0;36mstep_count value: 47, type: <class 'int'>\u001b[0m\n",
      "[\u001b[0;36mDEBUG\u001b[0m | src.tasks.clm_training.fabric.base]: \u001b[0;36mstep_count value: 47, type: <class 'int'>\u001b[0m\n",
      "2025-06-16 13:55:06 - src.tasks.clm_training.fabric.base - \u001b[0;36mDEBUG\u001b[0m - \u001b[0;36mValidation condition check - epochFinished: False, trainingFinished: False\u001b[0m\n",
      "[\u001b[0;36mDEBUG\u001b[0m | src.tasks.clm_training.fabric.base]: \u001b[0;36mValidation condition check - epochFinished: False, trainingFinished: False\u001b[0m\n",
      "2025-06-16 13:55:06 - src.tasks.clm_training.fabric.base - \u001b[0;36mDEBUG\u001b[0m - \u001b[0;36mAbout to check: step_count % validate_after_k_steps == 0 -> 47 % 1000 == 0\u001b[0m\n",
      "[\u001b[0;36mDEBUG\u001b[0m | src.tasks.clm_training.fabric.base]: \u001b[0;36mAbout to check: step_count % validate_after_k_steps == 0 -> 47 % 1000 == 0\u001b[0m\n",
      "2025-06-16 13:55:06 - src.tasks.clm_training.fabric.base - \u001b[0;36mDEBUG\u001b[0m - \u001b[0;36miter 94 step 47: loss 5.5312, iter time: 419.59ms remaining time: \u001b[0m\n",
      "[\u001b[0;36mDEBUG\u001b[0m | src.tasks.clm_training.fabric.base]: \u001b[0;36miter 94 step 47: loss 5.5312, iter time: 419.59ms remaining time: \u001b[0m\n",
      "  2%|\u001b[34m▊                                        \u001b[0m| 94/4682 [00:40<32:25,  2.36it/s]\u001b[0m2025-06-16 13:55:06 - src.tasks.clm_training.fabric.base - \u001b[0;36mDEBUG\u001b[0m - \u001b[0;36mgradient_accumulation_steps raw value: 2, type: <class 'int'>\u001b[0m\n",
      "[\u001b[0;36mDEBUG\u001b[0m | src.tasks.clm_training.fabric.base]: \u001b[0;36mgradient_accumulation_steps raw value: 2, type: <class 'int'>\u001b[0m\n",
      "2025-06-16 13:55:06 - src.tasks.clm_training.fabric.base - \u001b[0;36mDEBUG\u001b[0m - \u001b[0;36mgradient_accumulation_steps converted: 2, type: <class 'int'>\u001b[0m\n",
      "[\u001b[0;36mDEBUG\u001b[0m | src.tasks.clm_training.fabric.base]: \u001b[0;36mgradient_accumulation_steps converted: 2, type: <class 'int'>\u001b[0m\n",
      "2025-06-16 13:55:06 - src.tasks.clm_training.fabric.base - \u001b[0;36mDEBUG\u001b[0m - \u001b[0;36miter_num value: 94, type: <class 'int'>\u001b[0m\n",
      "[\u001b[0;36mDEBUG\u001b[0m | src.tasks.clm_training.fabric.base]: \u001b[0;36miter_num value: 94, type: <class 'int'>\u001b[0m\n",
      "2025-06-16 13:55:06 - src.tasks.clm_training.fabric.base - \u001b[0;36mDEBUG\u001b[0m - \u001b[0;36mAbout to check: (iter_num + 1) % gradient_accumulation_steps != 0 -> (94 + 1) % 2 != 0\u001b[0m\n",
      "[\u001b[0;36mDEBUG\u001b[0m | src.tasks.clm_training.fabric.base]: \u001b[0;36mAbout to check: (iter_num + 1) % gradient_accumulation_steps != 0 -> (94 + 1) % 2 != 0\u001b[0m\n",
      "2025-06-16 13:55:06 - src.tasks.clm_training.fabric.base - \u001b[0;36mDEBUG\u001b[0m - \u001b[0;36miter 95 step 47: loss 6.0312, iter time: 122.57ms remaining time: \u001b[0m\n",
      "[\u001b[0;36mDEBUG\u001b[0m | src.tasks.clm_training.fabric.base]: \u001b[0;36miter 95 step 47: loss 6.0312, iter time: 122.57ms remaining time: \u001b[0m\n",
      "  2%|\u001b[34m▊                                        \u001b[0m| 95/4682 [00:40<32:13,  2.37it/s]\u001b[0m2025-06-16 13:55:06 - src.tasks.clm_training.fabric.base - \u001b[0;36mDEBUG\u001b[0m - \u001b[0;36mgradient_accumulation_steps raw value: 2, type: <class 'int'>\u001b[0m\n",
      "[\u001b[0;36mDEBUG\u001b[0m | src.tasks.clm_training.fabric.base]: \u001b[0;36mgradient_accumulation_steps raw value: 2, type: <class 'int'>\u001b[0m\n",
      "2025-06-16 13:55:06 - src.tasks.clm_training.fabric.base - \u001b[0;36mDEBUG\u001b[0m - \u001b[0;36mgradient_accumulation_steps converted: 2, type: <class 'int'>\u001b[0m\n",
      "[\u001b[0;36mDEBUG\u001b[0m | src.tasks.clm_training.fabric.base]: \u001b[0;36mgradient_accumulation_steps converted: 2, type: <class 'int'>\u001b[0m\n",
      "2025-06-16 13:55:06 - src.tasks.clm_training.fabric.base - \u001b[0;36mDEBUG\u001b[0m - \u001b[0;36miter_num value: 95, type: <class 'int'>\u001b[0m\n",
      "[\u001b[0;36mDEBUG\u001b[0m | src.tasks.clm_training.fabric.base]: \u001b[0;36miter_num value: 95, type: <class 'int'>\u001b[0m\n",
      "2025-06-16 13:55:06 - src.tasks.clm_training.fabric.base - \u001b[0;36mDEBUG\u001b[0m - \u001b[0;36mAbout to check: (iter_num + 1) % gradient_accumulation_steps != 0 -> (95 + 1) % 2 != 0\u001b[0m\n",
      "[\u001b[0;36mDEBUG\u001b[0m | src.tasks.clm_training.fabric.base]: \u001b[0;36mAbout to check: (iter_num + 1) % gradient_accumulation_steps != 0 -> (95 + 1) % 2 != 0\u001b[0m\n",
      "2025-06-16 13:55:06 - src.tasks.clm_training.fabric.base - \u001b[0;36mDEBUG\u001b[0m - \u001b[0;36mGradient norm before clipping: 22.7500\u001b[0m\n",
      "[\u001b[0;36mDEBUG\u001b[0m | src.tasks.clm_training.fabric.base]: \u001b[0;36mGradient norm before clipping: 22.7500\u001b[0m\n",
      "2025-06-16 13:55:06 - src.tasks.clm_training.fabric.base - \u001b[0;36mDEBUG\u001b[0m - \u001b[0;36mvalidate_after_k_steps value: 1000, type: <class 'int'>\u001b[0m\n",
      "[\u001b[0;36mDEBUG\u001b[0m | src.tasks.clm_training.fabric.base]: \u001b[0;36mvalidate_after_k_steps value: 1000, type: <class 'int'>\u001b[0m\n",
      "2025-06-16 13:55:06 - src.tasks.clm_training.fabric.base - \u001b[0;36mDEBUG\u001b[0m - \u001b[0;36mstep_count value: 48, type: <class 'int'>\u001b[0m\n",
      "[\u001b[0;36mDEBUG\u001b[0m | src.tasks.clm_training.fabric.base]: \u001b[0;36mstep_count value: 48, type: <class 'int'>\u001b[0m\n",
      "2025-06-16 13:55:06 - src.tasks.clm_training.fabric.base - \u001b[0;36mDEBUG\u001b[0m - \u001b[0;36mValidation condition check - epochFinished: False, trainingFinished: False\u001b[0m\n",
      "[\u001b[0;36mDEBUG\u001b[0m | src.tasks.clm_training.fabric.base]: \u001b[0;36mValidation condition check - epochFinished: False, trainingFinished: False\u001b[0m\n",
      "2025-06-16 13:55:06 - src.tasks.clm_training.fabric.base - \u001b[0;36mDEBUG\u001b[0m - \u001b[0;36mAbout to check: step_count % validate_after_k_steps == 0 -> 48 % 1000 == 0\u001b[0m\n",
      "[\u001b[0;36mDEBUG\u001b[0m | src.tasks.clm_training.fabric.base]: \u001b[0;36mAbout to check: step_count % validate_after_k_steps == 0 -> 48 % 1000 == 0\u001b[0m\n",
      "2025-06-16 13:55:06 - src.tasks.clm_training.fabric.base - \u001b[0;36mDEBUG\u001b[0m - \u001b[0;36miter 96 step 48: loss 6.0312, iter time: 420.72ms remaining time: \u001b[0m\n",
      "[\u001b[0;36mDEBUG\u001b[0m | src.tasks.clm_training.fabric.base]: \u001b[0;36miter 96 step 48: loss 6.0312, iter time: 420.72ms remaining time: \u001b[0m\n",
      "  2%|\u001b[34m▊                                        \u001b[0m| 96/4682 [00:40<32:25,  2.36it/s]\u001b[0m2025-06-16 13:55:06 - src.tasks.clm_training.fabric.base - \u001b[0;36mDEBUG\u001b[0m - \u001b[0;36mgradient_accumulation_steps raw value: 2, type: <class 'int'>\u001b[0m\n",
      "[\u001b[0;36mDEBUG\u001b[0m | src.tasks.clm_training.fabric.base]: \u001b[0;36mgradient_accumulation_steps raw value: 2, type: <class 'int'>\u001b[0m\n",
      "2025-06-16 13:55:06 - src.tasks.clm_training.fabric.base - \u001b[0;36mDEBUG\u001b[0m - \u001b[0;36mgradient_accumulation_steps converted: 2, type: <class 'int'>\u001b[0m\n",
      "[\u001b[0;36mDEBUG\u001b[0m | src.tasks.clm_training.fabric.base]: \u001b[0;36mgradient_accumulation_steps converted: 2, type: <class 'int'>\u001b[0m\n",
      "2025-06-16 13:55:06 - src.tasks.clm_training.fabric.base - \u001b[0;36mDEBUG\u001b[0m - \u001b[0;36miter_num value: 96, type: <class 'int'>\u001b[0m\n",
      "[\u001b[0;36mDEBUG\u001b[0m | src.tasks.clm_training.fabric.base]: \u001b[0;36miter_num value: 96, type: <class 'int'>\u001b[0m\n",
      "2025-06-16 13:55:06 - src.tasks.clm_training.fabric.base - \u001b[0;36mDEBUG\u001b[0m - \u001b[0;36mAbout to check: (iter_num + 1) % gradient_accumulation_steps != 0 -> (96 + 1) % 2 != 0\u001b[0m\n",
      "[\u001b[0;36mDEBUG\u001b[0m | src.tasks.clm_training.fabric.base]: \u001b[0;36mAbout to check: (iter_num + 1) % gradient_accumulation_steps != 0 -> (96 + 1) % 2 != 0\u001b[0m\n",
      "2025-06-16 13:55:07 - src.tasks.clm_training.fabric.base - \u001b[0;36mDEBUG\u001b[0m - \u001b[0;36miter 97 step 48: loss 5.7188, iter time: 122.28ms remaining time: \u001b[0m\n",
      "[\u001b[0;36mDEBUG\u001b[0m | src.tasks.clm_training.fabric.base]: \u001b[0;36miter 97 step 48: loss 5.7188, iter time: 122.28ms remaining time: \u001b[0m\n",
      "  2%|\u001b[34m▊                                        \u001b[0m| 97/4682 [00:41<32:13,  2.37it/s]\u001b[0m2025-06-16 13:55:07 - src.tasks.clm_training.fabric.base - \u001b[0;36mDEBUG\u001b[0m - \u001b[0;36mgradient_accumulation_steps raw value: 2, type: <class 'int'>\u001b[0m\n",
      "[\u001b[0;36mDEBUG\u001b[0m | src.tasks.clm_training.fabric.base]: \u001b[0;36mgradient_accumulation_steps raw value: 2, type: <class 'int'>\u001b[0m\n",
      "2025-06-16 13:55:07 - src.tasks.clm_training.fabric.base - \u001b[0;36mDEBUG\u001b[0m - \u001b[0;36mgradient_accumulation_steps converted: 2, type: <class 'int'>\u001b[0m\n",
      "[\u001b[0;36mDEBUG\u001b[0m | src.tasks.clm_training.fabric.base]: \u001b[0;36mgradient_accumulation_steps converted: 2, type: <class 'int'>\u001b[0m\n",
      "2025-06-16 13:55:07 - src.tasks.clm_training.fabric.base - \u001b[0;36mDEBUG\u001b[0m - \u001b[0;36miter_num value: 97, type: <class 'int'>\u001b[0m\n",
      "[\u001b[0;36mDEBUG\u001b[0m | src.tasks.clm_training.fabric.base]: \u001b[0;36miter_num value: 97, type: <class 'int'>\u001b[0m\n",
      "2025-06-16 13:55:07 - src.tasks.clm_training.fabric.base - \u001b[0;36mDEBUG\u001b[0m - \u001b[0;36mAbout to check: (iter_num + 1) % gradient_accumulation_steps != 0 -> (97 + 1) % 2 != 0\u001b[0m\n",
      "[\u001b[0;36mDEBUG\u001b[0m | src.tasks.clm_training.fabric.base]: \u001b[0;36mAbout to check: (iter_num + 1) % gradient_accumulation_steps != 0 -> (97 + 1) % 2 != 0\u001b[0m\n",
      "2025-06-16 13:55:07 - src.tasks.clm_training.fabric.base - \u001b[0;36mDEBUG\u001b[0m - \u001b[0;36mGradient norm before clipping: 22.8750\u001b[0m\n",
      "[\u001b[0;36mDEBUG\u001b[0m | src.tasks.clm_training.fabric.base]: \u001b[0;36mGradient norm before clipping: 22.8750\u001b[0m\n",
      "2025-06-16 13:55:07 - src.tasks.clm_training.fabric.base - \u001b[0;36mDEBUG\u001b[0m - \u001b[0;36mvalidate_after_k_steps value: 1000, type: <class 'int'>\u001b[0m\n",
      "[\u001b[0;36mDEBUG\u001b[0m | src.tasks.clm_training.fabric.base]: \u001b[0;36mvalidate_after_k_steps value: 1000, type: <class 'int'>\u001b[0m\n",
      "2025-06-16 13:55:07 - src.tasks.clm_training.fabric.base - \u001b[0;36mDEBUG\u001b[0m - \u001b[0;36mstep_count value: 49, type: <class 'int'>\u001b[0m\n",
      "[\u001b[0;36mDEBUG\u001b[0m | src.tasks.clm_training.fabric.base]: \u001b[0;36mstep_count value: 49, type: <class 'int'>\u001b[0m\n",
      "2025-06-16 13:55:07 - src.tasks.clm_training.fabric.base - \u001b[0;36mDEBUG\u001b[0m - \u001b[0;36mValidation condition check - epochFinished: False, trainingFinished: False\u001b[0m\n",
      "[\u001b[0;36mDEBUG\u001b[0m | src.tasks.clm_training.fabric.base]: \u001b[0;36mValidation condition check - epochFinished: False, trainingFinished: False\u001b[0m\n",
      "2025-06-16 13:55:07 - src.tasks.clm_training.fabric.base - \u001b[0;36mDEBUG\u001b[0m - \u001b[0;36mAbout to check: step_count % validate_after_k_steps == 0 -> 49 % 1000 == 0\u001b[0m\n",
      "[\u001b[0;36mDEBUG\u001b[0m | src.tasks.clm_training.fabric.base]: \u001b[0;36mAbout to check: step_count % validate_after_k_steps == 0 -> 49 % 1000 == 0\u001b[0m\n",
      "2025-06-16 13:55:07 - src.tasks.clm_training.fabric.base - \u001b[0;36mDEBUG\u001b[0m - \u001b[0;36miter 98 step 49: loss 5.4688, iter time: 420.00ms remaining time: \u001b[0m\n",
      "[\u001b[0;36mDEBUG\u001b[0m | src.tasks.clm_training.fabric.base]: \u001b[0;36miter 98 step 49: loss 5.4688, iter time: 420.00ms remaining time: \u001b[0m\n",
      "  2%|\u001b[34m▊                                        \u001b[0m| 98/4682 [00:41<32:24,  2.36it/s]\u001b[0m2025-06-16 13:55:07 - src.tasks.clm_training.fabric.base - \u001b[0;36mDEBUG\u001b[0m - \u001b[0;36mgradient_accumulation_steps raw value: 2, type: <class 'int'>\u001b[0m\n",
      "[\u001b[0;36mDEBUG\u001b[0m | src.tasks.clm_training.fabric.base]: \u001b[0;36mgradient_accumulation_steps raw value: 2, type: <class 'int'>\u001b[0m\n",
      "2025-06-16 13:55:07 - src.tasks.clm_training.fabric.base - \u001b[0;36mDEBUG\u001b[0m - \u001b[0;36mgradient_accumulation_steps converted: 2, type: <class 'int'>\u001b[0m\n",
      "[\u001b[0;36mDEBUG\u001b[0m | src.tasks.clm_training.fabric.base]: \u001b[0;36mgradient_accumulation_steps converted: 2, type: <class 'int'>\u001b[0m\n",
      "2025-06-16 13:55:07 - src.tasks.clm_training.fabric.base - \u001b[0;36mDEBUG\u001b[0m - \u001b[0;36miter_num value: 98, type: <class 'int'>\u001b[0m\n",
      "[\u001b[0;36mDEBUG\u001b[0m | src.tasks.clm_training.fabric.base]: \u001b[0;36miter_num value: 98, type: <class 'int'>\u001b[0m\n",
      "2025-06-16 13:55:07 - src.tasks.clm_training.fabric.base - \u001b[0;36mDEBUG\u001b[0m - \u001b[0;36mAbout to check: (iter_num + 1) % gradient_accumulation_steps != 0 -> (98 + 1) % 2 != 0\u001b[0m\n",
      "[\u001b[0;36mDEBUG\u001b[0m | src.tasks.clm_training.fabric.base]: \u001b[0;36mAbout to check: (iter_num + 1) % gradient_accumulation_steps != 0 -> (98 + 1) % 2 != 0\u001b[0m\n",
      "2025-06-16 13:55:08 - src.tasks.clm_training.fabric.base - \u001b[0;36mDEBUG\u001b[0m - \u001b[0;36miter 99 step 49: loss 5.7188, iter time: 122.23ms remaining time: \u001b[0m\n",
      "[\u001b[0;36mDEBUG\u001b[0m | src.tasks.clm_training.fabric.base]: \u001b[0;36miter 99 step 49: loss 5.7188, iter time: 122.23ms remaining time: \u001b[0m\n",
      "  2%|\u001b[34m▊                                        \u001b[0m| 99/4682 [00:42<32:12,  2.37it/s]\u001b[0m2025-06-16 13:55:08 - src.tasks.clm_training.fabric.base - \u001b[0;36mDEBUG\u001b[0m - \u001b[0;36mgradient_accumulation_steps raw value: 2, type: <class 'int'>\u001b[0m\n",
      "[\u001b[0;36mDEBUG\u001b[0m | src.tasks.clm_training.fabric.base]: \u001b[0;36mgradient_accumulation_steps raw value: 2, type: <class 'int'>\u001b[0m\n",
      "2025-06-16 13:55:08 - src.tasks.clm_training.fabric.base - \u001b[0;36mDEBUG\u001b[0m - \u001b[0;36mgradient_accumulation_steps converted: 2, type: <class 'int'>\u001b[0m\n",
      "[\u001b[0;36mDEBUG\u001b[0m | src.tasks.clm_training.fabric.base]: \u001b[0;36mgradient_accumulation_steps converted: 2, type: <class 'int'>\u001b[0m\n",
      "2025-06-16 13:55:08 - src.tasks.clm_training.fabric.base - \u001b[0;36mDEBUG\u001b[0m - \u001b[0;36miter_num value: 99, type: <class 'int'>\u001b[0m\n",
      "[\u001b[0;36mDEBUG\u001b[0m | src.tasks.clm_training.fabric.base]: \u001b[0;36miter_num value: 99, type: <class 'int'>\u001b[0m\n",
      "2025-06-16 13:55:08 - src.tasks.clm_training.fabric.base - \u001b[0;36mDEBUG\u001b[0m - \u001b[0;36mAbout to check: (iter_num + 1) % gradient_accumulation_steps != 0 -> (99 + 1) % 2 != 0\u001b[0m\n",
      "[\u001b[0;36mDEBUG\u001b[0m | src.tasks.clm_training.fabric.base]: \u001b[0;36mAbout to check: (iter_num + 1) % gradient_accumulation_steps != 0 -> (99 + 1) % 2 != 0\u001b[0m\n",
      "2025-06-16 13:55:08 - src.tasks.clm_training.fabric.base - \u001b[0;36mDEBUG\u001b[0m - \u001b[0;36mGradient norm before clipping: 25.7500\u001b[0m\n",
      "[\u001b[0;36mDEBUG\u001b[0m | src.tasks.clm_training.fabric.base]: \u001b[0;36mGradient norm before clipping: 25.7500\u001b[0m\n",
      "2025-06-16 13:55:08 - src.tasks.clm_training.fabric.base - \u001b[0;36mDEBUG\u001b[0m - \u001b[0;36mvalidate_after_k_steps value: 1000, type: <class 'int'>\u001b[0m\n",
      "[\u001b[0;36mDEBUG\u001b[0m | src.tasks.clm_training.fabric.base]: \u001b[0;36mvalidate_after_k_steps value: 1000, type: <class 'int'>\u001b[0m\n",
      "2025-06-16 13:55:08 - src.tasks.clm_training.fabric.base - \u001b[0;36mDEBUG\u001b[0m - \u001b[0;36mstep_count value: 50, type: <class 'int'>\u001b[0m\n",
      "[\u001b[0;36mDEBUG\u001b[0m | src.tasks.clm_training.fabric.base]: \u001b[0;36mstep_count value: 50, type: <class 'int'>\u001b[0m\n",
      "2025-06-16 13:55:08 - src.tasks.clm_training.fabric.base - \u001b[0;36mDEBUG\u001b[0m - \u001b[0;36mValidation condition check - epochFinished: False, trainingFinished: False\u001b[0m\n",
      "[\u001b[0;36mDEBUG\u001b[0m | src.tasks.clm_training.fabric.base]: \u001b[0;36mValidation condition check - epochFinished: False, trainingFinished: False\u001b[0m\n",
      "2025-06-16 13:55:08 - src.tasks.clm_training.fabric.base - \u001b[0;36mDEBUG\u001b[0m - \u001b[0;36mAbout to check: step_count % validate_after_k_steps == 0 -> 50 % 1000 == 0\u001b[0m\n",
      "[\u001b[0;36mDEBUG\u001b[0m | src.tasks.clm_training.fabric.base]: \u001b[0;36mAbout to check: step_count % validate_after_k_steps == 0 -> 50 % 1000 == 0\u001b[0m\n",
      "2025-06-16 13:55:08 - src.tasks.clm_training.fabric.base - \u001b[0;36mDEBUG\u001b[0m - \u001b[0;36miter 100 step 50: loss 6.0938, iter time: 420.07ms remaining time: \u001b[0m\n",
      "[\u001b[0;36mDEBUG\u001b[0m | src.tasks.clm_training.fabric.base]: \u001b[0;36miter 100 step 50: loss 6.0938, iter time: 420.07ms remaining time: \u001b[0m\n",
      "  2%|\u001b[34m▊                                       \u001b[0m| 100/4682 [00:42<32:23,  2.36it/s]\u001b[0m2025-06-16 13:55:08 - src.tasks.clm_training.fabric.base - \u001b[0;36mDEBUG\u001b[0m - \u001b[0;36mgradient_accumulation_steps raw value: 2, type: <class 'int'>\u001b[0m\n",
      "[\u001b[0;36mDEBUG\u001b[0m | src.tasks.clm_training.fabric.base]: \u001b[0;36mgradient_accumulation_steps raw value: 2, type: <class 'int'>\u001b[0m\n",
      "2025-06-16 13:55:08 - src.tasks.clm_training.fabric.base - \u001b[0;36mDEBUG\u001b[0m - \u001b[0;36mgradient_accumulation_steps converted: 2, type: <class 'int'>\u001b[0m\n",
      "[\u001b[0;36mDEBUG\u001b[0m | src.tasks.clm_training.fabric.base]: \u001b[0;36mgradient_accumulation_steps converted: 2, type: <class 'int'>\u001b[0m\n",
      "2025-06-16 13:55:08 - src.tasks.clm_training.fabric.base - \u001b[0;36mDEBUG\u001b[0m - \u001b[0;36miter_num value: 100, type: <class 'int'>\u001b[0m\n",
      "[\u001b[0;36mDEBUG\u001b[0m | src.tasks.clm_training.fabric.base]: \u001b[0;36miter_num value: 100, type: <class 'int'>\u001b[0m\n",
      "2025-06-16 13:55:08 - src.tasks.clm_training.fabric.base - \u001b[0;36mDEBUG\u001b[0m - \u001b[0;36mAbout to check: (iter_num + 1) % gradient_accumulation_steps != 0 -> (100 + 1) % 2 != 0\u001b[0m\n",
      "[\u001b[0;36mDEBUG\u001b[0m | src.tasks.clm_training.fabric.base]: \u001b[0;36mAbout to check: (iter_num + 1) % gradient_accumulation_steps != 0 -> (100 + 1) % 2 != 0\u001b[0m\n",
      "2025-06-16 13:55:09 - src.tasks.clm_training.fabric.base - \u001b[0;36mDEBUG\u001b[0m - \u001b[0;36miter 101 step 50: loss 5.8750, iter time: 122.06ms remaining time: \u001b[0m\n",
      "[\u001b[0;36mDEBUG\u001b[0m | src.tasks.clm_training.fabric.base]: \u001b[0;36miter 101 step 50: loss 5.8750, iter time: 122.06ms remaining time: \u001b[0m\n",
      "  2%|\u001b[34m▊                                       \u001b[0m| 101/4682 [00:42<32:10,  2.37it/s]\u001b[0m2025-06-16 13:55:09 - src.tasks.clm_training.fabric.base - \u001b[0;36mDEBUG\u001b[0m - \u001b[0;36mgradient_accumulation_steps raw value: 2, type: <class 'int'>\u001b[0m\n",
      "[\u001b[0;36mDEBUG\u001b[0m | src.tasks.clm_training.fabric.base]: \u001b[0;36mgradient_accumulation_steps raw value: 2, type: <class 'int'>\u001b[0m\n",
      "2025-06-16 13:55:09 - src.tasks.clm_training.fabric.base - \u001b[0;36mDEBUG\u001b[0m - \u001b[0;36mgradient_accumulation_steps converted: 2, type: <class 'int'>\u001b[0m\n",
      "[\u001b[0;36mDEBUG\u001b[0m | src.tasks.clm_training.fabric.base]: \u001b[0;36mgradient_accumulation_steps converted: 2, type: <class 'int'>\u001b[0m\n",
      "2025-06-16 13:55:09 - src.tasks.clm_training.fabric.base - \u001b[0;36mDEBUG\u001b[0m - \u001b[0;36miter_num value: 101, type: <class 'int'>\u001b[0m\n",
      "[\u001b[0;36mDEBUG\u001b[0m | src.tasks.clm_training.fabric.base]: \u001b[0;36miter_num value: 101, type: <class 'int'>\u001b[0m\n",
      "2025-06-16 13:55:09 - src.tasks.clm_training.fabric.base - \u001b[0;36mDEBUG\u001b[0m - \u001b[0;36mAbout to check: (iter_num + 1) % gradient_accumulation_steps != 0 -> (101 + 1) % 2 != 0\u001b[0m\n",
      "[\u001b[0;36mDEBUG\u001b[0m | src.tasks.clm_training.fabric.base]: \u001b[0;36mAbout to check: (iter_num + 1) % gradient_accumulation_steps != 0 -> (101 + 1) % 2 != 0\u001b[0m\n",
      "2025-06-16 13:55:09 - src.tasks.clm_training.fabric.base - \u001b[0;36mDEBUG\u001b[0m - \u001b[0;36mGradient norm before clipping: 22.7500\u001b[0m\n",
      "[\u001b[0;36mDEBUG\u001b[0m | src.tasks.clm_training.fabric.base]: \u001b[0;36mGradient norm before clipping: 22.7500\u001b[0m\n",
      "2025-06-16 13:55:09 - src.tasks.clm_training.fabric.base - \u001b[0;36mDEBUG\u001b[0m - \u001b[0;36mvalidate_after_k_steps value: 1000, type: <class 'int'>\u001b[0m\n",
      "[\u001b[0;36mDEBUG\u001b[0m | src.tasks.clm_training.fabric.base]: \u001b[0;36mvalidate_after_k_steps value: 1000, type: <class 'int'>\u001b[0m\n",
      "2025-06-16 13:55:09 - src.tasks.clm_training.fabric.base - \u001b[0;36mDEBUG\u001b[0m - \u001b[0;36mstep_count value: 51, type: <class 'int'>\u001b[0m\n",
      "[\u001b[0;36mDEBUG\u001b[0m | src.tasks.clm_training.fabric.base]: \u001b[0;36mstep_count value: 51, type: <class 'int'>\u001b[0m\n",
      "2025-06-16 13:55:09 - src.tasks.clm_training.fabric.base - \u001b[0;36mDEBUG\u001b[0m - \u001b[0;36mValidation condition check - epochFinished: False, trainingFinished: False\u001b[0m\n",
      "[\u001b[0;36mDEBUG\u001b[0m | src.tasks.clm_training.fabric.base]: \u001b[0;36mValidation condition check - epochFinished: False, trainingFinished: False\u001b[0m\n",
      "2025-06-16 13:55:09 - src.tasks.clm_training.fabric.base - \u001b[0;36mDEBUG\u001b[0m - \u001b[0;36mAbout to check: step_count % validate_after_k_steps == 0 -> 51 % 1000 == 0\u001b[0m\n",
      "[\u001b[0;36mDEBUG\u001b[0m | src.tasks.clm_training.fabric.base]: \u001b[0;36mAbout to check: step_count % validate_after_k_steps == 0 -> 51 % 1000 == 0\u001b[0m\n",
      "2025-06-16 13:55:09 - src.tasks.clm_training.fabric.base - \u001b[0;36mDEBUG\u001b[0m - \u001b[0;36miter 102 step 51: loss 5.5000, iter time: 420.56ms remaining time: \u001b[0m\n",
      "[\u001b[0;36mDEBUG\u001b[0m | src.tasks.clm_training.fabric.base]: \u001b[0;36miter 102 step 51: loss 5.5000, iter time: 420.56ms remaining time: \u001b[0m\n",
      "  2%|\u001b[34m▊                                       \u001b[0m| 102/4682 [00:43<32:22,  2.36it/s]\u001b[0m2025-06-16 13:55:09 - src.tasks.clm_training.fabric.base - \u001b[0;36mDEBUG\u001b[0m - \u001b[0;36mgradient_accumulation_steps raw value: 2, type: <class 'int'>\u001b[0m\n",
      "[\u001b[0;36mDEBUG\u001b[0m | src.tasks.clm_training.fabric.base]: \u001b[0;36mgradient_accumulation_steps raw value: 2, type: <class 'int'>\u001b[0m\n",
      "2025-06-16 13:55:09 - src.tasks.clm_training.fabric.base - \u001b[0;36mDEBUG\u001b[0m - \u001b[0;36mgradient_accumulation_steps converted: 2, type: <class 'int'>\u001b[0m\n",
      "[\u001b[0;36mDEBUG\u001b[0m | src.tasks.clm_training.fabric.base]: \u001b[0;36mgradient_accumulation_steps converted: 2, type: <class 'int'>\u001b[0m\n",
      "2025-06-16 13:55:09 - src.tasks.clm_training.fabric.base - \u001b[0;36mDEBUG\u001b[0m - \u001b[0;36miter_num value: 102, type: <class 'int'>\u001b[0m\n",
      "[\u001b[0;36mDEBUG\u001b[0m | src.tasks.clm_training.fabric.base]: \u001b[0;36miter_num value: 102, type: <class 'int'>\u001b[0m\n",
      "2025-06-16 13:55:09 - src.tasks.clm_training.fabric.base - \u001b[0;36mDEBUG\u001b[0m - \u001b[0;36mAbout to check: (iter_num + 1) % gradient_accumulation_steps != 0 -> (102 + 1) % 2 != 0\u001b[0m\n",
      "[\u001b[0;36mDEBUG\u001b[0m | src.tasks.clm_training.fabric.base]: \u001b[0;36mAbout to check: (iter_num + 1) % gradient_accumulation_steps != 0 -> (102 + 1) % 2 != 0\u001b[0m\n",
      "2025-06-16 13:55:09 - src.tasks.clm_training.fabric.base - \u001b[0;36mDEBUG\u001b[0m - \u001b[0;36miter 103 step 51: loss 5.7812, iter time: 122.49ms remaining time: \u001b[0m\n",
      "[\u001b[0;36mDEBUG\u001b[0m | src.tasks.clm_training.fabric.base]: \u001b[0;36miter 103 step 51: loss 5.7812, iter time: 122.49ms remaining time: \u001b[0m\n",
      "  2%|\u001b[34m▉                                       \u001b[0m| 103/4682 [00:43<32:10,  2.37it/s]\u001b[0m2025-06-16 13:55:09 - src.tasks.clm_training.fabric.base - \u001b[0;36mDEBUG\u001b[0m - \u001b[0;36mgradient_accumulation_steps raw value: 2, type: <class 'int'>\u001b[0m\n",
      "[\u001b[0;36mDEBUG\u001b[0m | src.tasks.clm_training.fabric.base]: \u001b[0;36mgradient_accumulation_steps raw value: 2, type: <class 'int'>\u001b[0m\n",
      "2025-06-16 13:55:09 - src.tasks.clm_training.fabric.base - \u001b[0;36mDEBUG\u001b[0m - \u001b[0;36mgradient_accumulation_steps converted: 2, type: <class 'int'>\u001b[0m\n",
      "[\u001b[0;36mDEBUG\u001b[0m | src.tasks.clm_training.fabric.base]: \u001b[0;36mgradient_accumulation_steps converted: 2, type: <class 'int'>\u001b[0m\n",
      "2025-06-16 13:55:09 - src.tasks.clm_training.fabric.base - \u001b[0;36mDEBUG\u001b[0m - \u001b[0;36miter_num value: 103, type: <class 'int'>\u001b[0m\n",
      "[\u001b[0;36mDEBUG\u001b[0m | src.tasks.clm_training.fabric.base]: \u001b[0;36miter_num value: 103, type: <class 'int'>\u001b[0m\n",
      "2025-06-16 13:55:09 - src.tasks.clm_training.fabric.base - \u001b[0;36mDEBUG\u001b[0m - \u001b[0;36mAbout to check: (iter_num + 1) % gradient_accumulation_steps != 0 -> (103 + 1) % 2 != 0\u001b[0m\n",
      "[\u001b[0;36mDEBUG\u001b[0m | src.tasks.clm_training.fabric.base]: \u001b[0;36mAbout to check: (iter_num + 1) % gradient_accumulation_steps != 0 -> (103 + 1) % 2 != 0\u001b[0m\n",
      "2025-06-16 13:55:10 - src.tasks.clm_training.fabric.base - \u001b[0;36mDEBUG\u001b[0m - \u001b[0;36mGradient norm before clipping: 21.3750\u001b[0m\n",
      "[\u001b[0;36mDEBUG\u001b[0m | src.tasks.clm_training.fabric.base]: \u001b[0;36mGradient norm before clipping: 21.3750\u001b[0m\n",
      "2025-06-16 13:55:10 - src.tasks.clm_training.fabric.base - \u001b[0;36mDEBUG\u001b[0m - \u001b[0;36mvalidate_after_k_steps value: 1000, type: <class 'int'>\u001b[0m\n",
      "[\u001b[0;36mDEBUG\u001b[0m | src.tasks.clm_training.fabric.base]: \u001b[0;36mvalidate_after_k_steps value: 1000, type: <class 'int'>\u001b[0m\n",
      "2025-06-16 13:55:10 - src.tasks.clm_training.fabric.base - \u001b[0;36mDEBUG\u001b[0m - \u001b[0;36mstep_count value: 52, type: <class 'int'>\u001b[0m\n",
      "[\u001b[0;36mDEBUG\u001b[0m | src.tasks.clm_training.fabric.base]: \u001b[0;36mstep_count value: 52, type: <class 'int'>\u001b[0m\n",
      "2025-06-16 13:55:10 - src.tasks.clm_training.fabric.base - \u001b[0;36mDEBUG\u001b[0m - \u001b[0;36mValidation condition check - epochFinished: False, trainingFinished: False\u001b[0m\n",
      "[\u001b[0;36mDEBUG\u001b[0m | src.tasks.clm_training.fabric.base]: \u001b[0;36mValidation condition check - epochFinished: False, trainingFinished: False\u001b[0m\n",
      "2025-06-16 13:55:10 - src.tasks.clm_training.fabric.base - \u001b[0;36mDEBUG\u001b[0m - \u001b[0;36mAbout to check: step_count % validate_after_k_steps == 0 -> 52 % 1000 == 0\u001b[0m\n",
      "[\u001b[0;36mDEBUG\u001b[0m | src.tasks.clm_training.fabric.base]: \u001b[0;36mAbout to check: step_count % validate_after_k_steps == 0 -> 52 % 1000 == 0\u001b[0m\n",
      "2025-06-16 13:55:10 - src.tasks.clm_training.fabric.base - \u001b[0;36mDEBUG\u001b[0m - \u001b[0;36miter 104 step 52: loss 5.9375, iter time: 420.04ms remaining time: \u001b[0m\n",
      "[\u001b[0;36mDEBUG\u001b[0m | src.tasks.clm_training.fabric.base]: \u001b[0;36miter 104 step 52: loss 5.9375, iter time: 420.04ms remaining time: \u001b[0m\n",
      "  2%|\u001b[34m▉                                       \u001b[0m| 104/4682 [00:44<32:21,  2.36it/s]\u001b[0m2025-06-16 13:55:10 - src.tasks.clm_training.fabric.base - \u001b[0;36mDEBUG\u001b[0m - \u001b[0;36mgradient_accumulation_steps raw value: 2, type: <class 'int'>\u001b[0m\n",
      "[\u001b[0;36mDEBUG\u001b[0m | src.tasks.clm_training.fabric.base]: \u001b[0;36mgradient_accumulation_steps raw value: 2, type: <class 'int'>\u001b[0m\n",
      "2025-06-16 13:55:10 - src.tasks.clm_training.fabric.base - \u001b[0;36mDEBUG\u001b[0m - \u001b[0;36mgradient_accumulation_steps converted: 2, type: <class 'int'>\u001b[0m\n",
      "[\u001b[0;36mDEBUG\u001b[0m | src.tasks.clm_training.fabric.base]: \u001b[0;36mgradient_accumulation_steps converted: 2, type: <class 'int'>\u001b[0m\n",
      "2025-06-16 13:55:10 - src.tasks.clm_training.fabric.base - \u001b[0;36mDEBUG\u001b[0m - \u001b[0;36miter_num value: 104, type: <class 'int'>\u001b[0m\n",
      "[\u001b[0;36mDEBUG\u001b[0m | src.tasks.clm_training.fabric.base]: \u001b[0;36miter_num value: 104, type: <class 'int'>\u001b[0m\n",
      "2025-06-16 13:55:10 - src.tasks.clm_training.fabric.base - \u001b[0;36mDEBUG\u001b[0m - \u001b[0;36mAbout to check: (iter_num + 1) % gradient_accumulation_steps != 0 -> (104 + 1) % 2 != 0\u001b[0m\n",
      "[\u001b[0;36mDEBUG\u001b[0m | src.tasks.clm_training.fabric.base]: \u001b[0;36mAbout to check: (iter_num + 1) % gradient_accumulation_steps != 0 -> (104 + 1) % 2 != 0\u001b[0m\n",
      "2025-06-16 13:55:10 - src.tasks.clm_training.fabric.base - \u001b[0;36mDEBUG\u001b[0m - \u001b[0;36miter 105 step 52: loss 5.5938, iter time: 122.04ms remaining time: \u001b[0m\n",
      "[\u001b[0;36mDEBUG\u001b[0m | src.tasks.clm_training.fabric.base]: \u001b[0;36miter 105 step 52: loss 5.5938, iter time: 122.04ms remaining time: \u001b[0m\n",
      "  2%|\u001b[34m▉                                       \u001b[0m| 105/4682 [00:44<32:09,  2.37it/s]\u001b[0m2025-06-16 13:55:10 - src.tasks.clm_training.fabric.base - \u001b[0;36mDEBUG\u001b[0m - \u001b[0;36mgradient_accumulation_steps raw value: 2, type: <class 'int'>\u001b[0m\n",
      "[\u001b[0;36mDEBUG\u001b[0m | src.tasks.clm_training.fabric.base]: \u001b[0;36mgradient_accumulation_steps raw value: 2, type: <class 'int'>\u001b[0m\n",
      "2025-06-16 13:55:10 - src.tasks.clm_training.fabric.base - \u001b[0;36mDEBUG\u001b[0m - \u001b[0;36mgradient_accumulation_steps converted: 2, type: <class 'int'>\u001b[0m\n",
      "[\u001b[0;36mDEBUG\u001b[0m | src.tasks.clm_training.fabric.base]: \u001b[0;36mgradient_accumulation_steps converted: 2, type: <class 'int'>\u001b[0m\n",
      "2025-06-16 13:55:10 - src.tasks.clm_training.fabric.base - \u001b[0;36mDEBUG\u001b[0m - \u001b[0;36miter_num value: 105, type: <class 'int'>\u001b[0m\n",
      "[\u001b[0;36mDEBUG\u001b[0m | src.tasks.clm_training.fabric.base]: \u001b[0;36miter_num value: 105, type: <class 'int'>\u001b[0m\n",
      "2025-06-16 13:55:10 - src.tasks.clm_training.fabric.base - \u001b[0;36mDEBUG\u001b[0m - \u001b[0;36mAbout to check: (iter_num + 1) % gradient_accumulation_steps != 0 -> (105 + 1) % 2 != 0\u001b[0m\n",
      "[\u001b[0;36mDEBUG\u001b[0m | src.tasks.clm_training.fabric.base]: \u001b[0;36mAbout to check: (iter_num + 1) % gradient_accumulation_steps != 0 -> (105 + 1) % 2 != 0\u001b[0m\n",
      "2025-06-16 13:55:11 - src.tasks.clm_training.fabric.base - \u001b[0;36mDEBUG\u001b[0m - \u001b[0;36mGradient norm before clipping: 27.0000\u001b[0m\n",
      "[\u001b[0;36mDEBUG\u001b[0m | src.tasks.clm_training.fabric.base]: \u001b[0;36mGradient norm before clipping: 27.0000\u001b[0m\n",
      "2025-06-16 13:55:11 - src.tasks.clm_training.fabric.base - \u001b[0;36mDEBUG\u001b[0m - \u001b[0;36mvalidate_after_k_steps value: 1000, type: <class 'int'>\u001b[0m\n",
      "[\u001b[0;36mDEBUG\u001b[0m | src.tasks.clm_training.fabric.base]: \u001b[0;36mvalidate_after_k_steps value: 1000, type: <class 'int'>\u001b[0m\n",
      "2025-06-16 13:55:11 - src.tasks.clm_training.fabric.base - \u001b[0;36mDEBUG\u001b[0m - \u001b[0;36mstep_count value: 53, type: <class 'int'>\u001b[0m\n",
      "[\u001b[0;36mDEBUG\u001b[0m | src.tasks.clm_training.fabric.base]: \u001b[0;36mstep_count value: 53, type: <class 'int'>\u001b[0m\n",
      "2025-06-16 13:55:11 - src.tasks.clm_training.fabric.base - \u001b[0;36mDEBUG\u001b[0m - \u001b[0;36mValidation condition check - epochFinished: False, trainingFinished: False\u001b[0m\n",
      "[\u001b[0;36mDEBUG\u001b[0m | src.tasks.clm_training.fabric.base]: \u001b[0;36mValidation condition check - epochFinished: False, trainingFinished: False\u001b[0m\n",
      "2025-06-16 13:55:11 - src.tasks.clm_training.fabric.base - \u001b[0;36mDEBUG\u001b[0m - \u001b[0;36mAbout to check: step_count % validate_after_k_steps == 0 -> 53 % 1000 == 0\u001b[0m\n",
      "[\u001b[0;36mDEBUG\u001b[0m | src.tasks.clm_training.fabric.base]: \u001b[0;36mAbout to check: step_count % validate_after_k_steps == 0 -> 53 % 1000 == 0\u001b[0m\n",
      "2025-06-16 13:55:11 - src.tasks.clm_training.fabric.base - \u001b[0;36mDEBUG\u001b[0m - \u001b[0;36miter 106 step 53: loss 5.8750, iter time: 419.89ms remaining time: \u001b[0m\n",
      "[\u001b[0;36mDEBUG\u001b[0m | src.tasks.clm_training.fabric.base]: \u001b[0;36miter 106 step 53: loss 5.8750, iter time: 419.89ms remaining time: \u001b[0m\n",
      "  2%|\u001b[34m▉                                       \u001b[0m| 106/4682 [00:45<32:20,  2.36it/s]\u001b[0m2025-06-16 13:55:11 - src.tasks.clm_training.fabric.base - \u001b[0;36mDEBUG\u001b[0m - \u001b[0;36mgradient_accumulation_steps raw value: 2, type: <class 'int'>\u001b[0m\n",
      "[\u001b[0;36mDEBUG\u001b[0m | src.tasks.clm_training.fabric.base]: \u001b[0;36mgradient_accumulation_steps raw value: 2, type: <class 'int'>\u001b[0m\n",
      "2025-06-16 13:55:11 - src.tasks.clm_training.fabric.base - \u001b[0;36mDEBUG\u001b[0m - \u001b[0;36mgradient_accumulation_steps converted: 2, type: <class 'int'>\u001b[0m\n",
      "[\u001b[0;36mDEBUG\u001b[0m | src.tasks.clm_training.fabric.base]: \u001b[0;36mgradient_accumulation_steps converted: 2, type: <class 'int'>\u001b[0m\n",
      "2025-06-16 13:55:11 - src.tasks.clm_training.fabric.base - \u001b[0;36mDEBUG\u001b[0m - \u001b[0;36miter_num value: 106, type: <class 'int'>\u001b[0m\n",
      "[\u001b[0;36mDEBUG\u001b[0m | src.tasks.clm_training.fabric.base]: \u001b[0;36miter_num value: 106, type: <class 'int'>\u001b[0m\n",
      "2025-06-16 13:55:11 - src.tasks.clm_training.fabric.base - \u001b[0;36mDEBUG\u001b[0m - \u001b[0;36mAbout to check: (iter_num + 1) % gradient_accumulation_steps != 0 -> (106 + 1) % 2 != 0\u001b[0m\n",
      "[\u001b[0;36mDEBUG\u001b[0m | src.tasks.clm_training.fabric.base]: \u001b[0;36mAbout to check: (iter_num + 1) % gradient_accumulation_steps != 0 -> (106 + 1) % 2 != 0\u001b[0m\n",
      "2025-06-16 13:55:11 - src.tasks.clm_training.fabric.base - \u001b[0;36mDEBUG\u001b[0m - \u001b[0;36miter 107 step 53: loss 5.7500, iter time: 122.04ms remaining time: \u001b[0m\n",
      "[\u001b[0;36mDEBUG\u001b[0m | src.tasks.clm_training.fabric.base]: \u001b[0;36miter 107 step 53: loss 5.7500, iter time: 122.04ms remaining time: \u001b[0m\n",
      "  2%|\u001b[34m▉                                       \u001b[0m| 107/4682 [00:45<32:08,  2.37it/s]\u001b[0m2025-06-16 13:55:11 - src.tasks.clm_training.fabric.base - \u001b[0;36mDEBUG\u001b[0m - \u001b[0;36mgradient_accumulation_steps raw value: 2, type: <class 'int'>\u001b[0m\n",
      "[\u001b[0;36mDEBUG\u001b[0m | src.tasks.clm_training.fabric.base]: \u001b[0;36mgradient_accumulation_steps raw value: 2, type: <class 'int'>\u001b[0m\n",
      "2025-06-16 13:55:11 - src.tasks.clm_training.fabric.base - \u001b[0;36mDEBUG\u001b[0m - \u001b[0;36mgradient_accumulation_steps converted: 2, type: <class 'int'>\u001b[0m\n",
      "[\u001b[0;36mDEBUG\u001b[0m | src.tasks.clm_training.fabric.base]: \u001b[0;36mgradient_accumulation_steps converted: 2, type: <class 'int'>\u001b[0m\n",
      "2025-06-16 13:55:11 - src.tasks.clm_training.fabric.base - \u001b[0;36mDEBUG\u001b[0m - \u001b[0;36miter_num value: 107, type: <class 'int'>\u001b[0m\n",
      "[\u001b[0;36mDEBUG\u001b[0m | src.tasks.clm_training.fabric.base]: \u001b[0;36miter_num value: 107, type: <class 'int'>\u001b[0m\n",
      "2025-06-16 13:55:11 - src.tasks.clm_training.fabric.base - \u001b[0;36mDEBUG\u001b[0m - \u001b[0;36mAbout to check: (iter_num + 1) % gradient_accumulation_steps != 0 -> (107 + 1) % 2 != 0\u001b[0m\n",
      "[\u001b[0;36mDEBUG\u001b[0m | src.tasks.clm_training.fabric.base]: \u001b[0;36mAbout to check: (iter_num + 1) % gradient_accumulation_steps != 0 -> (107 + 1) % 2 != 0\u001b[0m\n",
      "2025-06-16 13:55:11 - src.tasks.clm_training.fabric.base - \u001b[0;36mDEBUG\u001b[0m - \u001b[0;36mGradient norm before clipping: 27.6250\u001b[0m\n",
      "[\u001b[0;36mDEBUG\u001b[0m | src.tasks.clm_training.fabric.base]: \u001b[0;36mGradient norm before clipping: 27.6250\u001b[0m\n",
      "2025-06-16 13:55:11 - src.tasks.clm_training.fabric.base - \u001b[0;36mDEBUG\u001b[0m - \u001b[0;36mvalidate_after_k_steps value: 1000, type: <class 'int'>\u001b[0m\n",
      "[\u001b[0;36mDEBUG\u001b[0m | src.tasks.clm_training.fabric.base]: \u001b[0;36mvalidate_after_k_steps value: 1000, type: <class 'int'>\u001b[0m\n",
      "2025-06-16 13:55:11 - src.tasks.clm_training.fabric.base - \u001b[0;36mDEBUG\u001b[0m - \u001b[0;36mstep_count value: 54, type: <class 'int'>\u001b[0m\n",
      "[\u001b[0;36mDEBUG\u001b[0m | src.tasks.clm_training.fabric.base]: \u001b[0;36mstep_count value: 54, type: <class 'int'>\u001b[0m\n",
      "2025-06-16 13:55:11 - src.tasks.clm_training.fabric.base - \u001b[0;36mDEBUG\u001b[0m - \u001b[0;36mValidation condition check - epochFinished: False, trainingFinished: False\u001b[0m\n",
      "[\u001b[0;36mDEBUG\u001b[0m | src.tasks.clm_training.fabric.base]: \u001b[0;36mValidation condition check - epochFinished: False, trainingFinished: False\u001b[0m\n",
      "2025-06-16 13:55:11 - src.tasks.clm_training.fabric.base - \u001b[0;36mDEBUG\u001b[0m - \u001b[0;36mAbout to check: step_count % validate_after_k_steps == 0 -> 54 % 1000 == 0\u001b[0m\n",
      "[\u001b[0;36mDEBUG\u001b[0m | src.tasks.clm_training.fabric.base]: \u001b[0;36mAbout to check: step_count % validate_after_k_steps == 0 -> 54 % 1000 == 0\u001b[0m\n",
      "2025-06-16 13:55:11 - src.tasks.clm_training.fabric.base - \u001b[0;36mDEBUG\u001b[0m - \u001b[0;36miter 108 step 54: loss 6.2188, iter time: 420.51ms remaining time: \u001b[0m\n",
      "[\u001b[0;36mDEBUG\u001b[0m | src.tasks.clm_training.fabric.base]: \u001b[0;36miter 108 step 54: loss 6.2188, iter time: 420.51ms remaining time: \u001b[0m\n",
      "  2%|\u001b[34m▉                                       \u001b[0m| 108/4682 [00:45<32:20,  2.36it/s]\u001b[0m2025-06-16 13:55:11 - src.tasks.clm_training.fabric.base - \u001b[0;36mDEBUG\u001b[0m - \u001b[0;36mgradient_accumulation_steps raw value: 2, type: <class 'int'>\u001b[0m\n",
      "[\u001b[0;36mDEBUG\u001b[0m | src.tasks.clm_training.fabric.base]: \u001b[0;36mgradient_accumulation_steps raw value: 2, type: <class 'int'>\u001b[0m\n",
      "2025-06-16 13:55:11 - src.tasks.clm_training.fabric.base - \u001b[0;36mDEBUG\u001b[0m - \u001b[0;36mgradient_accumulation_steps converted: 2, type: <class 'int'>\u001b[0m\n",
      "[\u001b[0;36mDEBUG\u001b[0m | src.tasks.clm_training.fabric.base]: \u001b[0;36mgradient_accumulation_steps converted: 2, type: <class 'int'>\u001b[0m\n",
      "2025-06-16 13:55:11 - src.tasks.clm_training.fabric.base - \u001b[0;36mDEBUG\u001b[0m - \u001b[0;36miter_num value: 108, type: <class 'int'>\u001b[0m\n",
      "[\u001b[0;36mDEBUG\u001b[0m | src.tasks.clm_training.fabric.base]: \u001b[0;36miter_num value: 108, type: <class 'int'>\u001b[0m\n",
      "2025-06-16 13:55:11 - src.tasks.clm_training.fabric.base - \u001b[0;36mDEBUG\u001b[0m - \u001b[0;36mAbout to check: (iter_num + 1) % gradient_accumulation_steps != 0 -> (108 + 1) % 2 != 0\u001b[0m\n",
      "[\u001b[0;36mDEBUG\u001b[0m | src.tasks.clm_training.fabric.base]: \u001b[0;36mAbout to check: (iter_num + 1) % gradient_accumulation_steps != 0 -> (108 + 1) % 2 != 0\u001b[0m\n",
      "2025-06-16 13:55:12 - src.tasks.clm_training.fabric.base - \u001b[0;36mDEBUG\u001b[0m - \u001b[0;36miter 109 step 54: loss 5.5312, iter time: 122.67ms remaining time: \u001b[0m\n",
      "[\u001b[0;36mDEBUG\u001b[0m | src.tasks.clm_training.fabric.base]: \u001b[0;36miter 109 step 54: loss 5.5312, iter time: 122.67ms remaining time: \u001b[0m\n",
      "  2%|\u001b[34m▉                                       \u001b[0m| 109/4682 [00:46<32:09,  2.37it/s]\u001b[0m2025-06-16 13:55:12 - src.tasks.clm_training.fabric.base - \u001b[0;36mDEBUG\u001b[0m - \u001b[0;36mgradient_accumulation_steps raw value: 2, type: <class 'int'>\u001b[0m\n",
      "[\u001b[0;36mDEBUG\u001b[0m | src.tasks.clm_training.fabric.base]: \u001b[0;36mgradient_accumulation_steps raw value: 2, type: <class 'int'>\u001b[0m\n",
      "2025-06-16 13:55:12 - src.tasks.clm_training.fabric.base - \u001b[0;36mDEBUG\u001b[0m - \u001b[0;36mgradient_accumulation_steps converted: 2, type: <class 'int'>\u001b[0m\n",
      "[\u001b[0;36mDEBUG\u001b[0m | src.tasks.clm_training.fabric.base]: \u001b[0;36mgradient_accumulation_steps converted: 2, type: <class 'int'>\u001b[0m\n",
      "2025-06-16 13:55:12 - src.tasks.clm_training.fabric.base - \u001b[0;36mDEBUG\u001b[0m - \u001b[0;36miter_num value: 109, type: <class 'int'>\u001b[0m\n",
      "[\u001b[0;36mDEBUG\u001b[0m | src.tasks.clm_training.fabric.base]: \u001b[0;36miter_num value: 109, type: <class 'int'>\u001b[0m\n",
      "2025-06-16 13:55:12 - src.tasks.clm_training.fabric.base - \u001b[0;36mDEBUG\u001b[0m - \u001b[0;36mAbout to check: (iter_num + 1) % gradient_accumulation_steps != 0 -> (109 + 1) % 2 != 0\u001b[0m\n",
      "[\u001b[0;36mDEBUG\u001b[0m | src.tasks.clm_training.fabric.base]: \u001b[0;36mAbout to check: (iter_num + 1) % gradient_accumulation_steps != 0 -> (109 + 1) % 2 != 0\u001b[0m\n",
      "2025-06-16 13:55:12 - src.tasks.clm_training.fabric.base - \u001b[0;36mDEBUG\u001b[0m - \u001b[0;36mGradient norm before clipping: 23.5000\u001b[0m\n",
      "[\u001b[0;36mDEBUG\u001b[0m | src.tasks.clm_training.fabric.base]: \u001b[0;36mGradient norm before clipping: 23.5000\u001b[0m\n",
      "2025-06-16 13:55:12 - src.tasks.clm_training.fabric.base - \u001b[0;36mDEBUG\u001b[0m - \u001b[0;36mvalidate_after_k_steps value: 1000, type: <class 'int'>\u001b[0m\n",
      "[\u001b[0;36mDEBUG\u001b[0m | src.tasks.clm_training.fabric.base]: \u001b[0;36mvalidate_after_k_steps value: 1000, type: <class 'int'>\u001b[0m\n",
      "2025-06-16 13:55:12 - src.tasks.clm_training.fabric.base - \u001b[0;36mDEBUG\u001b[0m - \u001b[0;36mstep_count value: 55, type: <class 'int'>\u001b[0m\n",
      "[\u001b[0;36mDEBUG\u001b[0m | src.tasks.clm_training.fabric.base]: \u001b[0;36mstep_count value: 55, type: <class 'int'>\u001b[0m\n",
      "2025-06-16 13:55:12 - src.tasks.clm_training.fabric.base - \u001b[0;36mDEBUG\u001b[0m - \u001b[0;36mValidation condition check - epochFinished: False, trainingFinished: False\u001b[0m\n",
      "[\u001b[0;36mDEBUG\u001b[0m | src.tasks.clm_training.fabric.base]: \u001b[0;36mValidation condition check - epochFinished: False, trainingFinished: False\u001b[0m\n",
      "2025-06-16 13:55:12 - src.tasks.clm_training.fabric.base - \u001b[0;36mDEBUG\u001b[0m - \u001b[0;36mAbout to check: step_count % validate_after_k_steps == 0 -> 55 % 1000 == 0\u001b[0m\n",
      "[\u001b[0;36mDEBUG\u001b[0m | src.tasks.clm_training.fabric.base]: \u001b[0;36mAbout to check: step_count % validate_after_k_steps == 0 -> 55 % 1000 == 0\u001b[0m\n",
      "2025-06-16 13:55:12 - src.tasks.clm_training.fabric.base - \u001b[0;36mDEBUG\u001b[0m - \u001b[0;36miter 110 step 55: loss 5.8438, iter time: 419.98ms remaining time: \u001b[0m\n",
      "[\u001b[0;36mDEBUG\u001b[0m | src.tasks.clm_training.fabric.base]: \u001b[0;36miter 110 step 55: loss 5.8438, iter time: 419.98ms remaining time: \u001b[0m\n",
      "  2%|\u001b[34m▉                                       \u001b[0m| 110/4682 [00:46<32:20,  2.36it/s]\u001b[0m2025-06-16 13:55:12 - src.tasks.clm_training.fabric.base - \u001b[0;36mDEBUG\u001b[0m - \u001b[0;36mgradient_accumulation_steps raw value: 2, type: <class 'int'>\u001b[0m\n",
      "[\u001b[0;36mDEBUG\u001b[0m | src.tasks.clm_training.fabric.base]: \u001b[0;36mgradient_accumulation_steps raw value: 2, type: <class 'int'>\u001b[0m\n",
      "2025-06-16 13:55:12 - src.tasks.clm_training.fabric.base - \u001b[0;36mDEBUG\u001b[0m - \u001b[0;36mgradient_accumulation_steps converted: 2, type: <class 'int'>\u001b[0m\n",
      "[\u001b[0;36mDEBUG\u001b[0m | src.tasks.clm_training.fabric.base]: \u001b[0;36mgradient_accumulation_steps converted: 2, type: <class 'int'>\u001b[0m\n",
      "2025-06-16 13:55:12 - src.tasks.clm_training.fabric.base - \u001b[0;36mDEBUG\u001b[0m - \u001b[0;36miter_num value: 110, type: <class 'int'>\u001b[0m\n",
      "[\u001b[0;36mDEBUG\u001b[0m | src.tasks.clm_training.fabric.base]: \u001b[0;36miter_num value: 110, type: <class 'int'>\u001b[0m\n",
      "2025-06-16 13:55:12 - src.tasks.clm_training.fabric.base - \u001b[0;36mDEBUG\u001b[0m - \u001b[0;36mAbout to check: (iter_num + 1) % gradient_accumulation_steps != 0 -> (110 + 1) % 2 != 0\u001b[0m\n",
      "[\u001b[0;36mDEBUG\u001b[0m | src.tasks.clm_training.fabric.base]: \u001b[0;36mAbout to check: (iter_num + 1) % gradient_accumulation_steps != 0 -> (110 + 1) % 2 != 0\u001b[0m\n",
      "2025-06-16 13:55:13 - src.tasks.clm_training.fabric.base - \u001b[0;36mDEBUG\u001b[0m - \u001b[0;36miter 111 step 55: loss 6.0938, iter time: 122.55ms remaining time: \u001b[0m\n",
      "[\u001b[0;36mDEBUG\u001b[0m | src.tasks.clm_training.fabric.base]: \u001b[0;36miter 111 step 55: loss 6.0938, iter time: 122.55ms remaining time: \u001b[0m\n",
      "  2%|\u001b[34m▉                                       \u001b[0m| 111/4682 [00:47<32:08,  2.37it/s]\u001b[0m2025-06-16 13:55:13 - src.tasks.clm_training.fabric.base - \u001b[0;36mDEBUG\u001b[0m - \u001b[0;36mgradient_accumulation_steps raw value: 2, type: <class 'int'>\u001b[0m\n",
      "[\u001b[0;36mDEBUG\u001b[0m | src.tasks.clm_training.fabric.base]: \u001b[0;36mgradient_accumulation_steps raw value: 2, type: <class 'int'>\u001b[0m\n",
      "2025-06-16 13:55:13 - src.tasks.clm_training.fabric.base - \u001b[0;36mDEBUG\u001b[0m - \u001b[0;36mgradient_accumulation_steps converted: 2, type: <class 'int'>\u001b[0m\n",
      "[\u001b[0;36mDEBUG\u001b[0m | src.tasks.clm_training.fabric.base]: \u001b[0;36mgradient_accumulation_steps converted: 2, type: <class 'int'>\u001b[0m\n",
      "2025-06-16 13:55:13 - src.tasks.clm_training.fabric.base - \u001b[0;36mDEBUG\u001b[0m - \u001b[0;36miter_num value: 111, type: <class 'int'>\u001b[0m\n",
      "[\u001b[0;36mDEBUG\u001b[0m | src.tasks.clm_training.fabric.base]: \u001b[0;36miter_num value: 111, type: <class 'int'>\u001b[0m\n",
      "2025-06-16 13:55:13 - src.tasks.clm_training.fabric.base - \u001b[0;36mDEBUG\u001b[0m - \u001b[0;36mAbout to check: (iter_num + 1) % gradient_accumulation_steps != 0 -> (111 + 1) % 2 != 0\u001b[0m\n",
      "[\u001b[0;36mDEBUG\u001b[0m | src.tasks.clm_training.fabric.base]: \u001b[0;36mAbout to check: (iter_num + 1) % gradient_accumulation_steps != 0 -> (111 + 1) % 2 != 0\u001b[0m\n",
      "2025-06-16 13:55:13 - src.tasks.clm_training.fabric.base - \u001b[0;36mDEBUG\u001b[0m - \u001b[0;36mGradient norm before clipping: 24.1250\u001b[0m\n",
      "[\u001b[0;36mDEBUG\u001b[0m | src.tasks.clm_training.fabric.base]: \u001b[0;36mGradient norm before clipping: 24.1250\u001b[0m\n",
      "2025-06-16 13:55:13 - src.tasks.clm_training.fabric.base - \u001b[0;36mDEBUG\u001b[0m - \u001b[0;36mvalidate_after_k_steps value: 1000, type: <class 'int'>\u001b[0m\n",
      "[\u001b[0;36mDEBUG\u001b[0m | src.tasks.clm_training.fabric.base]: \u001b[0;36mvalidate_after_k_steps value: 1000, type: <class 'int'>\u001b[0m\n",
      "2025-06-16 13:55:13 - src.tasks.clm_training.fabric.base - \u001b[0;36mDEBUG\u001b[0m - \u001b[0;36mstep_count value: 56, type: <class 'int'>\u001b[0m\n",
      "[\u001b[0;36mDEBUG\u001b[0m | src.tasks.clm_training.fabric.base]: \u001b[0;36mstep_count value: 56, type: <class 'int'>\u001b[0m\n",
      "2025-06-16 13:55:13 - src.tasks.clm_training.fabric.base - \u001b[0;36mDEBUG\u001b[0m - \u001b[0;36mValidation condition check - epochFinished: False, trainingFinished: False\u001b[0m\n",
      "[\u001b[0;36mDEBUG\u001b[0m | src.tasks.clm_training.fabric.base]: \u001b[0;36mValidation condition check - epochFinished: False, trainingFinished: False\u001b[0m\n",
      "2025-06-16 13:55:13 - src.tasks.clm_training.fabric.base - \u001b[0;36mDEBUG\u001b[0m - \u001b[0;36mAbout to check: step_count % validate_after_k_steps == 0 -> 56 % 1000 == 0\u001b[0m\n",
      "[\u001b[0;36mDEBUG\u001b[0m | src.tasks.clm_training.fabric.base]: \u001b[0;36mAbout to check: step_count % validate_after_k_steps == 0 -> 56 % 1000 == 0\u001b[0m\n",
      "2025-06-16 13:55:13 - src.tasks.clm_training.fabric.base - \u001b[0;36mDEBUG\u001b[0m - \u001b[0;36miter 112 step 56: loss 5.5312, iter time: 419.46ms remaining time: \u001b[0m\n",
      "[\u001b[0;36mDEBUG\u001b[0m | src.tasks.clm_training.fabric.base]: \u001b[0;36miter 112 step 56: loss 5.5312, iter time: 419.46ms remaining time: \u001b[0m\n",
      "  2%|\u001b[34m▉                                       \u001b[0m| 112/4682 [00:47<32:17,  2.36it/s]\u001b[0m2025-06-16 13:55:13 - src.tasks.clm_training.fabric.base - \u001b[0;36mDEBUG\u001b[0m - \u001b[0;36mgradient_accumulation_steps raw value: 2, type: <class 'int'>\u001b[0m\n",
      "[\u001b[0;36mDEBUG\u001b[0m | src.tasks.clm_training.fabric.base]: \u001b[0;36mgradient_accumulation_steps raw value: 2, type: <class 'int'>\u001b[0m\n",
      "2025-06-16 13:55:13 - src.tasks.clm_training.fabric.base - \u001b[0;36mDEBUG\u001b[0m - \u001b[0;36mgradient_accumulation_steps converted: 2, type: <class 'int'>\u001b[0m\n",
      "[\u001b[0;36mDEBUG\u001b[0m | src.tasks.clm_training.fabric.base]: \u001b[0;36mgradient_accumulation_steps converted: 2, type: <class 'int'>\u001b[0m\n",
      "2025-06-16 13:55:13 - src.tasks.clm_training.fabric.base - \u001b[0;36mDEBUG\u001b[0m - \u001b[0;36miter_num value: 112, type: <class 'int'>\u001b[0m\n",
      "[\u001b[0;36mDEBUG\u001b[0m | src.tasks.clm_training.fabric.base]: \u001b[0;36miter_num value: 112, type: <class 'int'>\u001b[0m\n",
      "2025-06-16 13:55:13 - src.tasks.clm_training.fabric.base - \u001b[0;36mDEBUG\u001b[0m - \u001b[0;36mAbout to check: (iter_num + 1) % gradient_accumulation_steps != 0 -> (112 + 1) % 2 != 0\u001b[0m\n",
      "[\u001b[0;36mDEBUG\u001b[0m | src.tasks.clm_training.fabric.base]: \u001b[0;36mAbout to check: (iter_num + 1) % gradient_accumulation_steps != 0 -> (112 + 1) % 2 != 0\u001b[0m\n",
      "2025-06-16 13:55:14 - src.tasks.clm_training.fabric.base - \u001b[0;36mDEBUG\u001b[0m - \u001b[0;36miter 113 step 56: loss 5.8125, iter time: 122.62ms remaining time: \u001b[0m\n",
      "[\u001b[0;36mDEBUG\u001b[0m | src.tasks.clm_training.fabric.base]: \u001b[0;36miter 113 step 56: loss 5.8125, iter time: 122.62ms remaining time: \u001b[0m\n",
      "  2%|\u001b[34m▉                                       \u001b[0m| 113/4682 [00:48<32:06,  2.37it/s]\u001b[0m2025-06-16 13:55:14 - src.tasks.clm_training.fabric.base - \u001b[0;36mDEBUG\u001b[0m - \u001b[0;36mgradient_accumulation_steps raw value: 2, type: <class 'int'>\u001b[0m\n",
      "[\u001b[0;36mDEBUG\u001b[0m | src.tasks.clm_training.fabric.base]: \u001b[0;36mgradient_accumulation_steps raw value: 2, type: <class 'int'>\u001b[0m\n",
      "2025-06-16 13:55:14 - src.tasks.clm_training.fabric.base - \u001b[0;36mDEBUG\u001b[0m - \u001b[0;36mgradient_accumulation_steps converted: 2, type: <class 'int'>\u001b[0m\n",
      "[\u001b[0;36mDEBUG\u001b[0m | src.tasks.clm_training.fabric.base]: \u001b[0;36mgradient_accumulation_steps converted: 2, type: <class 'int'>\u001b[0m\n",
      "2025-06-16 13:55:14 - src.tasks.clm_training.fabric.base - \u001b[0;36mDEBUG\u001b[0m - \u001b[0;36miter_num value: 113, type: <class 'int'>\u001b[0m\n",
      "[\u001b[0;36mDEBUG\u001b[0m | src.tasks.clm_training.fabric.base]: \u001b[0;36miter_num value: 113, type: <class 'int'>\u001b[0m\n",
      "2025-06-16 13:55:14 - src.tasks.clm_training.fabric.base - \u001b[0;36mDEBUG\u001b[0m - \u001b[0;36mAbout to check: (iter_num + 1) % gradient_accumulation_steps != 0 -> (113 + 1) % 2 != 0\u001b[0m\n",
      "[\u001b[0;36mDEBUG\u001b[0m | src.tasks.clm_training.fabric.base]: \u001b[0;36mAbout to check: (iter_num + 1) % gradient_accumulation_steps != 0 -> (113 + 1) % 2 != 0\u001b[0m\n",
      "2025-06-16 13:55:14 - src.tasks.clm_training.fabric.base - \u001b[0;36mDEBUG\u001b[0m - \u001b[0;36mGradient norm before clipping: 25.2500\u001b[0m\n",
      "[\u001b[0;36mDEBUG\u001b[0m | src.tasks.clm_training.fabric.base]: \u001b[0;36mGradient norm before clipping: 25.2500\u001b[0m\n",
      "2025-06-16 13:55:14 - src.tasks.clm_training.fabric.base - \u001b[0;36mDEBUG\u001b[0m - \u001b[0;36mvalidate_after_k_steps value: 1000, type: <class 'int'>\u001b[0m\n",
      "[\u001b[0;36mDEBUG\u001b[0m | src.tasks.clm_training.fabric.base]: \u001b[0;36mvalidate_after_k_steps value: 1000, type: <class 'int'>\u001b[0m\n",
      "2025-06-16 13:55:14 - src.tasks.clm_training.fabric.base - \u001b[0;36mDEBUG\u001b[0m - \u001b[0;36mstep_count value: 57, type: <class 'int'>\u001b[0m\n",
      "[\u001b[0;36mDEBUG\u001b[0m | src.tasks.clm_training.fabric.base]: \u001b[0;36mstep_count value: 57, type: <class 'int'>\u001b[0m\n",
      "2025-06-16 13:55:14 - src.tasks.clm_training.fabric.base - \u001b[0;36mDEBUG\u001b[0m - \u001b[0;36mValidation condition check - epochFinished: False, trainingFinished: False\u001b[0m\n",
      "[\u001b[0;36mDEBUG\u001b[0m | src.tasks.clm_training.fabric.base]: \u001b[0;36mValidation condition check - epochFinished: False, trainingFinished: False\u001b[0m\n",
      "2025-06-16 13:55:14 - src.tasks.clm_training.fabric.base - \u001b[0;36mDEBUG\u001b[0m - \u001b[0;36mAbout to check: step_count % validate_after_k_steps == 0 -> 57 % 1000 == 0\u001b[0m\n",
      "[\u001b[0;36mDEBUG\u001b[0m | src.tasks.clm_training.fabric.base]: \u001b[0;36mAbout to check: step_count % validate_after_k_steps == 0 -> 57 % 1000 == 0\u001b[0m\n",
      "2025-06-16 13:55:14 - src.tasks.clm_training.fabric.base - \u001b[0;36mDEBUG\u001b[0m - \u001b[0;36miter 114 step 57: loss 5.7500, iter time: 419.78ms remaining time: \u001b[0m\n",
      "[\u001b[0;36mDEBUG\u001b[0m | src.tasks.clm_training.fabric.base]: \u001b[0;36miter 114 step 57: loss 5.7500, iter time: 419.78ms remaining time: \u001b[0m\n",
      "  2%|\u001b[34m▉                                       \u001b[0m| 114/4682 [00:48<32:17,  2.36it/s]\u001b[0m2025-06-16 13:55:14 - src.tasks.clm_training.fabric.base - \u001b[0;36mDEBUG\u001b[0m - \u001b[0;36mgradient_accumulation_steps raw value: 2, type: <class 'int'>\u001b[0m\n",
      "[\u001b[0;36mDEBUG\u001b[0m | src.tasks.clm_training.fabric.base]: \u001b[0;36mgradient_accumulation_steps raw value: 2, type: <class 'int'>\u001b[0m\n",
      "2025-06-16 13:55:14 - src.tasks.clm_training.fabric.base - \u001b[0;36mDEBUG\u001b[0m - \u001b[0;36mgradient_accumulation_steps converted: 2, type: <class 'int'>\u001b[0m\n",
      "[\u001b[0;36mDEBUG\u001b[0m | src.tasks.clm_training.fabric.base]: \u001b[0;36mgradient_accumulation_steps converted: 2, type: <class 'int'>\u001b[0m\n",
      "2025-06-16 13:55:14 - src.tasks.clm_training.fabric.base - \u001b[0;36mDEBUG\u001b[0m - \u001b[0;36miter_num value: 114, type: <class 'int'>\u001b[0m\n",
      "[\u001b[0;36mDEBUG\u001b[0m | src.tasks.clm_training.fabric.base]: \u001b[0;36miter_num value: 114, type: <class 'int'>\u001b[0m\n",
      "2025-06-16 13:55:14 - src.tasks.clm_training.fabric.base - \u001b[0;36mDEBUG\u001b[0m - \u001b[0;36mAbout to check: (iter_num + 1) % gradient_accumulation_steps != 0 -> (114 + 1) % 2 != 0\u001b[0m\n",
      "[\u001b[0;36mDEBUG\u001b[0m | src.tasks.clm_training.fabric.base]: \u001b[0;36mAbout to check: (iter_num + 1) % gradient_accumulation_steps != 0 -> (114 + 1) % 2 != 0\u001b[0m\n",
      "2025-06-16 13:55:14 - src.tasks.clm_training.fabric.base - \u001b[0;36mDEBUG\u001b[0m - \u001b[0;36miter 115 step 57: loss 5.9062, iter time: 121.90ms remaining time: \u001b[0m\n",
      "[\u001b[0;36mDEBUG\u001b[0m | src.tasks.clm_training.fabric.base]: \u001b[0;36miter 115 step 57: loss 5.9062, iter time: 121.90ms remaining time: \u001b[0m\n",
      "  2%|\u001b[34m▉                                       \u001b[0m| 115/4682 [00:48<32:05,  2.37it/s]\u001b[0m2025-06-16 13:55:14 - src.tasks.clm_training.fabric.base - \u001b[0;36mDEBUG\u001b[0m - \u001b[0;36mgradient_accumulation_steps raw value: 2, type: <class 'int'>\u001b[0m\n",
      "[\u001b[0;36mDEBUG\u001b[0m | src.tasks.clm_training.fabric.base]: \u001b[0;36mgradient_accumulation_steps raw value: 2, type: <class 'int'>\u001b[0m\n",
      "2025-06-16 13:55:14 - src.tasks.clm_training.fabric.base - \u001b[0;36mDEBUG\u001b[0m - \u001b[0;36mgradient_accumulation_steps converted: 2, type: <class 'int'>\u001b[0m\n",
      "[\u001b[0;36mDEBUG\u001b[0m | src.tasks.clm_training.fabric.base]: \u001b[0;36mgradient_accumulation_steps converted: 2, type: <class 'int'>\u001b[0m\n",
      "2025-06-16 13:55:14 - src.tasks.clm_training.fabric.base - \u001b[0;36mDEBUG\u001b[0m - \u001b[0;36miter_num value: 115, type: <class 'int'>\u001b[0m\n",
      "[\u001b[0;36mDEBUG\u001b[0m | src.tasks.clm_training.fabric.base]: \u001b[0;36miter_num value: 115, type: <class 'int'>\u001b[0m\n",
      "2025-06-16 13:55:14 - src.tasks.clm_training.fabric.base - \u001b[0;36mDEBUG\u001b[0m - \u001b[0;36mAbout to check: (iter_num + 1) % gradient_accumulation_steps != 0 -> (115 + 1) % 2 != 0\u001b[0m\n",
      "[\u001b[0;36mDEBUG\u001b[0m | src.tasks.clm_training.fabric.base]: \u001b[0;36mAbout to check: (iter_num + 1) % gradient_accumulation_steps != 0 -> (115 + 1) % 2 != 0\u001b[0m\n",
      "2025-06-16 13:55:15 - src.tasks.clm_training.fabric.base - \u001b[0;36mDEBUG\u001b[0m - \u001b[0;36mGradient norm before clipping: 25.6250\u001b[0m\n",
      "[\u001b[0;36mDEBUG\u001b[0m | src.tasks.clm_training.fabric.base]: \u001b[0;36mGradient norm before clipping: 25.6250\u001b[0m\n",
      "2025-06-16 13:55:15 - src.tasks.clm_training.fabric.base - \u001b[0;36mDEBUG\u001b[0m - \u001b[0;36mvalidate_after_k_steps value: 1000, type: <class 'int'>\u001b[0m\n",
      "[\u001b[0;36mDEBUG\u001b[0m | src.tasks.clm_training.fabric.base]: \u001b[0;36mvalidate_after_k_steps value: 1000, type: <class 'int'>\u001b[0m\n",
      "2025-06-16 13:55:15 - src.tasks.clm_training.fabric.base - \u001b[0;36mDEBUG\u001b[0m - \u001b[0;36mstep_count value: 58, type: <class 'int'>\u001b[0m\n",
      "[\u001b[0;36mDEBUG\u001b[0m | src.tasks.clm_training.fabric.base]: \u001b[0;36mstep_count value: 58, type: <class 'int'>\u001b[0m\n",
      "2025-06-16 13:55:15 - src.tasks.clm_training.fabric.base - \u001b[0;36mDEBUG\u001b[0m - \u001b[0;36mValidation condition check - epochFinished: False, trainingFinished: False\u001b[0m\n",
      "[\u001b[0;36mDEBUG\u001b[0m | src.tasks.clm_training.fabric.base]: \u001b[0;36mValidation condition check - epochFinished: False, trainingFinished: False\u001b[0m\n",
      "2025-06-16 13:55:15 - src.tasks.clm_training.fabric.base - \u001b[0;36mDEBUG\u001b[0m - \u001b[0;36mAbout to check: step_count % validate_after_k_steps == 0 -> 58 % 1000 == 0\u001b[0m\n",
      "[\u001b[0;36mDEBUG\u001b[0m | src.tasks.clm_training.fabric.base]: \u001b[0;36mAbout to check: step_count % validate_after_k_steps == 0 -> 58 % 1000 == 0\u001b[0m\n",
      "2025-06-16 13:55:15 - src.tasks.clm_training.fabric.base - \u001b[0;36mDEBUG\u001b[0m - \u001b[0;36miter 116 step 58: loss 6.3438, iter time: 419.84ms remaining time: \u001b[0m\n",
      "[\u001b[0;36mDEBUG\u001b[0m | src.tasks.clm_training.fabric.base]: \u001b[0;36miter 116 step 58: loss 6.3438, iter time: 419.84ms remaining time: \u001b[0m\n",
      "  2%|\u001b[34m▉                                       \u001b[0m| 116/4682 [00:49<32:16,  2.36it/s]\u001b[0m2025-06-16 13:55:15 - src.tasks.clm_training.fabric.base - \u001b[0;36mDEBUG\u001b[0m - \u001b[0;36mgradient_accumulation_steps raw value: 2, type: <class 'int'>\u001b[0m\n",
      "[\u001b[0;36mDEBUG\u001b[0m | src.tasks.clm_training.fabric.base]: \u001b[0;36mgradient_accumulation_steps raw value: 2, type: <class 'int'>\u001b[0m\n",
      "2025-06-16 13:55:15 - src.tasks.clm_training.fabric.base - \u001b[0;36mDEBUG\u001b[0m - \u001b[0;36mgradient_accumulation_steps converted: 2, type: <class 'int'>\u001b[0m\n",
      "[\u001b[0;36mDEBUG\u001b[0m | src.tasks.clm_training.fabric.base]: \u001b[0;36mgradient_accumulation_steps converted: 2, type: <class 'int'>\u001b[0m\n",
      "2025-06-16 13:55:15 - src.tasks.clm_training.fabric.base - \u001b[0;36mDEBUG\u001b[0m - \u001b[0;36miter_num value: 116, type: <class 'int'>\u001b[0m\n",
      "[\u001b[0;36mDEBUG\u001b[0m | src.tasks.clm_training.fabric.base]: \u001b[0;36miter_num value: 116, type: <class 'int'>\u001b[0m\n",
      "2025-06-16 13:55:15 - src.tasks.clm_training.fabric.base - \u001b[0;36mDEBUG\u001b[0m - \u001b[0;36mAbout to check: (iter_num + 1) % gradient_accumulation_steps != 0 -> (116 + 1) % 2 != 0\u001b[0m\n",
      "[\u001b[0;36mDEBUG\u001b[0m | src.tasks.clm_training.fabric.base]: \u001b[0;36mAbout to check: (iter_num + 1) % gradient_accumulation_steps != 0 -> (116 + 1) % 2 != 0\u001b[0m\n",
      "2025-06-16 13:55:15 - src.tasks.clm_training.fabric.base - \u001b[0;36mDEBUG\u001b[0m - \u001b[0;36miter 117 step 58: loss 5.7188, iter time: 122.17ms remaining time: \u001b[0m\n",
      "[\u001b[0;36mDEBUG\u001b[0m | src.tasks.clm_training.fabric.base]: \u001b[0;36miter 117 step 58: loss 5.7188, iter time: 122.17ms remaining time: \u001b[0m\n",
      "  2%|\u001b[34m▉                                       \u001b[0m| 117/4682 [00:49<32:03,  2.37it/s]\u001b[0m2025-06-16 13:55:15 - src.tasks.clm_training.fabric.base - \u001b[0;36mDEBUG\u001b[0m - \u001b[0;36mgradient_accumulation_steps raw value: 2, type: <class 'int'>\u001b[0m\n",
      "[\u001b[0;36mDEBUG\u001b[0m | src.tasks.clm_training.fabric.base]: \u001b[0;36mgradient_accumulation_steps raw value: 2, type: <class 'int'>\u001b[0m\n",
      "2025-06-16 13:55:15 - src.tasks.clm_training.fabric.base - \u001b[0;36mDEBUG\u001b[0m - \u001b[0;36mgradient_accumulation_steps converted: 2, type: <class 'int'>\u001b[0m\n",
      "[\u001b[0;36mDEBUG\u001b[0m | src.tasks.clm_training.fabric.base]: \u001b[0;36mgradient_accumulation_steps converted: 2, type: <class 'int'>\u001b[0m\n",
      "2025-06-16 13:55:15 - src.tasks.clm_training.fabric.base - \u001b[0;36mDEBUG\u001b[0m - \u001b[0;36miter_num value: 117, type: <class 'int'>\u001b[0m\n",
      "[\u001b[0;36mDEBUG\u001b[0m | src.tasks.clm_training.fabric.base]: \u001b[0;36miter_num value: 117, type: <class 'int'>\u001b[0m\n",
      "2025-06-16 13:55:15 - src.tasks.clm_training.fabric.base - \u001b[0;36mDEBUG\u001b[0m - \u001b[0;36mAbout to check: (iter_num + 1) % gradient_accumulation_steps != 0 -> (117 + 1) % 2 != 0\u001b[0m\n",
      "[\u001b[0;36mDEBUG\u001b[0m | src.tasks.clm_training.fabric.base]: \u001b[0;36mAbout to check: (iter_num + 1) % gradient_accumulation_steps != 0 -> (117 + 1) % 2 != 0\u001b[0m\n",
      "2025-06-16 13:55:16 - src.tasks.clm_training.fabric.base - \u001b[0;36mDEBUG\u001b[0m - \u001b[0;36mGradient norm before clipping: 22.5000\u001b[0m\n",
      "[\u001b[0;36mDEBUG\u001b[0m | src.tasks.clm_training.fabric.base]: \u001b[0;36mGradient norm before clipping: 22.5000\u001b[0m\n",
      "2025-06-16 13:55:16 - src.tasks.clm_training.fabric.base - \u001b[0;36mDEBUG\u001b[0m - \u001b[0;36mvalidate_after_k_steps value: 1000, type: <class 'int'>\u001b[0m\n",
      "[\u001b[0;36mDEBUG\u001b[0m | src.tasks.clm_training.fabric.base]: \u001b[0;36mvalidate_after_k_steps value: 1000, type: <class 'int'>\u001b[0m\n",
      "2025-06-16 13:55:16 - src.tasks.clm_training.fabric.base - \u001b[0;36mDEBUG\u001b[0m - \u001b[0;36mstep_count value: 59, type: <class 'int'>\u001b[0m\n",
      "[\u001b[0;36mDEBUG\u001b[0m | src.tasks.clm_training.fabric.base]: \u001b[0;36mstep_count value: 59, type: <class 'int'>\u001b[0m\n",
      "2025-06-16 13:55:16 - src.tasks.clm_training.fabric.base - \u001b[0;36mDEBUG\u001b[0m - \u001b[0;36mValidation condition check - epochFinished: False, trainingFinished: False\u001b[0m\n",
      "[\u001b[0;36mDEBUG\u001b[0m | src.tasks.clm_training.fabric.base]: \u001b[0;36mValidation condition check - epochFinished: False, trainingFinished: False\u001b[0m\n",
      "2025-06-16 13:55:16 - src.tasks.clm_training.fabric.base - \u001b[0;36mDEBUG\u001b[0m - \u001b[0;36mAbout to check: step_count % validate_after_k_steps == 0 -> 59 % 1000 == 0\u001b[0m\n",
      "[\u001b[0;36mDEBUG\u001b[0m | src.tasks.clm_training.fabric.base]: \u001b[0;36mAbout to check: step_count % validate_after_k_steps == 0 -> 59 % 1000 == 0\u001b[0m\n",
      "2025-06-16 13:55:16 - src.tasks.clm_training.fabric.base - \u001b[0;36mDEBUG\u001b[0m - \u001b[0;36miter 118 step 59: loss 5.8750, iter time: 421.09ms remaining time: \u001b[0m\n",
      "[\u001b[0;36mDEBUG\u001b[0m | src.tasks.clm_training.fabric.base]: \u001b[0;36miter 118 step 59: loss 5.8750, iter time: 421.09ms remaining time: \u001b[0m\n",
      "  3%|\u001b[34m█                                       \u001b[0m| 118/4682 [00:50<32:16,  2.36it/s]\u001b[0m2025-06-16 13:55:16 - src.tasks.clm_training.fabric.base - \u001b[0;36mDEBUG\u001b[0m - \u001b[0;36mgradient_accumulation_steps raw value: 2, type: <class 'int'>\u001b[0m\n",
      "[\u001b[0;36mDEBUG\u001b[0m | src.tasks.clm_training.fabric.base]: \u001b[0;36mgradient_accumulation_steps raw value: 2, type: <class 'int'>\u001b[0m\n",
      "2025-06-16 13:55:16 - src.tasks.clm_training.fabric.base - \u001b[0;36mDEBUG\u001b[0m - \u001b[0;36mgradient_accumulation_steps converted: 2, type: <class 'int'>\u001b[0m\n",
      "[\u001b[0;36mDEBUG\u001b[0m | src.tasks.clm_training.fabric.base]: \u001b[0;36mgradient_accumulation_steps converted: 2, type: <class 'int'>\u001b[0m\n",
      "2025-06-16 13:55:16 - src.tasks.clm_training.fabric.base - \u001b[0;36mDEBUG\u001b[0m - \u001b[0;36miter_num value: 118, type: <class 'int'>\u001b[0m\n",
      "[\u001b[0;36mDEBUG\u001b[0m | src.tasks.clm_training.fabric.base]: \u001b[0;36miter_num value: 118, type: <class 'int'>\u001b[0m\n",
      "2025-06-16 13:55:16 - src.tasks.clm_training.fabric.base - \u001b[0;36mDEBUG\u001b[0m - \u001b[0;36mAbout to check: (iter_num + 1) % gradient_accumulation_steps != 0 -> (118 + 1) % 2 != 0\u001b[0m\n",
      "[\u001b[0;36mDEBUG\u001b[0m | src.tasks.clm_training.fabric.base]: \u001b[0;36mAbout to check: (iter_num + 1) % gradient_accumulation_steps != 0 -> (118 + 1) % 2 != 0\u001b[0m\n",
      "2025-06-16 13:55:16 - src.tasks.clm_training.fabric.base - \u001b[0;36mDEBUG\u001b[0m - \u001b[0;36miter 119 step 59: loss 6.0938, iter time: 122.77ms remaining time: \u001b[0m\n",
      "[\u001b[0;36mDEBUG\u001b[0m | src.tasks.clm_training.fabric.base]: \u001b[0;36miter 119 step 59: loss 6.0938, iter time: 122.77ms remaining time: \u001b[0m\n",
      "  3%|\u001b[34m█                                       \u001b[0m| 119/4682 [00:50<32:04,  2.37it/s]\u001b[0m2025-06-16 13:55:16 - src.tasks.clm_training.fabric.base - \u001b[0;36mDEBUG\u001b[0m - \u001b[0;36mgradient_accumulation_steps raw value: 2, type: <class 'int'>\u001b[0m\n",
      "[\u001b[0;36mDEBUG\u001b[0m | src.tasks.clm_training.fabric.base]: \u001b[0;36mgradient_accumulation_steps raw value: 2, type: <class 'int'>\u001b[0m\n",
      "2025-06-16 13:55:16 - src.tasks.clm_training.fabric.base - \u001b[0;36mDEBUG\u001b[0m - \u001b[0;36mgradient_accumulation_steps converted: 2, type: <class 'int'>\u001b[0m\n",
      "[\u001b[0;36mDEBUG\u001b[0m | src.tasks.clm_training.fabric.base]: \u001b[0;36mgradient_accumulation_steps converted: 2, type: <class 'int'>\u001b[0m\n",
      "2025-06-16 13:55:16 - src.tasks.clm_training.fabric.base - \u001b[0;36mDEBUG\u001b[0m - \u001b[0;36miter_num value: 119, type: <class 'int'>\u001b[0m\n",
      "[\u001b[0;36mDEBUG\u001b[0m | src.tasks.clm_training.fabric.base]: \u001b[0;36miter_num value: 119, type: <class 'int'>\u001b[0m\n",
      "2025-06-16 13:55:16 - src.tasks.clm_training.fabric.base - \u001b[0;36mDEBUG\u001b[0m - \u001b[0;36mAbout to check: (iter_num + 1) % gradient_accumulation_steps != 0 -> (119 + 1) % 2 != 0\u001b[0m\n",
      "[\u001b[0;36mDEBUG\u001b[0m | src.tasks.clm_training.fabric.base]: \u001b[0;36mAbout to check: (iter_num + 1) % gradient_accumulation_steps != 0 -> (119 + 1) % 2 != 0\u001b[0m\n",
      "2025-06-16 13:55:17 - src.tasks.clm_training.fabric.base - \u001b[0;36mDEBUG\u001b[0m - \u001b[0;36mGradient norm before clipping: 24.6250\u001b[0m\n",
      "[\u001b[0;36mDEBUG\u001b[0m | src.tasks.clm_training.fabric.base]: \u001b[0;36mGradient norm before clipping: 24.6250\u001b[0m\n",
      "2025-06-16 13:55:17 - src.tasks.clm_training.fabric.base - \u001b[0;36mDEBUG\u001b[0m - \u001b[0;36mvalidate_after_k_steps value: 1000, type: <class 'int'>\u001b[0m\n",
      "[\u001b[0;36mDEBUG\u001b[0m | src.tasks.clm_training.fabric.base]: \u001b[0;36mvalidate_after_k_steps value: 1000, type: <class 'int'>\u001b[0m\n",
      "2025-06-16 13:55:17 - src.tasks.clm_training.fabric.base - \u001b[0;36mDEBUG\u001b[0m - \u001b[0;36mstep_count value: 60, type: <class 'int'>\u001b[0m\n",
      "[\u001b[0;36mDEBUG\u001b[0m | src.tasks.clm_training.fabric.base]: \u001b[0;36mstep_count value: 60, type: <class 'int'>\u001b[0m\n",
      "2025-06-16 13:55:17 - src.tasks.clm_training.fabric.base - \u001b[0;36mDEBUG\u001b[0m - \u001b[0;36mValidation condition check - epochFinished: False, trainingFinished: False\u001b[0m\n",
      "[\u001b[0;36mDEBUG\u001b[0m | src.tasks.clm_training.fabric.base]: \u001b[0;36mValidation condition check - epochFinished: False, trainingFinished: False\u001b[0m\n",
      "2025-06-16 13:55:17 - src.tasks.clm_training.fabric.base - \u001b[0;36mDEBUG\u001b[0m - \u001b[0;36mAbout to check: step_count % validate_after_k_steps == 0 -> 60 % 1000 == 0\u001b[0m\n",
      "[\u001b[0;36mDEBUG\u001b[0m | src.tasks.clm_training.fabric.base]: \u001b[0;36mAbout to check: step_count % validate_after_k_steps == 0 -> 60 % 1000 == 0\u001b[0m\n",
      "2025-06-16 13:55:17 - src.tasks.clm_training.fabric.base - \u001b[0;36mDEBUG\u001b[0m - \u001b[0;36miter 120 step 60: loss 5.6562, iter time: 419.86ms remaining time: \u001b[0m\n",
      "[\u001b[0;36mDEBUG\u001b[0m | src.tasks.clm_training.fabric.base]: \u001b[0;36miter 120 step 60: loss 5.6562, iter time: 419.86ms remaining time: \u001b[0m\n",
      "  3%|\u001b[34m█                                       \u001b[0m| 120/4682 [00:51<32:14,  2.36it/s]\u001b[0m2025-06-16 13:55:17 - src.tasks.clm_training.fabric.base - \u001b[0;36mDEBUG\u001b[0m - \u001b[0;36mgradient_accumulation_steps raw value: 2, type: <class 'int'>\u001b[0m\n",
      "[\u001b[0;36mDEBUG\u001b[0m | src.tasks.clm_training.fabric.base]: \u001b[0;36mgradient_accumulation_steps raw value: 2, type: <class 'int'>\u001b[0m\n",
      "2025-06-16 13:55:17 - src.tasks.clm_training.fabric.base - \u001b[0;36mDEBUG\u001b[0m - \u001b[0;36mgradient_accumulation_steps converted: 2, type: <class 'int'>\u001b[0m\n",
      "[\u001b[0;36mDEBUG\u001b[0m | src.tasks.clm_training.fabric.base]: \u001b[0;36mgradient_accumulation_steps converted: 2, type: <class 'int'>\u001b[0m\n",
      "2025-06-16 13:55:17 - src.tasks.clm_training.fabric.base - \u001b[0;36mDEBUG\u001b[0m - \u001b[0;36miter_num value: 120, type: <class 'int'>\u001b[0m\n",
      "[\u001b[0;36mDEBUG\u001b[0m | src.tasks.clm_training.fabric.base]: \u001b[0;36miter_num value: 120, type: <class 'int'>\u001b[0m\n",
      "2025-06-16 13:55:17 - src.tasks.clm_training.fabric.base - \u001b[0;36mDEBUG\u001b[0m - \u001b[0;36mAbout to check: (iter_num + 1) % gradient_accumulation_steps != 0 -> (120 + 1) % 2 != 0\u001b[0m\n",
      "[\u001b[0;36mDEBUG\u001b[0m | src.tasks.clm_training.fabric.base]: \u001b[0;36mAbout to check: (iter_num + 1) % gradient_accumulation_steps != 0 -> (120 + 1) % 2 != 0\u001b[0m\n",
      "2025-06-16 13:55:17 - src.tasks.clm_training.fabric.base - \u001b[0;36mDEBUG\u001b[0m - \u001b[0;36miter 121 step 60: loss 5.2812, iter time: 122.00ms remaining time: \u001b[0m\n",
      "[\u001b[0;36mDEBUG\u001b[0m | src.tasks.clm_training.fabric.base]: \u001b[0;36miter 121 step 60: loss 5.2812, iter time: 122.00ms remaining time: \u001b[0m\n",
      "  3%|\u001b[34m█                                       \u001b[0m| 121/4682 [00:51<32:02,  2.37it/s]\u001b[0m2025-06-16 13:55:17 - src.tasks.clm_training.fabric.base - \u001b[0;36mDEBUG\u001b[0m - \u001b[0;36mgradient_accumulation_steps raw value: 2, type: <class 'int'>\u001b[0m\n",
      "[\u001b[0;36mDEBUG\u001b[0m | src.tasks.clm_training.fabric.base]: \u001b[0;36mgradient_accumulation_steps raw value: 2, type: <class 'int'>\u001b[0m\n",
      "2025-06-16 13:55:17 - src.tasks.clm_training.fabric.base - \u001b[0;36mDEBUG\u001b[0m - \u001b[0;36mgradient_accumulation_steps converted: 2, type: <class 'int'>\u001b[0m\n",
      "[\u001b[0;36mDEBUG\u001b[0m | src.tasks.clm_training.fabric.base]: \u001b[0;36mgradient_accumulation_steps converted: 2, type: <class 'int'>\u001b[0m\n",
      "2025-06-16 13:55:17 - src.tasks.clm_training.fabric.base - \u001b[0;36mDEBUG\u001b[0m - \u001b[0;36miter_num value: 121, type: <class 'int'>\u001b[0m\n",
      "[\u001b[0;36mDEBUG\u001b[0m | src.tasks.clm_training.fabric.base]: \u001b[0;36miter_num value: 121, type: <class 'int'>\u001b[0m\n",
      "2025-06-16 13:55:17 - src.tasks.clm_training.fabric.base - \u001b[0;36mDEBUG\u001b[0m - \u001b[0;36mAbout to check: (iter_num + 1) % gradient_accumulation_steps != 0 -> (121 + 1) % 2 != 0\u001b[0m\n",
      "[\u001b[0;36mDEBUG\u001b[0m | src.tasks.clm_training.fabric.base]: \u001b[0;36mAbout to check: (iter_num + 1) % gradient_accumulation_steps != 0 -> (121 + 1) % 2 != 0\u001b[0m\n",
      "2025-06-16 13:55:17 - src.tasks.clm_training.fabric.base - \u001b[0;36mDEBUG\u001b[0m - \u001b[0;36mGradient norm before clipping: 27.1250\u001b[0m\n",
      "[\u001b[0;36mDEBUG\u001b[0m | src.tasks.clm_training.fabric.base]: \u001b[0;36mGradient norm before clipping: 27.1250\u001b[0m\n",
      "2025-06-16 13:55:17 - src.tasks.clm_training.fabric.base - \u001b[0;36mDEBUG\u001b[0m - \u001b[0;36mvalidate_after_k_steps value: 1000, type: <class 'int'>\u001b[0m\n",
      "[\u001b[0;36mDEBUG\u001b[0m | src.tasks.clm_training.fabric.base]: \u001b[0;36mvalidate_after_k_steps value: 1000, type: <class 'int'>\u001b[0m\n",
      "2025-06-16 13:55:17 - src.tasks.clm_training.fabric.base - \u001b[0;36mDEBUG\u001b[0m - \u001b[0;36mstep_count value: 61, type: <class 'int'>\u001b[0m\n",
      "[\u001b[0;36mDEBUG\u001b[0m | src.tasks.clm_training.fabric.base]: \u001b[0;36mstep_count value: 61, type: <class 'int'>\u001b[0m\n",
      "2025-06-16 13:55:17 - src.tasks.clm_training.fabric.base - \u001b[0;36mDEBUG\u001b[0m - \u001b[0;36mValidation condition check - epochFinished: False, trainingFinished: False\u001b[0m\n",
      "[\u001b[0;36mDEBUG\u001b[0m | src.tasks.clm_training.fabric.base]: \u001b[0;36mValidation condition check - epochFinished: False, trainingFinished: False\u001b[0m\n",
      "2025-06-16 13:55:17 - src.tasks.clm_training.fabric.base - \u001b[0;36mDEBUG\u001b[0m - \u001b[0;36mAbout to check: step_count % validate_after_k_steps == 0 -> 61 % 1000 == 0\u001b[0m\n",
      "[\u001b[0;36mDEBUG\u001b[0m | src.tasks.clm_training.fabric.base]: \u001b[0;36mAbout to check: step_count % validate_after_k_steps == 0 -> 61 % 1000 == 0\u001b[0m\n",
      "2025-06-16 13:55:17 - src.tasks.clm_training.fabric.base - \u001b[0;36mDEBUG\u001b[0m - \u001b[0;36miter 122 step 61: loss 6.1562, iter time: 420.47ms remaining time: \u001b[0m\n",
      "[\u001b[0;36mDEBUG\u001b[0m | src.tasks.clm_training.fabric.base]: \u001b[0;36miter 122 step 61: loss 6.1562, iter time: 420.47ms remaining time: \u001b[0m\n",
      "  3%|\u001b[34m█                                       \u001b[0m| 122/4682 [00:51<32:14,  2.36it/s]\u001b[0m2025-06-16 13:55:17 - src.tasks.clm_training.fabric.base - \u001b[0;36mDEBUG\u001b[0m - \u001b[0;36mgradient_accumulation_steps raw value: 2, type: <class 'int'>\u001b[0m\n",
      "[\u001b[0;36mDEBUG\u001b[0m | src.tasks.clm_training.fabric.base]: \u001b[0;36mgradient_accumulation_steps raw value: 2, type: <class 'int'>\u001b[0m\n",
      "2025-06-16 13:55:17 - src.tasks.clm_training.fabric.base - \u001b[0;36mDEBUG\u001b[0m - \u001b[0;36mgradient_accumulation_steps converted: 2, type: <class 'int'>\u001b[0m\n",
      "[\u001b[0;36mDEBUG\u001b[0m | src.tasks.clm_training.fabric.base]: \u001b[0;36mgradient_accumulation_steps converted: 2, type: <class 'int'>\u001b[0m\n",
      "2025-06-16 13:55:17 - src.tasks.clm_training.fabric.base - \u001b[0;36mDEBUG\u001b[0m - \u001b[0;36miter_num value: 122, type: <class 'int'>\u001b[0m\n",
      "[\u001b[0;36mDEBUG\u001b[0m | src.tasks.clm_training.fabric.base]: \u001b[0;36miter_num value: 122, type: <class 'int'>\u001b[0m\n",
      "2025-06-16 13:55:17 - src.tasks.clm_training.fabric.base - \u001b[0;36mDEBUG\u001b[0m - \u001b[0;36mAbout to check: (iter_num + 1) % gradient_accumulation_steps != 0 -> (122 + 1) % 2 != 0\u001b[0m\n",
      "[\u001b[0;36mDEBUG\u001b[0m | src.tasks.clm_training.fabric.base]: \u001b[0;36mAbout to check: (iter_num + 1) % gradient_accumulation_steps != 0 -> (122 + 1) % 2 != 0\u001b[0m\n",
      "2025-06-16 13:55:18 - src.tasks.clm_training.fabric.base - \u001b[0;36mDEBUG\u001b[0m - \u001b[0;36miter 123 step 61: loss 5.4375, iter time: 122.58ms remaining time: \u001b[0m\n",
      "[\u001b[0;36mDEBUG\u001b[0m | src.tasks.clm_training.fabric.base]: \u001b[0;36miter 123 step 61: loss 5.4375, iter time: 122.58ms remaining time: \u001b[0m\n",
      "  3%|\u001b[34m█                                       \u001b[0m| 123/4682 [00:52<32:03,  2.37it/s]\u001b[0m2025-06-16 13:55:18 - src.tasks.clm_training.fabric.base - \u001b[0;36mDEBUG\u001b[0m - \u001b[0;36mgradient_accumulation_steps raw value: 2, type: <class 'int'>\u001b[0m\n",
      "[\u001b[0;36mDEBUG\u001b[0m | src.tasks.clm_training.fabric.base]: \u001b[0;36mgradient_accumulation_steps raw value: 2, type: <class 'int'>\u001b[0m\n",
      "2025-06-16 13:55:18 - src.tasks.clm_training.fabric.base - \u001b[0;36mDEBUG\u001b[0m - \u001b[0;36mgradient_accumulation_steps converted: 2, type: <class 'int'>\u001b[0m\n",
      "[\u001b[0;36mDEBUG\u001b[0m | src.tasks.clm_training.fabric.base]: \u001b[0;36mgradient_accumulation_steps converted: 2, type: <class 'int'>\u001b[0m\n",
      "2025-06-16 13:55:18 - src.tasks.clm_training.fabric.base - \u001b[0;36mDEBUG\u001b[0m - \u001b[0;36miter_num value: 123, type: <class 'int'>\u001b[0m\n",
      "[\u001b[0;36mDEBUG\u001b[0m | src.tasks.clm_training.fabric.base]: \u001b[0;36miter_num value: 123, type: <class 'int'>\u001b[0m\n",
      "2025-06-16 13:55:18 - src.tasks.clm_training.fabric.base - \u001b[0;36mDEBUG\u001b[0m - \u001b[0;36mAbout to check: (iter_num + 1) % gradient_accumulation_steps != 0 -> (123 + 1) % 2 != 0\u001b[0m\n",
      "[\u001b[0;36mDEBUG\u001b[0m | src.tasks.clm_training.fabric.base]: \u001b[0;36mAbout to check: (iter_num + 1) % gradient_accumulation_steps != 0 -> (123 + 1) % 2 != 0\u001b[0m\n",
      "2025-06-16 13:55:18 - src.tasks.clm_training.fabric.base - \u001b[0;36mDEBUG\u001b[0m - \u001b[0;36mGradient norm before clipping: 24.2500\u001b[0m\n",
      "[\u001b[0;36mDEBUG\u001b[0m | src.tasks.clm_training.fabric.base]: \u001b[0;36mGradient norm before clipping: 24.2500\u001b[0m\n",
      "2025-06-16 13:55:18 - src.tasks.clm_training.fabric.base - \u001b[0;36mDEBUG\u001b[0m - \u001b[0;36mvalidate_after_k_steps value: 1000, type: <class 'int'>\u001b[0m\n",
      "[\u001b[0;36mDEBUG\u001b[0m | src.tasks.clm_training.fabric.base]: \u001b[0;36mvalidate_after_k_steps value: 1000, type: <class 'int'>\u001b[0m\n",
      "2025-06-16 13:55:18 - src.tasks.clm_training.fabric.base - \u001b[0;36mDEBUG\u001b[0m - \u001b[0;36mstep_count value: 62, type: <class 'int'>\u001b[0m\n",
      "[\u001b[0;36mDEBUG\u001b[0m | src.tasks.clm_training.fabric.base]: \u001b[0;36mstep_count value: 62, type: <class 'int'>\u001b[0m\n",
      "2025-06-16 13:55:18 - src.tasks.clm_training.fabric.base - \u001b[0;36mDEBUG\u001b[0m - \u001b[0;36mValidation condition check - epochFinished: False, trainingFinished: False\u001b[0m\n",
      "[\u001b[0;36mDEBUG\u001b[0m | src.tasks.clm_training.fabric.base]: \u001b[0;36mValidation condition check - epochFinished: False, trainingFinished: False\u001b[0m\n",
      "2025-06-16 13:55:18 - src.tasks.clm_training.fabric.base - \u001b[0;36mDEBUG\u001b[0m - \u001b[0;36mAbout to check: step_count % validate_after_k_steps == 0 -> 62 % 1000 == 0\u001b[0m\n",
      "[\u001b[0;36mDEBUG\u001b[0m | src.tasks.clm_training.fabric.base]: \u001b[0;36mAbout to check: step_count % validate_after_k_steps == 0 -> 62 % 1000 == 0\u001b[0m\n",
      "2025-06-16 13:55:18 - src.tasks.clm_training.fabric.base - \u001b[0;36mDEBUG\u001b[0m - \u001b[0;36miter 124 step 62: loss 5.9688, iter time: 419.88ms remaining time: \u001b[0m\n",
      "[\u001b[0;36mDEBUG\u001b[0m | src.tasks.clm_training.fabric.base]: \u001b[0;36miter 124 step 62: loss 5.9688, iter time: 419.88ms remaining time: \u001b[0m\n",
      "  3%|\u001b[34m█                                       \u001b[0m| 124/4682 [00:52<32:13,  2.36it/s]\u001b[0m2025-06-16 13:55:18 - src.tasks.clm_training.fabric.base - \u001b[0;36mDEBUG\u001b[0m - \u001b[0;36mgradient_accumulation_steps raw value: 2, type: <class 'int'>\u001b[0m\n",
      "[\u001b[0;36mDEBUG\u001b[0m | src.tasks.clm_training.fabric.base]: \u001b[0;36mgradient_accumulation_steps raw value: 2, type: <class 'int'>\u001b[0m\n",
      "2025-06-16 13:55:18 - src.tasks.clm_training.fabric.base - \u001b[0;36mDEBUG\u001b[0m - \u001b[0;36mgradient_accumulation_steps converted: 2, type: <class 'int'>\u001b[0m\n",
      "[\u001b[0;36mDEBUG\u001b[0m | src.tasks.clm_training.fabric.base]: \u001b[0;36mgradient_accumulation_steps converted: 2, type: <class 'int'>\u001b[0m\n",
      "2025-06-16 13:55:18 - src.tasks.clm_training.fabric.base - \u001b[0;36mDEBUG\u001b[0m - \u001b[0;36miter_num value: 124, type: <class 'int'>\u001b[0m\n",
      "[\u001b[0;36mDEBUG\u001b[0m | src.tasks.clm_training.fabric.base]: \u001b[0;36miter_num value: 124, type: <class 'int'>\u001b[0m\n",
      "2025-06-16 13:55:18 - src.tasks.clm_training.fabric.base - \u001b[0;36mDEBUG\u001b[0m - \u001b[0;36mAbout to check: (iter_num + 1) % gradient_accumulation_steps != 0 -> (124 + 1) % 2 != 0\u001b[0m\n",
      "[\u001b[0;36mDEBUG\u001b[0m | src.tasks.clm_training.fabric.base]: \u001b[0;36mAbout to check: (iter_num + 1) % gradient_accumulation_steps != 0 -> (124 + 1) % 2 != 0\u001b[0m\n",
      "2025-06-16 13:55:19 - src.tasks.clm_training.fabric.base - \u001b[0;36mDEBUG\u001b[0m - \u001b[0;36miter 125 step 62: loss 5.8125, iter time: 122.08ms remaining time: \u001b[0m\n",
      "[\u001b[0;36mDEBUG\u001b[0m | src.tasks.clm_training.fabric.base]: \u001b[0;36miter 125 step 62: loss 5.8125, iter time: 122.08ms remaining time: \u001b[0m\n",
      "  3%|\u001b[34m█                                       \u001b[0m| 125/4682 [00:53<32:02,  2.37it/s]\u001b[0m2025-06-16 13:55:19 - src.tasks.clm_training.fabric.base - \u001b[0;36mDEBUG\u001b[0m - \u001b[0;36mgradient_accumulation_steps raw value: 2, type: <class 'int'>\u001b[0m\n",
      "[\u001b[0;36mDEBUG\u001b[0m | src.tasks.clm_training.fabric.base]: \u001b[0;36mgradient_accumulation_steps raw value: 2, type: <class 'int'>\u001b[0m\n",
      "2025-06-16 13:55:19 - src.tasks.clm_training.fabric.base - \u001b[0;36mDEBUG\u001b[0m - \u001b[0;36mgradient_accumulation_steps converted: 2, type: <class 'int'>\u001b[0m\n",
      "[\u001b[0;36mDEBUG\u001b[0m | src.tasks.clm_training.fabric.base]: \u001b[0;36mgradient_accumulation_steps converted: 2, type: <class 'int'>\u001b[0m\n",
      "2025-06-16 13:55:19 - src.tasks.clm_training.fabric.base - \u001b[0;36mDEBUG\u001b[0m - \u001b[0;36miter_num value: 125, type: <class 'int'>\u001b[0m\n",
      "[\u001b[0;36mDEBUG\u001b[0m | src.tasks.clm_training.fabric.base]: \u001b[0;36miter_num value: 125, type: <class 'int'>\u001b[0m\n",
      "2025-06-16 13:55:19 - src.tasks.clm_training.fabric.base - \u001b[0;36mDEBUG\u001b[0m - \u001b[0;36mAbout to check: (iter_num + 1) % gradient_accumulation_steps != 0 -> (125 + 1) % 2 != 0\u001b[0m\n",
      "[\u001b[0;36mDEBUG\u001b[0m | src.tasks.clm_training.fabric.base]: \u001b[0;36mAbout to check: (iter_num + 1) % gradient_accumulation_steps != 0 -> (125 + 1) % 2 != 0\u001b[0m\n",
      "2025-06-16 13:55:19 - src.tasks.clm_training.fabric.base - \u001b[0;36mDEBUG\u001b[0m - \u001b[0;36mGradient norm before clipping: 24.8750\u001b[0m\n",
      "[\u001b[0;36mDEBUG\u001b[0m | src.tasks.clm_training.fabric.base]: \u001b[0;36mGradient norm before clipping: 24.8750\u001b[0m\n",
      "2025-06-16 13:55:19 - src.tasks.clm_training.fabric.base - \u001b[0;36mDEBUG\u001b[0m - \u001b[0;36mvalidate_after_k_steps value: 1000, type: <class 'int'>\u001b[0m\n",
      "[\u001b[0;36mDEBUG\u001b[0m | src.tasks.clm_training.fabric.base]: \u001b[0;36mvalidate_after_k_steps value: 1000, type: <class 'int'>\u001b[0m\n",
      "2025-06-16 13:55:19 - src.tasks.clm_training.fabric.base - \u001b[0;36mDEBUG\u001b[0m - \u001b[0;36mstep_count value: 63, type: <class 'int'>\u001b[0m\n",
      "[\u001b[0;36mDEBUG\u001b[0m | src.tasks.clm_training.fabric.base]: \u001b[0;36mstep_count value: 63, type: <class 'int'>\u001b[0m\n",
      "2025-06-16 13:55:19 - src.tasks.clm_training.fabric.base - \u001b[0;36mDEBUG\u001b[0m - \u001b[0;36mValidation condition check - epochFinished: False, trainingFinished: False\u001b[0m\n",
      "[\u001b[0;36mDEBUG\u001b[0m | src.tasks.clm_training.fabric.base]: \u001b[0;36mValidation condition check - epochFinished: False, trainingFinished: False\u001b[0m\n",
      "2025-06-16 13:55:19 - src.tasks.clm_training.fabric.base - \u001b[0;36mDEBUG\u001b[0m - \u001b[0;36mAbout to check: step_count % validate_after_k_steps == 0 -> 63 % 1000 == 0\u001b[0m\n",
      "[\u001b[0;36mDEBUG\u001b[0m | src.tasks.clm_training.fabric.base]: \u001b[0;36mAbout to check: step_count % validate_after_k_steps == 0 -> 63 % 1000 == 0\u001b[0m\n",
      "2025-06-16 13:55:19 - src.tasks.clm_training.fabric.base - \u001b[0;36mDEBUG\u001b[0m - \u001b[0;36miter 126 step 63: loss 5.8438, iter time: 420.43ms remaining time: \u001b[0m\n",
      "[\u001b[0;36mDEBUG\u001b[0m | src.tasks.clm_training.fabric.base]: \u001b[0;36miter 126 step 63: loss 5.8438, iter time: 420.43ms remaining time: \u001b[0m\n",
      "  3%|\u001b[34m█                                       \u001b[0m| 126/4682 [00:53<32:13,  2.36it/s]\u001b[0m2025-06-16 13:55:19 - src.tasks.clm_training.fabric.base - \u001b[0;36mDEBUG\u001b[0m - \u001b[0;36mgradient_accumulation_steps raw value: 2, type: <class 'int'>\u001b[0m\n",
      "[\u001b[0;36mDEBUG\u001b[0m | src.tasks.clm_training.fabric.base]: \u001b[0;36mgradient_accumulation_steps raw value: 2, type: <class 'int'>\u001b[0m\n",
      "2025-06-16 13:55:19 - src.tasks.clm_training.fabric.base - \u001b[0;36mDEBUG\u001b[0m - \u001b[0;36mgradient_accumulation_steps converted: 2, type: <class 'int'>\u001b[0m\n",
      "[\u001b[0;36mDEBUG\u001b[0m | src.tasks.clm_training.fabric.base]: \u001b[0;36mgradient_accumulation_steps converted: 2, type: <class 'int'>\u001b[0m\n",
      "2025-06-16 13:55:19 - src.tasks.clm_training.fabric.base - \u001b[0;36mDEBUG\u001b[0m - \u001b[0;36miter_num value: 126, type: <class 'int'>\u001b[0m\n",
      "[\u001b[0;36mDEBUG\u001b[0m | src.tasks.clm_training.fabric.base]: \u001b[0;36miter_num value: 126, type: <class 'int'>\u001b[0m\n",
      "2025-06-16 13:55:19 - src.tasks.clm_training.fabric.base - \u001b[0;36mDEBUG\u001b[0m - \u001b[0;36mAbout to check: (iter_num + 1) % gradient_accumulation_steps != 0 -> (126 + 1) % 2 != 0\u001b[0m\n",
      "[\u001b[0;36mDEBUG\u001b[0m | src.tasks.clm_training.fabric.base]: \u001b[0;36mAbout to check: (iter_num + 1) % gradient_accumulation_steps != 0 -> (126 + 1) % 2 != 0\u001b[0m\n",
      "2025-06-16 13:55:20 - src.tasks.clm_training.fabric.base - \u001b[0;36mDEBUG\u001b[0m - \u001b[0;36miter 127 step 63: loss 5.4062, iter time: 122.20ms remaining time: \u001b[0m\n",
      "[\u001b[0;36mDEBUG\u001b[0m | src.tasks.clm_training.fabric.base]: \u001b[0;36miter 127 step 63: loss 5.4062, iter time: 122.20ms remaining time: \u001b[0m\n",
      "  3%|\u001b[34m█                                       \u001b[0m| 127/4682 [00:53<32:01,  2.37it/s]\u001b[0m2025-06-16 13:55:20 - src.tasks.clm_training.fabric.base - \u001b[0;36mDEBUG\u001b[0m - \u001b[0;36mgradient_accumulation_steps raw value: 2, type: <class 'int'>\u001b[0m\n",
      "[\u001b[0;36mDEBUG\u001b[0m | src.tasks.clm_training.fabric.base]: \u001b[0;36mgradient_accumulation_steps raw value: 2, type: <class 'int'>\u001b[0m\n",
      "2025-06-16 13:55:20 - src.tasks.clm_training.fabric.base - \u001b[0;36mDEBUG\u001b[0m - \u001b[0;36mgradient_accumulation_steps converted: 2, type: <class 'int'>\u001b[0m\n",
      "[\u001b[0;36mDEBUG\u001b[0m | src.tasks.clm_training.fabric.base]: \u001b[0;36mgradient_accumulation_steps converted: 2, type: <class 'int'>\u001b[0m\n",
      "2025-06-16 13:55:20 - src.tasks.clm_training.fabric.base - \u001b[0;36mDEBUG\u001b[0m - \u001b[0;36miter_num value: 127, type: <class 'int'>\u001b[0m\n",
      "[\u001b[0;36mDEBUG\u001b[0m | src.tasks.clm_training.fabric.base]: \u001b[0;36miter_num value: 127, type: <class 'int'>\u001b[0m\n",
      "2025-06-16 13:55:20 - src.tasks.clm_training.fabric.base - \u001b[0;36mDEBUG\u001b[0m - \u001b[0;36mAbout to check: (iter_num + 1) % gradient_accumulation_steps != 0 -> (127 + 1) % 2 != 0\u001b[0m\n",
      "[\u001b[0;36mDEBUG\u001b[0m | src.tasks.clm_training.fabric.base]: \u001b[0;36mAbout to check: (iter_num + 1) % gradient_accumulation_steps != 0 -> (127 + 1) % 2 != 0\u001b[0m\n",
      "2025-06-16 13:55:20 - src.tasks.clm_training.fabric.base - \u001b[0;36mDEBUG\u001b[0m - \u001b[0;36mGradient norm before clipping: 24.5000\u001b[0m\n",
      "[\u001b[0;36mDEBUG\u001b[0m | src.tasks.clm_training.fabric.base]: \u001b[0;36mGradient norm before clipping: 24.5000\u001b[0m\n",
      "2025-06-16 13:55:20 - src.tasks.clm_training.fabric.base - \u001b[0;36mDEBUG\u001b[0m - \u001b[0;36mvalidate_after_k_steps value: 1000, type: <class 'int'>\u001b[0m\n",
      "[\u001b[0;36mDEBUG\u001b[0m | src.tasks.clm_training.fabric.base]: \u001b[0;36mvalidate_after_k_steps value: 1000, type: <class 'int'>\u001b[0m\n",
      "2025-06-16 13:55:20 - src.tasks.clm_training.fabric.base - \u001b[0;36mDEBUG\u001b[0m - \u001b[0;36mstep_count value: 64, type: <class 'int'>\u001b[0m\n",
      "[\u001b[0;36mDEBUG\u001b[0m | src.tasks.clm_training.fabric.base]: \u001b[0;36mstep_count value: 64, type: <class 'int'>\u001b[0m\n",
      "2025-06-16 13:55:20 - src.tasks.clm_training.fabric.base - \u001b[0;36mDEBUG\u001b[0m - \u001b[0;36mValidation condition check - epochFinished: False, trainingFinished: False\u001b[0m\n",
      "[\u001b[0;36mDEBUG\u001b[0m | src.tasks.clm_training.fabric.base]: \u001b[0;36mValidation condition check - epochFinished: False, trainingFinished: False\u001b[0m\n",
      "2025-06-16 13:55:20 - src.tasks.clm_training.fabric.base - \u001b[0;36mDEBUG\u001b[0m - \u001b[0;36mAbout to check: step_count % validate_after_k_steps == 0 -> 64 % 1000 == 0\u001b[0m\n",
      "[\u001b[0;36mDEBUG\u001b[0m | src.tasks.clm_training.fabric.base]: \u001b[0;36mAbout to check: step_count % validate_after_k_steps == 0 -> 64 % 1000 == 0\u001b[0m\n",
      "2025-06-16 13:55:20 - src.tasks.clm_training.fabric.base - \u001b[0;36mDEBUG\u001b[0m - \u001b[0;36miter 128 step 64: loss 5.5312, iter time: 421.04ms remaining time: \u001b[0m\n",
      "[\u001b[0;36mDEBUG\u001b[0m | src.tasks.clm_training.fabric.base]: \u001b[0;36miter 128 step 64: loss 5.5312, iter time: 421.04ms remaining time: \u001b[0m\n",
      "  3%|\u001b[34m█                                       \u001b[0m| 128/4682 [00:54<32:13,  2.36it/s]\u001b[0m2025-06-16 13:55:20 - src.tasks.clm_training.fabric.base - \u001b[0;36mDEBUG\u001b[0m - \u001b[0;36mgradient_accumulation_steps raw value: 2, type: <class 'int'>\u001b[0m\n",
      "[\u001b[0;36mDEBUG\u001b[0m | src.tasks.clm_training.fabric.base]: \u001b[0;36mgradient_accumulation_steps raw value: 2, type: <class 'int'>\u001b[0m\n",
      "2025-06-16 13:55:20 - src.tasks.clm_training.fabric.base - \u001b[0;36mDEBUG\u001b[0m - \u001b[0;36mgradient_accumulation_steps converted: 2, type: <class 'int'>\u001b[0m\n",
      "[\u001b[0;36mDEBUG\u001b[0m | src.tasks.clm_training.fabric.base]: \u001b[0;36mgradient_accumulation_steps converted: 2, type: <class 'int'>\u001b[0m\n",
      "2025-06-16 13:55:20 - src.tasks.clm_training.fabric.base - \u001b[0;36mDEBUG\u001b[0m - \u001b[0;36miter_num value: 128, type: <class 'int'>\u001b[0m\n",
      "[\u001b[0;36mDEBUG\u001b[0m | src.tasks.clm_training.fabric.base]: \u001b[0;36miter_num value: 128, type: <class 'int'>\u001b[0m\n",
      "2025-06-16 13:55:20 - src.tasks.clm_training.fabric.base - \u001b[0;36mDEBUG\u001b[0m - \u001b[0;36mAbout to check: (iter_num + 1) % gradient_accumulation_steps != 0 -> (128 + 1) % 2 != 0\u001b[0m\n",
      "[\u001b[0;36mDEBUG\u001b[0m | src.tasks.clm_training.fabric.base]: \u001b[0;36mAbout to check: (iter_num + 1) % gradient_accumulation_steps != 0 -> (128 + 1) % 2 != 0\u001b[0m\n",
      "2025-06-16 13:55:20 - src.tasks.clm_training.fabric.base - \u001b[0;36mDEBUG\u001b[0m - \u001b[0;36miter 129 step 64: loss 5.7812, iter time: 122.60ms remaining time: \u001b[0m\n",
      "[\u001b[0;36mDEBUG\u001b[0m | src.tasks.clm_training.fabric.base]: \u001b[0;36miter 129 step 64: loss 5.7812, iter time: 122.60ms remaining time: \u001b[0m\n",
      "  3%|\u001b[34m█                                       \u001b[0m| 129/4682 [00:54<32:01,  2.37it/s]\u001b[0m2025-06-16 13:55:20 - src.tasks.clm_training.fabric.base - \u001b[0;36mDEBUG\u001b[0m - \u001b[0;36mgradient_accumulation_steps raw value: 2, type: <class 'int'>\u001b[0m\n",
      "[\u001b[0;36mDEBUG\u001b[0m | src.tasks.clm_training.fabric.base]: \u001b[0;36mgradient_accumulation_steps raw value: 2, type: <class 'int'>\u001b[0m\n",
      "2025-06-16 13:55:20 - src.tasks.clm_training.fabric.base - \u001b[0;36mDEBUG\u001b[0m - \u001b[0;36mgradient_accumulation_steps converted: 2, type: <class 'int'>\u001b[0m\n",
      "[\u001b[0;36mDEBUG\u001b[0m | src.tasks.clm_training.fabric.base]: \u001b[0;36mgradient_accumulation_steps converted: 2, type: <class 'int'>\u001b[0m\n",
      "2025-06-16 13:55:20 - src.tasks.clm_training.fabric.base - \u001b[0;36mDEBUG\u001b[0m - \u001b[0;36miter_num value: 129, type: <class 'int'>\u001b[0m\n",
      "[\u001b[0;36mDEBUG\u001b[0m | src.tasks.clm_training.fabric.base]: \u001b[0;36miter_num value: 129, type: <class 'int'>\u001b[0m\n",
      "2025-06-16 13:55:20 - src.tasks.clm_training.fabric.base - \u001b[0;36mDEBUG\u001b[0m - \u001b[0;36mAbout to check: (iter_num + 1) % gradient_accumulation_steps != 0 -> (129 + 1) % 2 != 0\u001b[0m\n",
      "[\u001b[0;36mDEBUG\u001b[0m | src.tasks.clm_training.fabric.base]: \u001b[0;36mAbout to check: (iter_num + 1) % gradient_accumulation_steps != 0 -> (129 + 1) % 2 != 0\u001b[0m\n",
      "2025-06-16 13:55:21 - src.tasks.clm_training.fabric.base - \u001b[0;36mDEBUG\u001b[0m - \u001b[0;36mGradient norm before clipping: 25.0000\u001b[0m\n",
      "[\u001b[0;36mDEBUG\u001b[0m | src.tasks.clm_training.fabric.base]: \u001b[0;36mGradient norm before clipping: 25.0000\u001b[0m\n",
      "2025-06-16 13:55:21 - src.tasks.clm_training.fabric.base - \u001b[0;36mDEBUG\u001b[0m - \u001b[0;36mvalidate_after_k_steps value: 1000, type: <class 'int'>\u001b[0m\n",
      "[\u001b[0;36mDEBUG\u001b[0m | src.tasks.clm_training.fabric.base]: \u001b[0;36mvalidate_after_k_steps value: 1000, type: <class 'int'>\u001b[0m\n",
      "2025-06-16 13:55:21 - src.tasks.clm_training.fabric.base - \u001b[0;36mDEBUG\u001b[0m - \u001b[0;36mstep_count value: 65, type: <class 'int'>\u001b[0m\n",
      "[\u001b[0;36mDEBUG\u001b[0m | src.tasks.clm_training.fabric.base]: \u001b[0;36mstep_count value: 65, type: <class 'int'>\u001b[0m\n",
      "2025-06-16 13:55:21 - src.tasks.clm_training.fabric.base - \u001b[0;36mDEBUG\u001b[0m - \u001b[0;36mValidation condition check - epochFinished: False, trainingFinished: False\u001b[0m\n",
      "[\u001b[0;36mDEBUG\u001b[0m | src.tasks.clm_training.fabric.base]: \u001b[0;36mValidation condition check - epochFinished: False, trainingFinished: False\u001b[0m\n",
      "2025-06-16 13:55:21 - src.tasks.clm_training.fabric.base - \u001b[0;36mDEBUG\u001b[0m - \u001b[0;36mAbout to check: step_count % validate_after_k_steps == 0 -> 65 % 1000 == 0\u001b[0m\n",
      "[\u001b[0;36mDEBUG\u001b[0m | src.tasks.clm_training.fabric.base]: \u001b[0;36mAbout to check: step_count % validate_after_k_steps == 0 -> 65 % 1000 == 0\u001b[0m\n",
      "2025-06-16 13:55:21 - src.tasks.clm_training.fabric.base - \u001b[0;36mDEBUG\u001b[0m - \u001b[0;36miter 130 step 65: loss 5.7188, iter time: 420.97ms remaining time: \u001b[0m\n",
      "[\u001b[0;36mDEBUG\u001b[0m | src.tasks.clm_training.fabric.base]: \u001b[0;36miter 130 step 65: loss 5.7188, iter time: 420.97ms remaining time: \u001b[0m\n",
      "  3%|\u001b[34m█                                       \u001b[0m| 130/4682 [00:55<32:13,  2.35it/s]\u001b[0m2025-06-16 13:55:21 - src.tasks.clm_training.fabric.base - \u001b[0;36mDEBUG\u001b[0m - \u001b[0;36mgradient_accumulation_steps raw value: 2, type: <class 'int'>\u001b[0m\n",
      "[\u001b[0;36mDEBUG\u001b[0m | src.tasks.clm_training.fabric.base]: \u001b[0;36mgradient_accumulation_steps raw value: 2, type: <class 'int'>\u001b[0m\n",
      "2025-06-16 13:55:21 - src.tasks.clm_training.fabric.base - \u001b[0;36mDEBUG\u001b[0m - \u001b[0;36mgradient_accumulation_steps converted: 2, type: <class 'int'>\u001b[0m\n",
      "[\u001b[0;36mDEBUG\u001b[0m | src.tasks.clm_training.fabric.base]: \u001b[0;36mgradient_accumulation_steps converted: 2, type: <class 'int'>\u001b[0m\n",
      "2025-06-16 13:55:21 - src.tasks.clm_training.fabric.base - \u001b[0;36mDEBUG\u001b[0m - \u001b[0;36miter_num value: 130, type: <class 'int'>\u001b[0m\n",
      "[\u001b[0;36mDEBUG\u001b[0m | src.tasks.clm_training.fabric.base]: \u001b[0;36miter_num value: 130, type: <class 'int'>\u001b[0m\n",
      "2025-06-16 13:55:21 - src.tasks.clm_training.fabric.base - \u001b[0;36mDEBUG\u001b[0m - \u001b[0;36mAbout to check: (iter_num + 1) % gradient_accumulation_steps != 0 -> (130 + 1) % 2 != 0\u001b[0m\n",
      "[\u001b[0;36mDEBUG\u001b[0m | src.tasks.clm_training.fabric.base]: \u001b[0;36mAbout to check: (iter_num + 1) % gradient_accumulation_steps != 0 -> (130 + 1) % 2 != 0\u001b[0m\n",
      "2025-06-16 13:55:21 - src.tasks.clm_training.fabric.base - \u001b[0;36mDEBUG\u001b[0m - \u001b[0;36miter 131 step 65: loss 5.7500, iter time: 122.64ms remaining time: \u001b[0m\n",
      "[\u001b[0;36mDEBUG\u001b[0m | src.tasks.clm_training.fabric.base]: \u001b[0;36miter 131 step 65: loss 5.7500, iter time: 122.64ms remaining time: \u001b[0m\n",
      "  3%|\u001b[34m█                                       \u001b[0m| 131/4682 [00:55<32:01,  2.37it/s]\u001b[0m2025-06-16 13:55:21 - src.tasks.clm_training.fabric.base - \u001b[0;36mDEBUG\u001b[0m - \u001b[0;36mgradient_accumulation_steps raw value: 2, type: <class 'int'>\u001b[0m\n",
      "[\u001b[0;36mDEBUG\u001b[0m | src.tasks.clm_training.fabric.base]: \u001b[0;36mgradient_accumulation_steps raw value: 2, type: <class 'int'>\u001b[0m\n",
      "2025-06-16 13:55:21 - src.tasks.clm_training.fabric.base - \u001b[0;36mDEBUG\u001b[0m - \u001b[0;36mgradient_accumulation_steps converted: 2, type: <class 'int'>\u001b[0m\n",
      "[\u001b[0;36mDEBUG\u001b[0m | src.tasks.clm_training.fabric.base]: \u001b[0;36mgradient_accumulation_steps converted: 2, type: <class 'int'>\u001b[0m\n",
      "2025-06-16 13:55:21 - src.tasks.clm_training.fabric.base - \u001b[0;36mDEBUG\u001b[0m - \u001b[0;36miter_num value: 131, type: <class 'int'>\u001b[0m\n",
      "[\u001b[0;36mDEBUG\u001b[0m | src.tasks.clm_training.fabric.base]: \u001b[0;36miter_num value: 131, type: <class 'int'>\u001b[0m\n",
      "2025-06-16 13:55:21 - src.tasks.clm_training.fabric.base - \u001b[0;36mDEBUG\u001b[0m - \u001b[0;36mAbout to check: (iter_num + 1) % gradient_accumulation_steps != 0 -> (131 + 1) % 2 != 0\u001b[0m\n",
      "[\u001b[0;36mDEBUG\u001b[0m | src.tasks.clm_training.fabric.base]: \u001b[0;36mAbout to check: (iter_num + 1) % gradient_accumulation_steps != 0 -> (131 + 1) % 2 != 0\u001b[0m\n",
      "2025-06-16 13:55:22 - src.tasks.clm_training.fabric.base - \u001b[0;36mDEBUG\u001b[0m - \u001b[0;36mGradient norm before clipping: 26.8750\u001b[0m\n",
      "[\u001b[0;36mDEBUG\u001b[0m | src.tasks.clm_training.fabric.base]: \u001b[0;36mGradient norm before clipping: 26.8750\u001b[0m\n",
      "2025-06-16 13:55:22 - src.tasks.clm_training.fabric.base - \u001b[0;36mDEBUG\u001b[0m - \u001b[0;36mvalidate_after_k_steps value: 1000, type: <class 'int'>\u001b[0m\n",
      "[\u001b[0;36mDEBUG\u001b[0m | src.tasks.clm_training.fabric.base]: \u001b[0;36mvalidate_after_k_steps value: 1000, type: <class 'int'>\u001b[0m\n",
      "2025-06-16 13:55:22 - src.tasks.clm_training.fabric.base - \u001b[0;36mDEBUG\u001b[0m - \u001b[0;36mstep_count value: 66, type: <class 'int'>\u001b[0m\n",
      "[\u001b[0;36mDEBUG\u001b[0m | src.tasks.clm_training.fabric.base]: \u001b[0;36mstep_count value: 66, type: <class 'int'>\u001b[0m\n",
      "2025-06-16 13:55:22 - src.tasks.clm_training.fabric.base - \u001b[0;36mDEBUG\u001b[0m - \u001b[0;36mValidation condition check - epochFinished: False, trainingFinished: False\u001b[0m\n",
      "[\u001b[0;36mDEBUG\u001b[0m | src.tasks.clm_training.fabric.base]: \u001b[0;36mValidation condition check - epochFinished: False, trainingFinished: False\u001b[0m\n",
      "2025-06-16 13:55:22 - src.tasks.clm_training.fabric.base - \u001b[0;36mDEBUG\u001b[0m - \u001b[0;36mAbout to check: step_count % validate_after_k_steps == 0 -> 66 % 1000 == 0\u001b[0m\n",
      "[\u001b[0;36mDEBUG\u001b[0m | src.tasks.clm_training.fabric.base]: \u001b[0;36mAbout to check: step_count % validate_after_k_steps == 0 -> 66 % 1000 == 0\u001b[0m\n",
      "2025-06-16 13:55:22 - src.tasks.clm_training.fabric.base - \u001b[0;36mDEBUG\u001b[0m - \u001b[0;36miter 132 step 66: loss 6.0000, iter time: 420.77ms remaining time: \u001b[0m\n",
      "[\u001b[0;36mDEBUG\u001b[0m | src.tasks.clm_training.fabric.base]: \u001b[0;36miter 132 step 66: loss 6.0000, iter time: 420.77ms remaining time: \u001b[0m\n",
      "  3%|\u001b[34m█▏                                      \u001b[0m| 132/4682 [00:56<32:12,  2.35it/s]\u001b[0m2025-06-16 13:55:22 - src.tasks.clm_training.fabric.base - \u001b[0;36mDEBUG\u001b[0m - \u001b[0;36mgradient_accumulation_steps raw value: 2, type: <class 'int'>\u001b[0m\n",
      "[\u001b[0;36mDEBUG\u001b[0m | src.tasks.clm_training.fabric.base]: \u001b[0;36mgradient_accumulation_steps raw value: 2, type: <class 'int'>\u001b[0m\n",
      "2025-06-16 13:55:22 - src.tasks.clm_training.fabric.base - \u001b[0;36mDEBUG\u001b[0m - \u001b[0;36mgradient_accumulation_steps converted: 2, type: <class 'int'>\u001b[0m\n",
      "[\u001b[0;36mDEBUG\u001b[0m | src.tasks.clm_training.fabric.base]: \u001b[0;36mgradient_accumulation_steps converted: 2, type: <class 'int'>\u001b[0m\n",
      "2025-06-16 13:55:22 - src.tasks.clm_training.fabric.base - \u001b[0;36mDEBUG\u001b[0m - \u001b[0;36miter_num value: 132, type: <class 'int'>\u001b[0m\n",
      "[\u001b[0;36mDEBUG\u001b[0m | src.tasks.clm_training.fabric.base]: \u001b[0;36miter_num value: 132, type: <class 'int'>\u001b[0m\n",
      "2025-06-16 13:55:22 - src.tasks.clm_training.fabric.base - \u001b[0;36mDEBUG\u001b[0m - \u001b[0;36mAbout to check: (iter_num + 1) % gradient_accumulation_steps != 0 -> (132 + 1) % 2 != 0\u001b[0m\n",
      "[\u001b[0;36mDEBUG\u001b[0m | src.tasks.clm_training.fabric.base]: \u001b[0;36mAbout to check: (iter_num + 1) % gradient_accumulation_steps != 0 -> (132 + 1) % 2 != 0\u001b[0m\n",
      "2025-06-16 13:55:22 - src.tasks.clm_training.fabric.base - \u001b[0;36mDEBUG\u001b[0m - \u001b[0;36miter 133 step 66: loss 5.7188, iter time: 122.30ms remaining time: \u001b[0m\n",
      "[\u001b[0;36mDEBUG\u001b[0m | src.tasks.clm_training.fabric.base]: \u001b[0;36miter 133 step 66: loss 5.7188, iter time: 122.30ms remaining time: \u001b[0m\n",
      "  3%|\u001b[34m█▏                                      \u001b[0m| 133/4682 [00:56<32:00,  2.37it/s]\u001b[0m2025-06-16 13:55:22 - src.tasks.clm_training.fabric.base - \u001b[0;36mDEBUG\u001b[0m - \u001b[0;36mgradient_accumulation_steps raw value: 2, type: <class 'int'>\u001b[0m\n",
      "[\u001b[0;36mDEBUG\u001b[0m | src.tasks.clm_training.fabric.base]: \u001b[0;36mgradient_accumulation_steps raw value: 2, type: <class 'int'>\u001b[0m\n",
      "2025-06-16 13:55:22 - src.tasks.clm_training.fabric.base - \u001b[0;36mDEBUG\u001b[0m - \u001b[0;36mgradient_accumulation_steps converted: 2, type: <class 'int'>\u001b[0m\n",
      "[\u001b[0;36mDEBUG\u001b[0m | src.tasks.clm_training.fabric.base]: \u001b[0;36mgradient_accumulation_steps converted: 2, type: <class 'int'>\u001b[0m\n",
      "2025-06-16 13:55:22 - src.tasks.clm_training.fabric.base - \u001b[0;36mDEBUG\u001b[0m - \u001b[0;36miter_num value: 133, type: <class 'int'>\u001b[0m\n",
      "[\u001b[0;36mDEBUG\u001b[0m | src.tasks.clm_training.fabric.base]: \u001b[0;36miter_num value: 133, type: <class 'int'>\u001b[0m\n",
      "2025-06-16 13:55:22 - src.tasks.clm_training.fabric.base - \u001b[0;36mDEBUG\u001b[0m - \u001b[0;36mAbout to check: (iter_num + 1) % gradient_accumulation_steps != 0 -> (133 + 1) % 2 != 0\u001b[0m\n",
      "[\u001b[0;36mDEBUG\u001b[0m | src.tasks.clm_training.fabric.base]: \u001b[0;36mAbout to check: (iter_num + 1) % gradient_accumulation_steps != 0 -> (133 + 1) % 2 != 0\u001b[0m\n",
      "2025-06-16 13:55:22 - src.tasks.clm_training.fabric.base - \u001b[0;36mDEBUG\u001b[0m - \u001b[0;36mGradient norm before clipping: 23.0000\u001b[0m\n",
      "[\u001b[0;36mDEBUG\u001b[0m | src.tasks.clm_training.fabric.base]: \u001b[0;36mGradient norm before clipping: 23.0000\u001b[0m\n",
      "2025-06-16 13:55:22 - src.tasks.clm_training.fabric.base - \u001b[0;36mDEBUG\u001b[0m - \u001b[0;36mvalidate_after_k_steps value: 1000, type: <class 'int'>\u001b[0m\n",
      "[\u001b[0;36mDEBUG\u001b[0m | src.tasks.clm_training.fabric.base]: \u001b[0;36mvalidate_after_k_steps value: 1000, type: <class 'int'>\u001b[0m\n",
      "2025-06-16 13:55:22 - src.tasks.clm_training.fabric.base - \u001b[0;36mDEBUG\u001b[0m - \u001b[0;36mstep_count value: 67, type: <class 'int'>\u001b[0m\n",
      "[\u001b[0;36mDEBUG\u001b[0m | src.tasks.clm_training.fabric.base]: \u001b[0;36mstep_count value: 67, type: <class 'int'>\u001b[0m\n",
      "2025-06-16 13:55:22 - src.tasks.clm_training.fabric.base - \u001b[0;36mDEBUG\u001b[0m - \u001b[0;36mValidation condition check - epochFinished: False, trainingFinished: False\u001b[0m\n",
      "[\u001b[0;36mDEBUG\u001b[0m | src.tasks.clm_training.fabric.base]: \u001b[0;36mValidation condition check - epochFinished: False, trainingFinished: False\u001b[0m\n",
      "2025-06-16 13:55:22 - src.tasks.clm_training.fabric.base - \u001b[0;36mDEBUG\u001b[0m - \u001b[0;36mAbout to check: step_count % validate_after_k_steps == 0 -> 67 % 1000 == 0\u001b[0m\n",
      "[\u001b[0;36mDEBUG\u001b[0m | src.tasks.clm_training.fabric.base]: \u001b[0;36mAbout to check: step_count % validate_after_k_steps == 0 -> 67 % 1000 == 0\u001b[0m\n",
      "2025-06-16 13:55:22 - src.tasks.clm_training.fabric.base - \u001b[0;36mDEBUG\u001b[0m - \u001b[0;36miter 134 step 67: loss 5.9062, iter time: 420.58ms remaining time: \u001b[0m\n",
      "[\u001b[0;36mDEBUG\u001b[0m | src.tasks.clm_training.fabric.base]: \u001b[0;36miter 134 step 67: loss 5.9062, iter time: 420.58ms remaining time: \u001b[0m\n",
      "  3%|\u001b[34m█▏                                      \u001b[0m| 134/4682 [00:56<32:11,  2.35it/s]\u001b[0m2025-06-16 13:55:22 - src.tasks.clm_training.fabric.base - \u001b[0;36mDEBUG\u001b[0m - \u001b[0;36mgradient_accumulation_steps raw value: 2, type: <class 'int'>\u001b[0m\n",
      "[\u001b[0;36mDEBUG\u001b[0m | src.tasks.clm_training.fabric.base]: \u001b[0;36mgradient_accumulation_steps raw value: 2, type: <class 'int'>\u001b[0m\n",
      "2025-06-16 13:55:22 - src.tasks.clm_training.fabric.base - \u001b[0;36mDEBUG\u001b[0m - \u001b[0;36mgradient_accumulation_steps converted: 2, type: <class 'int'>\u001b[0m\n",
      "[\u001b[0;36mDEBUG\u001b[0m | src.tasks.clm_training.fabric.base]: \u001b[0;36mgradient_accumulation_steps converted: 2, type: <class 'int'>\u001b[0m\n",
      "2025-06-16 13:55:22 - src.tasks.clm_training.fabric.base - \u001b[0;36mDEBUG\u001b[0m - \u001b[0;36miter_num value: 134, type: <class 'int'>\u001b[0m\n",
      "[\u001b[0;36mDEBUG\u001b[0m | src.tasks.clm_training.fabric.base]: \u001b[0;36miter_num value: 134, type: <class 'int'>\u001b[0m\n",
      "2025-06-16 13:55:22 - src.tasks.clm_training.fabric.base - \u001b[0;36mDEBUG\u001b[0m - \u001b[0;36mAbout to check: (iter_num + 1) % gradient_accumulation_steps != 0 -> (134 + 1) % 2 != 0\u001b[0m\n",
      "[\u001b[0;36mDEBUG\u001b[0m | src.tasks.clm_training.fabric.base]: \u001b[0;36mAbout to check: (iter_num + 1) % gradient_accumulation_steps != 0 -> (134 + 1) % 2 != 0\u001b[0m\n",
      "2025-06-16 13:55:23 - src.tasks.clm_training.fabric.base - \u001b[0;36mDEBUG\u001b[0m - \u001b[0;36miter 135 step 67: loss 5.8750, iter time: 122.69ms remaining time: \u001b[0m\n",
      "[\u001b[0;36mDEBUG\u001b[0m | src.tasks.clm_training.fabric.base]: \u001b[0;36miter 135 step 67: loss 5.8750, iter time: 122.69ms remaining time: \u001b[0m\n",
      "  3%|\u001b[34m█▏                                      \u001b[0m| 135/4682 [00:57<32:00,  2.37it/s]\u001b[0m2025-06-16 13:55:23 - src.tasks.clm_training.fabric.base - \u001b[0;36mDEBUG\u001b[0m - \u001b[0;36mgradient_accumulation_steps raw value: 2, type: <class 'int'>\u001b[0m\n",
      "[\u001b[0;36mDEBUG\u001b[0m | src.tasks.clm_training.fabric.base]: \u001b[0;36mgradient_accumulation_steps raw value: 2, type: <class 'int'>\u001b[0m\n",
      "2025-06-16 13:55:23 - src.tasks.clm_training.fabric.base - \u001b[0;36mDEBUG\u001b[0m - \u001b[0;36mgradient_accumulation_steps converted: 2, type: <class 'int'>\u001b[0m\n",
      "[\u001b[0;36mDEBUG\u001b[0m | src.tasks.clm_training.fabric.base]: \u001b[0;36mgradient_accumulation_steps converted: 2, type: <class 'int'>\u001b[0m\n",
      "2025-06-16 13:55:23 - src.tasks.clm_training.fabric.base - \u001b[0;36mDEBUG\u001b[0m - \u001b[0;36miter_num value: 135, type: <class 'int'>\u001b[0m\n",
      "[\u001b[0;36mDEBUG\u001b[0m | src.tasks.clm_training.fabric.base]: \u001b[0;36miter_num value: 135, type: <class 'int'>\u001b[0m\n",
      "2025-06-16 13:55:23 - src.tasks.clm_training.fabric.base - \u001b[0;36mDEBUG\u001b[0m - \u001b[0;36mAbout to check: (iter_num + 1) % gradient_accumulation_steps != 0 -> (135 + 1) % 2 != 0\u001b[0m\n",
      "[\u001b[0;36mDEBUG\u001b[0m | src.tasks.clm_training.fabric.base]: \u001b[0;36mAbout to check: (iter_num + 1) % gradient_accumulation_steps != 0 -> (135 + 1) % 2 != 0\u001b[0m\n",
      "2025-06-16 13:55:23 - src.tasks.clm_training.fabric.base - \u001b[0;36mDEBUG\u001b[0m - \u001b[0;36mGradient norm before clipping: 23.8750\u001b[0m\n",
      "[\u001b[0;36mDEBUG\u001b[0m | src.tasks.clm_training.fabric.base]: \u001b[0;36mGradient norm before clipping: 23.8750\u001b[0m\n",
      "2025-06-16 13:55:23 - src.tasks.clm_training.fabric.base - \u001b[0;36mDEBUG\u001b[0m - \u001b[0;36mvalidate_after_k_steps value: 1000, type: <class 'int'>\u001b[0m\n",
      "[\u001b[0;36mDEBUG\u001b[0m | src.tasks.clm_training.fabric.base]: \u001b[0;36mvalidate_after_k_steps value: 1000, type: <class 'int'>\u001b[0m\n",
      "2025-06-16 13:55:23 - src.tasks.clm_training.fabric.base - \u001b[0;36mDEBUG\u001b[0m - \u001b[0;36mstep_count value: 68, type: <class 'int'>\u001b[0m\n",
      "[\u001b[0;36mDEBUG\u001b[0m | src.tasks.clm_training.fabric.base]: \u001b[0;36mstep_count value: 68, type: <class 'int'>\u001b[0m\n",
      "2025-06-16 13:55:23 - src.tasks.clm_training.fabric.base - \u001b[0;36mDEBUG\u001b[0m - \u001b[0;36mValidation condition check - epochFinished: False, trainingFinished: False\u001b[0m\n",
      "[\u001b[0;36mDEBUG\u001b[0m | src.tasks.clm_training.fabric.base]: \u001b[0;36mValidation condition check - epochFinished: False, trainingFinished: False\u001b[0m\n",
      "2025-06-16 13:55:23 - src.tasks.clm_training.fabric.base - \u001b[0;36mDEBUG\u001b[0m - \u001b[0;36mAbout to check: step_count % validate_after_k_steps == 0 -> 68 % 1000 == 0\u001b[0m\n",
      "[\u001b[0;36mDEBUG\u001b[0m | src.tasks.clm_training.fabric.base]: \u001b[0;36mAbout to check: step_count % validate_after_k_steps == 0 -> 68 % 1000 == 0\u001b[0m\n",
      "2025-06-16 13:55:23 - src.tasks.clm_training.fabric.base - \u001b[0;36mDEBUG\u001b[0m - \u001b[0;36miter 136 step 68: loss 5.6250, iter time: 420.79ms remaining time: \u001b[0m\n",
      "[\u001b[0;36mDEBUG\u001b[0m | src.tasks.clm_training.fabric.base]: \u001b[0;36miter 136 step 68: loss 5.6250, iter time: 420.79ms remaining time: \u001b[0m\n",
      "  3%|\u001b[34m█▏                                      \u001b[0m| 136/4682 [00:57<32:11,  2.35it/s]\u001b[0m2025-06-16 13:55:23 - src.tasks.clm_training.fabric.base - \u001b[0;36mDEBUG\u001b[0m - \u001b[0;36mgradient_accumulation_steps raw value: 2, type: <class 'int'>\u001b[0m\n",
      "[\u001b[0;36mDEBUG\u001b[0m | src.tasks.clm_training.fabric.base]: \u001b[0;36mgradient_accumulation_steps raw value: 2, type: <class 'int'>\u001b[0m\n",
      "2025-06-16 13:55:23 - src.tasks.clm_training.fabric.base - \u001b[0;36mDEBUG\u001b[0m - \u001b[0;36mgradient_accumulation_steps converted: 2, type: <class 'int'>\u001b[0m\n",
      "[\u001b[0;36mDEBUG\u001b[0m | src.tasks.clm_training.fabric.base]: \u001b[0;36mgradient_accumulation_steps converted: 2, type: <class 'int'>\u001b[0m\n",
      "2025-06-16 13:55:23 - src.tasks.clm_training.fabric.base - \u001b[0;36mDEBUG\u001b[0m - \u001b[0;36miter_num value: 136, type: <class 'int'>\u001b[0m\n",
      "[\u001b[0;36mDEBUG\u001b[0m | src.tasks.clm_training.fabric.base]: \u001b[0;36miter_num value: 136, type: <class 'int'>\u001b[0m\n",
      "2025-06-16 13:55:23 - src.tasks.clm_training.fabric.base - \u001b[0;36mDEBUG\u001b[0m - \u001b[0;36mAbout to check: (iter_num + 1) % gradient_accumulation_steps != 0 -> (136 + 1) % 2 != 0\u001b[0m\n",
      "[\u001b[0;36mDEBUG\u001b[0m | src.tasks.clm_training.fabric.base]: \u001b[0;36mAbout to check: (iter_num + 1) % gradient_accumulation_steps != 0 -> (136 + 1) % 2 != 0\u001b[0m\n",
      "2025-06-16 13:55:24 - src.tasks.clm_training.fabric.base - \u001b[0;36mDEBUG\u001b[0m - \u001b[0;36miter 137 step 68: loss 5.3438, iter time: 121.98ms remaining time: \u001b[0m\n",
      "[\u001b[0;36mDEBUG\u001b[0m | src.tasks.clm_training.fabric.base]: \u001b[0;36miter 137 step 68: loss 5.3438, iter time: 121.98ms remaining time: \u001b[0m\n",
      "  3%|\u001b[34m█▏                                      \u001b[0m| 137/4682 [00:58<31:58,  2.37it/s]\u001b[0m2025-06-16 13:55:24 - src.tasks.clm_training.fabric.base - \u001b[0;36mDEBUG\u001b[0m - \u001b[0;36mgradient_accumulation_steps raw value: 2, type: <class 'int'>\u001b[0m\n",
      "[\u001b[0;36mDEBUG\u001b[0m | src.tasks.clm_training.fabric.base]: \u001b[0;36mgradient_accumulation_steps raw value: 2, type: <class 'int'>\u001b[0m\n",
      "2025-06-16 13:55:24 - src.tasks.clm_training.fabric.base - \u001b[0;36mDEBUG\u001b[0m - \u001b[0;36mgradient_accumulation_steps converted: 2, type: <class 'int'>\u001b[0m\n",
      "[\u001b[0;36mDEBUG\u001b[0m | src.tasks.clm_training.fabric.base]: \u001b[0;36mgradient_accumulation_steps converted: 2, type: <class 'int'>\u001b[0m\n",
      "2025-06-16 13:55:24 - src.tasks.clm_training.fabric.base - \u001b[0;36mDEBUG\u001b[0m - \u001b[0;36miter_num value: 137, type: <class 'int'>\u001b[0m\n",
      "[\u001b[0;36mDEBUG\u001b[0m | src.tasks.clm_training.fabric.base]: \u001b[0;36miter_num value: 137, type: <class 'int'>\u001b[0m\n",
      "2025-06-16 13:55:24 - src.tasks.clm_training.fabric.base - \u001b[0;36mDEBUG\u001b[0m - \u001b[0;36mAbout to check: (iter_num + 1) % gradient_accumulation_steps != 0 -> (137 + 1) % 2 != 0\u001b[0m\n",
      "[\u001b[0;36mDEBUG\u001b[0m | src.tasks.clm_training.fabric.base]: \u001b[0;36mAbout to check: (iter_num + 1) % gradient_accumulation_steps != 0 -> (137 + 1) % 2 != 0\u001b[0m\n",
      "2025-06-16 13:55:24 - src.tasks.clm_training.fabric.base - \u001b[0;36mDEBUG\u001b[0m - \u001b[0;36mGradient norm before clipping: 25.7500\u001b[0m\n",
      "[\u001b[0;36mDEBUG\u001b[0m | src.tasks.clm_training.fabric.base]: \u001b[0;36mGradient norm before clipping: 25.7500\u001b[0m\n",
      "2025-06-16 13:55:24 - src.tasks.clm_training.fabric.base - \u001b[0;36mDEBUG\u001b[0m - \u001b[0;36mvalidate_after_k_steps value: 1000, type: <class 'int'>\u001b[0m\n",
      "[\u001b[0;36mDEBUG\u001b[0m | src.tasks.clm_training.fabric.base]: \u001b[0;36mvalidate_after_k_steps value: 1000, type: <class 'int'>\u001b[0m\n",
      "2025-06-16 13:55:24 - src.tasks.clm_training.fabric.base - \u001b[0;36mDEBUG\u001b[0m - \u001b[0;36mstep_count value: 69, type: <class 'int'>\u001b[0m\n",
      "[\u001b[0;36mDEBUG\u001b[0m | src.tasks.clm_training.fabric.base]: \u001b[0;36mstep_count value: 69, type: <class 'int'>\u001b[0m\n",
      "2025-06-16 13:55:24 - src.tasks.clm_training.fabric.base - \u001b[0;36mDEBUG\u001b[0m - \u001b[0;36mValidation condition check - epochFinished: False, trainingFinished: False\u001b[0m\n",
      "[\u001b[0;36mDEBUG\u001b[0m | src.tasks.clm_training.fabric.base]: \u001b[0;36mValidation condition check - epochFinished: False, trainingFinished: False\u001b[0m\n",
      "2025-06-16 13:55:24 - src.tasks.clm_training.fabric.base - \u001b[0;36mDEBUG\u001b[0m - \u001b[0;36mAbout to check: step_count % validate_after_k_steps == 0 -> 69 % 1000 == 0\u001b[0m\n",
      "[\u001b[0;36mDEBUG\u001b[0m | src.tasks.clm_training.fabric.base]: \u001b[0;36mAbout to check: step_count % validate_after_k_steps == 0 -> 69 % 1000 == 0\u001b[0m\n",
      "2025-06-16 13:55:24 - src.tasks.clm_training.fabric.base - \u001b[0;36mDEBUG\u001b[0m - \u001b[0;36miter 138 step 69: loss 5.9688, iter time: 420.36ms remaining time: \u001b[0m\n",
      "[\u001b[0;36mDEBUG\u001b[0m | src.tasks.clm_training.fabric.base]: \u001b[0;36miter 138 step 69: loss 5.9688, iter time: 420.36ms remaining time: \u001b[0m\n",
      "  3%|\u001b[34m█▏                                      \u001b[0m| 138/4682 [00:58<32:09,  2.36it/s]\u001b[0m2025-06-16 13:55:24 - src.tasks.clm_training.fabric.base - \u001b[0;36mDEBUG\u001b[0m - \u001b[0;36mgradient_accumulation_steps raw value: 2, type: <class 'int'>\u001b[0m\n",
      "[\u001b[0;36mDEBUG\u001b[0m | src.tasks.clm_training.fabric.base]: \u001b[0;36mgradient_accumulation_steps raw value: 2, type: <class 'int'>\u001b[0m\n",
      "2025-06-16 13:55:24 - src.tasks.clm_training.fabric.base - \u001b[0;36mDEBUG\u001b[0m - \u001b[0;36mgradient_accumulation_steps converted: 2, type: <class 'int'>\u001b[0m\n",
      "[\u001b[0;36mDEBUG\u001b[0m | src.tasks.clm_training.fabric.base]: \u001b[0;36mgradient_accumulation_steps converted: 2, type: <class 'int'>\u001b[0m\n",
      "2025-06-16 13:55:24 - src.tasks.clm_training.fabric.base - \u001b[0;36mDEBUG\u001b[0m - \u001b[0;36miter_num value: 138, type: <class 'int'>\u001b[0m\n",
      "[\u001b[0;36mDEBUG\u001b[0m | src.tasks.clm_training.fabric.base]: \u001b[0;36miter_num value: 138, type: <class 'int'>\u001b[0m\n",
      "2025-06-16 13:55:24 - src.tasks.clm_training.fabric.base - \u001b[0;36mDEBUG\u001b[0m - \u001b[0;36mAbout to check: (iter_num + 1) % gradient_accumulation_steps != 0 -> (138 + 1) % 2 != 0\u001b[0m\n",
      "[\u001b[0;36mDEBUG\u001b[0m | src.tasks.clm_training.fabric.base]: \u001b[0;36mAbout to check: (iter_num + 1) % gradient_accumulation_steps != 0 -> (138 + 1) % 2 != 0\u001b[0m\n",
      "2025-06-16 13:55:25 - src.tasks.clm_training.fabric.base - \u001b[0;36mDEBUG\u001b[0m - \u001b[0;36miter 139 step 69: loss 6.2812, iter time: 122.53ms remaining time: \u001b[0m\n",
      "[\u001b[0;36mDEBUG\u001b[0m | src.tasks.clm_training.fabric.base]: \u001b[0;36miter 139 step 69: loss 6.2812, iter time: 122.53ms remaining time: \u001b[0m\n",
      "  3%|\u001b[34m█▏                                      \u001b[0m| 139/4682 [00:59<31:56,  2.37it/s]\u001b[0m2025-06-16 13:55:25 - src.tasks.clm_training.fabric.base - \u001b[0;36mDEBUG\u001b[0m - \u001b[0;36mgradient_accumulation_steps raw value: 2, type: <class 'int'>\u001b[0m\n",
      "[\u001b[0;36mDEBUG\u001b[0m | src.tasks.clm_training.fabric.base]: \u001b[0;36mgradient_accumulation_steps raw value: 2, type: <class 'int'>\u001b[0m\n",
      "2025-06-16 13:55:25 - src.tasks.clm_training.fabric.base - \u001b[0;36mDEBUG\u001b[0m - \u001b[0;36mgradient_accumulation_steps converted: 2, type: <class 'int'>\u001b[0m\n",
      "[\u001b[0;36mDEBUG\u001b[0m | src.tasks.clm_training.fabric.base]: \u001b[0;36mgradient_accumulation_steps converted: 2, type: <class 'int'>\u001b[0m\n",
      "2025-06-16 13:55:25 - src.tasks.clm_training.fabric.base - \u001b[0;36mDEBUG\u001b[0m - \u001b[0;36miter_num value: 139, type: <class 'int'>\u001b[0m\n",
      "[\u001b[0;36mDEBUG\u001b[0m | src.tasks.clm_training.fabric.base]: \u001b[0;36miter_num value: 139, type: <class 'int'>\u001b[0m\n",
      "2025-06-16 13:55:25 - src.tasks.clm_training.fabric.base - \u001b[0;36mDEBUG\u001b[0m - \u001b[0;36mAbout to check: (iter_num + 1) % gradient_accumulation_steps != 0 -> (139 + 1) % 2 != 0\u001b[0m\n",
      "[\u001b[0;36mDEBUG\u001b[0m | src.tasks.clm_training.fabric.base]: \u001b[0;36mAbout to check: (iter_num + 1) % gradient_accumulation_steps != 0 -> (139 + 1) % 2 != 0\u001b[0m\n",
      "2025-06-16 13:55:25 - src.tasks.clm_training.fabric.base - \u001b[0;36mDEBUG\u001b[0m - \u001b[0;36mGradient norm before clipping: 24.1250\u001b[0m\n",
      "[\u001b[0;36mDEBUG\u001b[0m | src.tasks.clm_training.fabric.base]: \u001b[0;36mGradient norm before clipping: 24.1250\u001b[0m\n",
      "2025-06-16 13:55:25 - src.tasks.clm_training.fabric.base - \u001b[0;36mDEBUG\u001b[0m - \u001b[0;36mvalidate_after_k_steps value: 1000, type: <class 'int'>\u001b[0m\n",
      "[\u001b[0;36mDEBUG\u001b[0m | src.tasks.clm_training.fabric.base]: \u001b[0;36mvalidate_after_k_steps value: 1000, type: <class 'int'>\u001b[0m\n",
      "2025-06-16 13:55:25 - src.tasks.clm_training.fabric.base - \u001b[0;36mDEBUG\u001b[0m - \u001b[0;36mstep_count value: 70, type: <class 'int'>\u001b[0m\n",
      "[\u001b[0;36mDEBUG\u001b[0m | src.tasks.clm_training.fabric.base]: \u001b[0;36mstep_count value: 70, type: <class 'int'>\u001b[0m\n",
      "2025-06-16 13:55:25 - src.tasks.clm_training.fabric.base - \u001b[0;36mDEBUG\u001b[0m - \u001b[0;36mValidation condition check - epochFinished: False, trainingFinished: False\u001b[0m\n",
      "[\u001b[0;36mDEBUG\u001b[0m | src.tasks.clm_training.fabric.base]: \u001b[0;36mValidation condition check - epochFinished: False, trainingFinished: False\u001b[0m\n",
      "2025-06-16 13:55:25 - src.tasks.clm_training.fabric.base - \u001b[0;36mDEBUG\u001b[0m - \u001b[0;36mAbout to check: step_count % validate_after_k_steps == 0 -> 70 % 1000 == 0\u001b[0m\n",
      "[\u001b[0;36mDEBUG\u001b[0m | src.tasks.clm_training.fabric.base]: \u001b[0;36mAbout to check: step_count % validate_after_k_steps == 0 -> 70 % 1000 == 0\u001b[0m\n",
      "2025-06-16 13:55:25 - src.tasks.clm_training.fabric.base - \u001b[0;36mDEBUG\u001b[0m - \u001b[0;36miter 140 step 70: loss 5.3438, iter time: 420.23ms remaining time: \u001b[0m\n",
      "[\u001b[0;36mDEBUG\u001b[0m | src.tasks.clm_training.fabric.base]: \u001b[0;36miter 140 step 70: loss 5.3438, iter time: 420.23ms remaining time: \u001b[0m\n",
      "  3%|\u001b[34m█▏                                      \u001b[0m| 140/4682 [00:59<32:07,  2.36it/s]\u001b[0m2025-06-16 13:55:25 - src.tasks.clm_training.fabric.base - \u001b[0;36mDEBUG\u001b[0m - \u001b[0;36mgradient_accumulation_steps raw value: 2, type: <class 'int'>\u001b[0m\n",
      "[\u001b[0;36mDEBUG\u001b[0m | src.tasks.clm_training.fabric.base]: \u001b[0;36mgradient_accumulation_steps raw value: 2, type: <class 'int'>\u001b[0m\n",
      "2025-06-16 13:55:25 - src.tasks.clm_training.fabric.base - \u001b[0;36mDEBUG\u001b[0m - \u001b[0;36mgradient_accumulation_steps converted: 2, type: <class 'int'>\u001b[0m\n",
      "[\u001b[0;36mDEBUG\u001b[0m | src.tasks.clm_training.fabric.base]: \u001b[0;36mgradient_accumulation_steps converted: 2, type: <class 'int'>\u001b[0m\n",
      "2025-06-16 13:55:25 - src.tasks.clm_training.fabric.base - \u001b[0;36mDEBUG\u001b[0m - \u001b[0;36miter_num value: 140, type: <class 'int'>\u001b[0m\n",
      "[\u001b[0;36mDEBUG\u001b[0m | src.tasks.clm_training.fabric.base]: \u001b[0;36miter_num value: 140, type: <class 'int'>\u001b[0m\n",
      "2025-06-16 13:55:25 - src.tasks.clm_training.fabric.base - \u001b[0;36mDEBUG\u001b[0m - \u001b[0;36mAbout to check: (iter_num + 1) % gradient_accumulation_steps != 0 -> (140 + 1) % 2 != 0\u001b[0m\n",
      "[\u001b[0;36mDEBUG\u001b[0m | src.tasks.clm_training.fabric.base]: \u001b[0;36mAbout to check: (iter_num + 1) % gradient_accumulation_steps != 0 -> (140 + 1) % 2 != 0\u001b[0m\n",
      "2025-06-16 13:55:25 - src.tasks.clm_training.fabric.base - \u001b[0;36mDEBUG\u001b[0m - \u001b[0;36miter 141 step 70: loss 5.8750, iter time: 122.02ms remaining time: \u001b[0m\n",
      "[\u001b[0;36mDEBUG\u001b[0m | src.tasks.clm_training.fabric.base]: \u001b[0;36miter 141 step 70: loss 5.8750, iter time: 122.02ms remaining time: \u001b[0m\n",
      "  3%|\u001b[34m█▏                                      \u001b[0m| 141/4682 [00:59<31:55,  2.37it/s]\u001b[0m2025-06-16 13:55:25 - src.tasks.clm_training.fabric.base - \u001b[0;36mDEBUG\u001b[0m - \u001b[0;36mgradient_accumulation_steps raw value: 2, type: <class 'int'>\u001b[0m\n",
      "[\u001b[0;36mDEBUG\u001b[0m | src.tasks.clm_training.fabric.base]: \u001b[0;36mgradient_accumulation_steps raw value: 2, type: <class 'int'>\u001b[0m\n",
      "2025-06-16 13:55:25 - src.tasks.clm_training.fabric.base - \u001b[0;36mDEBUG\u001b[0m - \u001b[0;36mgradient_accumulation_steps converted: 2, type: <class 'int'>\u001b[0m\n",
      "[\u001b[0;36mDEBUG\u001b[0m | src.tasks.clm_training.fabric.base]: \u001b[0;36mgradient_accumulation_steps converted: 2, type: <class 'int'>\u001b[0m\n",
      "2025-06-16 13:55:25 - src.tasks.clm_training.fabric.base - \u001b[0;36mDEBUG\u001b[0m - \u001b[0;36miter_num value: 141, type: <class 'int'>\u001b[0m\n",
      "[\u001b[0;36mDEBUG\u001b[0m | src.tasks.clm_training.fabric.base]: \u001b[0;36miter_num value: 141, type: <class 'int'>\u001b[0m\n",
      "2025-06-16 13:55:25 - src.tasks.clm_training.fabric.base - \u001b[0;36mDEBUG\u001b[0m - \u001b[0;36mAbout to check: (iter_num + 1) % gradient_accumulation_steps != 0 -> (141 + 1) % 2 != 0\u001b[0m\n",
      "[\u001b[0;36mDEBUG\u001b[0m | src.tasks.clm_training.fabric.base]: \u001b[0;36mAbout to check: (iter_num + 1) % gradient_accumulation_steps != 0 -> (141 + 1) % 2 != 0\u001b[0m\n",
      "2025-06-16 13:55:26 - src.tasks.clm_training.fabric.base - \u001b[0;36mDEBUG\u001b[0m - \u001b[0;36mGradient norm before clipping: 24.0000\u001b[0m\n",
      "[\u001b[0;36mDEBUG\u001b[0m | src.tasks.clm_training.fabric.base]: \u001b[0;36mGradient norm before clipping: 24.0000\u001b[0m\n",
      "2025-06-16 13:55:26 - src.tasks.clm_training.fabric.base - \u001b[0;36mDEBUG\u001b[0m - \u001b[0;36mvalidate_after_k_steps value: 1000, type: <class 'int'>\u001b[0m\n",
      "[\u001b[0;36mDEBUG\u001b[0m | src.tasks.clm_training.fabric.base]: \u001b[0;36mvalidate_after_k_steps value: 1000, type: <class 'int'>\u001b[0m\n",
      "2025-06-16 13:55:26 - src.tasks.clm_training.fabric.base - \u001b[0;36mDEBUG\u001b[0m - \u001b[0;36mstep_count value: 71, type: <class 'int'>\u001b[0m\n",
      "[\u001b[0;36mDEBUG\u001b[0m | src.tasks.clm_training.fabric.base]: \u001b[0;36mstep_count value: 71, type: <class 'int'>\u001b[0m\n",
      "2025-06-16 13:55:26 - src.tasks.clm_training.fabric.base - \u001b[0;36mDEBUG\u001b[0m - \u001b[0;36mValidation condition check - epochFinished: False, trainingFinished: False\u001b[0m\n",
      "[\u001b[0;36mDEBUG\u001b[0m | src.tasks.clm_training.fabric.base]: \u001b[0;36mValidation condition check - epochFinished: False, trainingFinished: False\u001b[0m\n",
      "2025-06-16 13:55:26 - src.tasks.clm_training.fabric.base - \u001b[0;36mDEBUG\u001b[0m - \u001b[0;36mAbout to check: step_count % validate_after_k_steps == 0 -> 71 % 1000 == 0\u001b[0m\n",
      "[\u001b[0;36mDEBUG\u001b[0m | src.tasks.clm_training.fabric.base]: \u001b[0;36mAbout to check: step_count % validate_after_k_steps == 0 -> 71 % 1000 == 0\u001b[0m\n",
      "2025-06-16 13:55:26 - src.tasks.clm_training.fabric.base - \u001b[0;36mDEBUG\u001b[0m - \u001b[0;36miter 142 step 71: loss 5.4688, iter time: 420.68ms remaining time: \u001b[0m\n",
      "[\u001b[0;36mDEBUG\u001b[0m | src.tasks.clm_training.fabric.base]: \u001b[0;36miter 142 step 71: loss 5.4688, iter time: 420.68ms remaining time: \u001b[0m\n",
      "  3%|\u001b[34m█▏                                      \u001b[0m| 142/4682 [01:00<32:07,  2.36it/s]\u001b[0m2025-06-16 13:55:26 - src.tasks.clm_training.fabric.base - \u001b[0;36mDEBUG\u001b[0m - \u001b[0;36mgradient_accumulation_steps raw value: 2, type: <class 'int'>\u001b[0m\n",
      "[\u001b[0;36mDEBUG\u001b[0m | src.tasks.clm_training.fabric.base]: \u001b[0;36mgradient_accumulation_steps raw value: 2, type: <class 'int'>\u001b[0m\n",
      "2025-06-16 13:55:26 - src.tasks.clm_training.fabric.base - \u001b[0;36mDEBUG\u001b[0m - \u001b[0;36mgradient_accumulation_steps converted: 2, type: <class 'int'>\u001b[0m\n",
      "[\u001b[0;36mDEBUG\u001b[0m | src.tasks.clm_training.fabric.base]: \u001b[0;36mgradient_accumulation_steps converted: 2, type: <class 'int'>\u001b[0m\n",
      "2025-06-16 13:55:26 - src.tasks.clm_training.fabric.base - \u001b[0;36mDEBUG\u001b[0m - \u001b[0;36miter_num value: 142, type: <class 'int'>\u001b[0m\n",
      "[\u001b[0;36mDEBUG\u001b[0m | src.tasks.clm_training.fabric.base]: \u001b[0;36miter_num value: 142, type: <class 'int'>\u001b[0m\n",
      "2025-06-16 13:55:26 - src.tasks.clm_training.fabric.base - \u001b[0;36mDEBUG\u001b[0m - \u001b[0;36mAbout to check: (iter_num + 1) % gradient_accumulation_steps != 0 -> (142 + 1) % 2 != 0\u001b[0m\n",
      "[\u001b[0;36mDEBUG\u001b[0m | src.tasks.clm_training.fabric.base]: \u001b[0;36mAbout to check: (iter_num + 1) % gradient_accumulation_steps != 0 -> (142 + 1) % 2 != 0\u001b[0m\n",
      "2025-06-16 13:55:26 - src.tasks.clm_training.fabric.base - \u001b[0;36mDEBUG\u001b[0m - \u001b[0;36miter 143 step 71: loss 5.1875, iter time: 122.82ms remaining time: \u001b[0m\n",
      "[\u001b[0;36mDEBUG\u001b[0m | src.tasks.clm_training.fabric.base]: \u001b[0;36miter 143 step 71: loss 5.1875, iter time: 122.82ms remaining time: \u001b[0m\n",
      "  3%|\u001b[34m█▏                                      \u001b[0m| 143/4682 [01:00<31:56,  2.37it/s]\u001b[0m2025-06-16 13:55:26 - src.tasks.clm_training.fabric.base - \u001b[0;36mDEBUG\u001b[0m - \u001b[0;36mgradient_accumulation_steps raw value: 2, type: <class 'int'>\u001b[0m\n",
      "[\u001b[0;36mDEBUG\u001b[0m | src.tasks.clm_training.fabric.base]: \u001b[0;36mgradient_accumulation_steps raw value: 2, type: <class 'int'>\u001b[0m\n",
      "2025-06-16 13:55:26 - src.tasks.clm_training.fabric.base - \u001b[0;36mDEBUG\u001b[0m - \u001b[0;36mgradient_accumulation_steps converted: 2, type: <class 'int'>\u001b[0m\n",
      "[\u001b[0;36mDEBUG\u001b[0m | src.tasks.clm_training.fabric.base]: \u001b[0;36mgradient_accumulation_steps converted: 2, type: <class 'int'>\u001b[0m\n",
      "2025-06-16 13:55:26 - src.tasks.clm_training.fabric.base - \u001b[0;36mDEBUG\u001b[0m - \u001b[0;36miter_num value: 143, type: <class 'int'>\u001b[0m\n",
      "[\u001b[0;36mDEBUG\u001b[0m | src.tasks.clm_training.fabric.base]: \u001b[0;36miter_num value: 143, type: <class 'int'>\u001b[0m\n",
      "2025-06-16 13:55:26 - src.tasks.clm_training.fabric.base - \u001b[0;36mDEBUG\u001b[0m - \u001b[0;36mAbout to check: (iter_num + 1) % gradient_accumulation_steps != 0 -> (143 + 1) % 2 != 0\u001b[0m\n",
      "[\u001b[0;36mDEBUG\u001b[0m | src.tasks.clm_training.fabric.base]: \u001b[0;36mAbout to check: (iter_num + 1) % gradient_accumulation_steps != 0 -> (143 + 1) % 2 != 0\u001b[0m\n",
      "2025-06-16 13:55:27 - src.tasks.clm_training.fabric.base - \u001b[0;36mDEBUG\u001b[0m - \u001b[0;36mGradient norm before clipping: 22.8750\u001b[0m\n",
      "[\u001b[0;36mDEBUG\u001b[0m | src.tasks.clm_training.fabric.base]: \u001b[0;36mGradient norm before clipping: 22.8750\u001b[0m\n",
      "2025-06-16 13:55:27 - src.tasks.clm_training.fabric.base - \u001b[0;36mDEBUG\u001b[0m - \u001b[0;36mvalidate_after_k_steps value: 1000, type: <class 'int'>\u001b[0m\n",
      "[\u001b[0;36mDEBUG\u001b[0m | src.tasks.clm_training.fabric.base]: \u001b[0;36mvalidate_after_k_steps value: 1000, type: <class 'int'>\u001b[0m\n",
      "2025-06-16 13:55:27 - src.tasks.clm_training.fabric.base - \u001b[0;36mDEBUG\u001b[0m - \u001b[0;36mstep_count value: 72, type: <class 'int'>\u001b[0m\n",
      "[\u001b[0;36mDEBUG\u001b[0m | src.tasks.clm_training.fabric.base]: \u001b[0;36mstep_count value: 72, type: <class 'int'>\u001b[0m\n",
      "2025-06-16 13:55:27 - src.tasks.clm_training.fabric.base - \u001b[0;36mDEBUG\u001b[0m - \u001b[0;36mValidation condition check - epochFinished: False, trainingFinished: False\u001b[0m\n",
      "[\u001b[0;36mDEBUG\u001b[0m | src.tasks.clm_training.fabric.base]: \u001b[0;36mValidation condition check - epochFinished: False, trainingFinished: False\u001b[0m\n",
      "2025-06-16 13:55:27 - src.tasks.clm_training.fabric.base - \u001b[0;36mDEBUG\u001b[0m - \u001b[0;36mAbout to check: step_count % validate_after_k_steps == 0 -> 72 % 1000 == 0\u001b[0m\n",
      "[\u001b[0;36mDEBUG\u001b[0m | src.tasks.clm_training.fabric.base]: \u001b[0;36mAbout to check: step_count % validate_after_k_steps == 0 -> 72 % 1000 == 0\u001b[0m\n",
      "2025-06-16 13:55:27 - src.tasks.clm_training.fabric.base - \u001b[0;36mDEBUG\u001b[0m - \u001b[0;36miter 144 step 72: loss 5.7188, iter time: 420.58ms remaining time: \u001b[0m\n",
      "[\u001b[0;36mDEBUG\u001b[0m | src.tasks.clm_training.fabric.base]: \u001b[0;36miter 144 step 72: loss 5.7188, iter time: 420.58ms remaining time: \u001b[0m\n",
      "  3%|\u001b[34m█▏                                      \u001b[0m| 144/4682 [01:01<32:07,  2.35it/s]\u001b[0m2025-06-16 13:55:27 - src.tasks.clm_training.fabric.base - \u001b[0;36mDEBUG\u001b[0m - \u001b[0;36mgradient_accumulation_steps raw value: 2, type: <class 'int'>\u001b[0m\n",
      "[\u001b[0;36mDEBUG\u001b[0m | src.tasks.clm_training.fabric.base]: \u001b[0;36mgradient_accumulation_steps raw value: 2, type: <class 'int'>\u001b[0m\n",
      "2025-06-16 13:55:27 - src.tasks.clm_training.fabric.base - \u001b[0;36mDEBUG\u001b[0m - \u001b[0;36mgradient_accumulation_steps converted: 2, type: <class 'int'>\u001b[0m\n",
      "[\u001b[0;36mDEBUG\u001b[0m | src.tasks.clm_training.fabric.base]: \u001b[0;36mgradient_accumulation_steps converted: 2, type: <class 'int'>\u001b[0m\n",
      "2025-06-16 13:55:27 - src.tasks.clm_training.fabric.base - \u001b[0;36mDEBUG\u001b[0m - \u001b[0;36miter_num value: 144, type: <class 'int'>\u001b[0m\n",
      "[\u001b[0;36mDEBUG\u001b[0m | src.tasks.clm_training.fabric.base]: \u001b[0;36miter_num value: 144, type: <class 'int'>\u001b[0m\n",
      "2025-06-16 13:55:27 - src.tasks.clm_training.fabric.base - \u001b[0;36mDEBUG\u001b[0m - \u001b[0;36mAbout to check: (iter_num + 1) % gradient_accumulation_steps != 0 -> (144 + 1) % 2 != 0\u001b[0m\n",
      "[\u001b[0;36mDEBUG\u001b[0m | src.tasks.clm_training.fabric.base]: \u001b[0;36mAbout to check: (iter_num + 1) % gradient_accumulation_steps != 0 -> (144 + 1) % 2 != 0\u001b[0m\n",
      "2025-06-16 13:55:27 - src.tasks.clm_training.fabric.base - \u001b[0;36mDEBUG\u001b[0m - \u001b[0;36miter 145 step 72: loss 5.5000, iter time: 122.33ms remaining time: \u001b[0m\n",
      "[\u001b[0;36mDEBUG\u001b[0m | src.tasks.clm_training.fabric.base]: \u001b[0;36miter 145 step 72: loss 5.5000, iter time: 122.33ms remaining time: \u001b[0m\n",
      "  3%|\u001b[34m█▏                                      \u001b[0m| 145/4682 [01:01<31:56,  2.37it/s]\u001b[0m2025-06-16 13:55:27 - src.tasks.clm_training.fabric.base - \u001b[0;36mDEBUG\u001b[0m - \u001b[0;36mgradient_accumulation_steps raw value: 2, type: <class 'int'>\u001b[0m\n",
      "[\u001b[0;36mDEBUG\u001b[0m | src.tasks.clm_training.fabric.base]: \u001b[0;36mgradient_accumulation_steps raw value: 2, type: <class 'int'>\u001b[0m\n",
      "2025-06-16 13:55:27 - src.tasks.clm_training.fabric.base - \u001b[0;36mDEBUG\u001b[0m - \u001b[0;36mgradient_accumulation_steps converted: 2, type: <class 'int'>\u001b[0m\n",
      "[\u001b[0;36mDEBUG\u001b[0m | src.tasks.clm_training.fabric.base]: \u001b[0;36mgradient_accumulation_steps converted: 2, type: <class 'int'>\u001b[0m\n",
      "2025-06-16 13:55:27 - src.tasks.clm_training.fabric.base - \u001b[0;36mDEBUG\u001b[0m - \u001b[0;36miter_num value: 145, type: <class 'int'>\u001b[0m\n",
      "[\u001b[0;36mDEBUG\u001b[0m | src.tasks.clm_training.fabric.base]: \u001b[0;36miter_num value: 145, type: <class 'int'>\u001b[0m\n",
      "2025-06-16 13:55:27 - src.tasks.clm_training.fabric.base - \u001b[0;36mDEBUG\u001b[0m - \u001b[0;36mAbout to check: (iter_num + 1) % gradient_accumulation_steps != 0 -> (145 + 1) % 2 != 0\u001b[0m\n",
      "[\u001b[0;36mDEBUG\u001b[0m | src.tasks.clm_training.fabric.base]: \u001b[0;36mAbout to check: (iter_num + 1) % gradient_accumulation_steps != 0 -> (145 + 1) % 2 != 0\u001b[0m\n",
      "2025-06-16 13:55:28 - src.tasks.clm_training.fabric.base - \u001b[0;36mDEBUG\u001b[0m - \u001b[0;36mGradient norm before clipping: 24.5000\u001b[0m\n",
      "[\u001b[0;36mDEBUG\u001b[0m | src.tasks.clm_training.fabric.base]: \u001b[0;36mGradient norm before clipping: 24.5000\u001b[0m\n",
      "2025-06-16 13:55:28 - src.tasks.clm_training.fabric.base - \u001b[0;36mDEBUG\u001b[0m - \u001b[0;36mvalidate_after_k_steps value: 1000, type: <class 'int'>\u001b[0m\n",
      "[\u001b[0;36mDEBUG\u001b[0m | src.tasks.clm_training.fabric.base]: \u001b[0;36mvalidate_after_k_steps value: 1000, type: <class 'int'>\u001b[0m\n",
      "2025-06-16 13:55:28 - src.tasks.clm_training.fabric.base - \u001b[0;36mDEBUG\u001b[0m - \u001b[0;36mstep_count value: 73, type: <class 'int'>\u001b[0m\n",
      "[\u001b[0;36mDEBUG\u001b[0m | src.tasks.clm_training.fabric.base]: \u001b[0;36mstep_count value: 73, type: <class 'int'>\u001b[0m\n",
      "2025-06-16 13:55:28 - src.tasks.clm_training.fabric.base - \u001b[0;36mDEBUG\u001b[0m - \u001b[0;36mValidation condition check - epochFinished: False, trainingFinished: False\u001b[0m\n",
      "[\u001b[0;36mDEBUG\u001b[0m | src.tasks.clm_training.fabric.base]: \u001b[0;36mValidation condition check - epochFinished: False, trainingFinished: False\u001b[0m\n",
      "2025-06-16 13:55:28 - src.tasks.clm_training.fabric.base - \u001b[0;36mDEBUG\u001b[0m - \u001b[0;36mAbout to check: step_count % validate_after_k_steps == 0 -> 73 % 1000 == 0\u001b[0m\n",
      "[\u001b[0;36mDEBUG\u001b[0m | src.tasks.clm_training.fabric.base]: \u001b[0;36mAbout to check: step_count % validate_after_k_steps == 0 -> 73 % 1000 == 0\u001b[0m\n",
      "2025-06-16 13:55:28 - src.tasks.clm_training.fabric.base - \u001b[0;36mDEBUG\u001b[0m - \u001b[0;36miter 146 step 73: loss 5.7188, iter time: 421.26ms remaining time: \u001b[0m\n",
      "[\u001b[0;36mDEBUG\u001b[0m | src.tasks.clm_training.fabric.base]: \u001b[0;36miter 146 step 73: loss 5.7188, iter time: 421.26ms remaining time: \u001b[0m\n",
      "  3%|\u001b[34m█▏                                      \u001b[0m| 146/4682 [01:02<32:08,  2.35it/s]\u001b[0m2025-06-16 13:55:28 - src.tasks.clm_training.fabric.base - \u001b[0;36mDEBUG\u001b[0m - \u001b[0;36mgradient_accumulation_steps raw value: 2, type: <class 'int'>\u001b[0m\n",
      "[\u001b[0;36mDEBUG\u001b[0m | src.tasks.clm_training.fabric.base]: \u001b[0;36mgradient_accumulation_steps raw value: 2, type: <class 'int'>\u001b[0m\n",
      "2025-06-16 13:55:28 - src.tasks.clm_training.fabric.base - \u001b[0;36mDEBUG\u001b[0m - \u001b[0;36mgradient_accumulation_steps converted: 2, type: <class 'int'>\u001b[0m\n",
      "[\u001b[0;36mDEBUG\u001b[0m | src.tasks.clm_training.fabric.base]: \u001b[0;36mgradient_accumulation_steps converted: 2, type: <class 'int'>\u001b[0m\n",
      "2025-06-16 13:55:28 - src.tasks.clm_training.fabric.base - \u001b[0;36mDEBUG\u001b[0m - \u001b[0;36miter_num value: 146, type: <class 'int'>\u001b[0m\n",
      "[\u001b[0;36mDEBUG\u001b[0m | src.tasks.clm_training.fabric.base]: \u001b[0;36miter_num value: 146, type: <class 'int'>\u001b[0m\n",
      "2025-06-16 13:55:28 - src.tasks.clm_training.fabric.base - \u001b[0;36mDEBUG\u001b[0m - \u001b[0;36mAbout to check: (iter_num + 1) % gradient_accumulation_steps != 0 -> (146 + 1) % 2 != 0\u001b[0m\n",
      "[\u001b[0;36mDEBUG\u001b[0m | src.tasks.clm_training.fabric.base]: \u001b[0;36mAbout to check: (iter_num + 1) % gradient_accumulation_steps != 0 -> (146 + 1) % 2 != 0\u001b[0m\n",
      "2025-06-16 13:55:28 - src.tasks.clm_training.fabric.base - \u001b[0;36mDEBUG\u001b[0m - \u001b[0;36miter 147 step 73: loss 5.7188, iter time: 122.36ms remaining time: \u001b[0m\n",
      "[\u001b[0;36mDEBUG\u001b[0m | src.tasks.clm_training.fabric.base]: \u001b[0;36miter 147 step 73: loss 5.7188, iter time: 122.36ms remaining time: \u001b[0m\n",
      "  3%|\u001b[34m█▎                                      \u001b[0m| 147/4682 [01:02<31:55,  2.37it/s]\u001b[0m2025-06-16 13:55:28 - src.tasks.clm_training.fabric.base - \u001b[0;36mDEBUG\u001b[0m - \u001b[0;36mgradient_accumulation_steps raw value: 2, type: <class 'int'>\u001b[0m\n",
      "[\u001b[0;36mDEBUG\u001b[0m | src.tasks.clm_training.fabric.base]: \u001b[0;36mgradient_accumulation_steps raw value: 2, type: <class 'int'>\u001b[0m\n",
      "2025-06-16 13:55:28 - src.tasks.clm_training.fabric.base - \u001b[0;36mDEBUG\u001b[0m - \u001b[0;36mgradient_accumulation_steps converted: 2, type: <class 'int'>\u001b[0m\n",
      "[\u001b[0;36mDEBUG\u001b[0m | src.tasks.clm_training.fabric.base]: \u001b[0;36mgradient_accumulation_steps converted: 2, type: <class 'int'>\u001b[0m\n",
      "2025-06-16 13:55:28 - src.tasks.clm_training.fabric.base - \u001b[0;36mDEBUG\u001b[0m - \u001b[0;36miter_num value: 147, type: <class 'int'>\u001b[0m\n",
      "[\u001b[0;36mDEBUG\u001b[0m | src.tasks.clm_training.fabric.base]: \u001b[0;36miter_num value: 147, type: <class 'int'>\u001b[0m\n",
      "2025-06-16 13:55:28 - src.tasks.clm_training.fabric.base - \u001b[0;36mDEBUG\u001b[0m - \u001b[0;36mAbout to check: (iter_num + 1) % gradient_accumulation_steps != 0 -> (147 + 1) % 2 != 0\u001b[0m\n",
      "[\u001b[0;36mDEBUG\u001b[0m | src.tasks.clm_training.fabric.base]: \u001b[0;36mAbout to check: (iter_num + 1) % gradient_accumulation_steps != 0 -> (147 + 1) % 2 != 0\u001b[0m\n",
      "2025-06-16 13:55:28 - src.tasks.clm_training.fabric.base - \u001b[0;36mDEBUG\u001b[0m - \u001b[0;36mGradient norm before clipping: 22.3750\u001b[0m\n",
      "[\u001b[0;36mDEBUG\u001b[0m | src.tasks.clm_training.fabric.base]: \u001b[0;36mGradient norm before clipping: 22.3750\u001b[0m\n",
      "2025-06-16 13:55:28 - src.tasks.clm_training.fabric.base - \u001b[0;36mDEBUG\u001b[0m - \u001b[0;36mvalidate_after_k_steps value: 1000, type: <class 'int'>\u001b[0m\n",
      "[\u001b[0;36mDEBUG\u001b[0m | src.tasks.clm_training.fabric.base]: \u001b[0;36mvalidate_after_k_steps value: 1000, type: <class 'int'>\u001b[0m\n",
      "2025-06-16 13:55:28 - src.tasks.clm_training.fabric.base - \u001b[0;36mDEBUG\u001b[0m - \u001b[0;36mstep_count value: 74, type: <class 'int'>\u001b[0m\n",
      "[\u001b[0;36mDEBUG\u001b[0m | src.tasks.clm_training.fabric.base]: \u001b[0;36mstep_count value: 74, type: <class 'int'>\u001b[0m\n",
      "2025-06-16 13:55:28 - src.tasks.clm_training.fabric.base - \u001b[0;36mDEBUG\u001b[0m - \u001b[0;36mValidation condition check - epochFinished: False, trainingFinished: False\u001b[0m\n",
      "[\u001b[0;36mDEBUG\u001b[0m | src.tasks.clm_training.fabric.base]: \u001b[0;36mValidation condition check - epochFinished: False, trainingFinished: False\u001b[0m\n",
      "2025-06-16 13:55:28 - src.tasks.clm_training.fabric.base - \u001b[0;36mDEBUG\u001b[0m - \u001b[0;36mAbout to check: step_count % validate_after_k_steps == 0 -> 74 % 1000 == 0\u001b[0m\n",
      "[\u001b[0;36mDEBUG\u001b[0m | src.tasks.clm_training.fabric.base]: \u001b[0;36mAbout to check: step_count % validate_after_k_steps == 0 -> 74 % 1000 == 0\u001b[0m\n",
      "2025-06-16 13:55:28 - src.tasks.clm_training.fabric.base - \u001b[0;36mDEBUG\u001b[0m - \u001b[0;36miter 148 step 74: loss 5.9062, iter time: 420.34ms remaining time: \u001b[0m\n",
      "[\u001b[0;36mDEBUG\u001b[0m | src.tasks.clm_training.fabric.base]: \u001b[0;36miter 148 step 74: loss 5.9062, iter time: 420.34ms remaining time: \u001b[0m\n",
      "  3%|\u001b[34m█▎                                      \u001b[0m| 148/4682 [01:02<32:05,  2.35it/s]\u001b[0m2025-06-16 13:55:28 - src.tasks.clm_training.fabric.base - \u001b[0;36mDEBUG\u001b[0m - \u001b[0;36mgradient_accumulation_steps raw value: 2, type: <class 'int'>\u001b[0m\n",
      "[\u001b[0;36mDEBUG\u001b[0m | src.tasks.clm_training.fabric.base]: \u001b[0;36mgradient_accumulation_steps raw value: 2, type: <class 'int'>\u001b[0m\n",
      "2025-06-16 13:55:28 - src.tasks.clm_training.fabric.base - \u001b[0;36mDEBUG\u001b[0m - \u001b[0;36mgradient_accumulation_steps converted: 2, type: <class 'int'>\u001b[0m\n",
      "[\u001b[0;36mDEBUG\u001b[0m | src.tasks.clm_training.fabric.base]: \u001b[0;36mgradient_accumulation_steps converted: 2, type: <class 'int'>\u001b[0m\n",
      "2025-06-16 13:55:28 - src.tasks.clm_training.fabric.base - \u001b[0;36mDEBUG\u001b[0m - \u001b[0;36miter_num value: 148, type: <class 'int'>\u001b[0m\n",
      "[\u001b[0;36mDEBUG\u001b[0m | src.tasks.clm_training.fabric.base]: \u001b[0;36miter_num value: 148, type: <class 'int'>\u001b[0m\n",
      "2025-06-16 13:55:28 - src.tasks.clm_training.fabric.base - \u001b[0;36mDEBUG\u001b[0m - \u001b[0;36mAbout to check: (iter_num + 1) % gradient_accumulation_steps != 0 -> (148 + 1) % 2 != 0\u001b[0m\n",
      "[\u001b[0;36mDEBUG\u001b[0m | src.tasks.clm_training.fabric.base]: \u001b[0;36mAbout to check: (iter_num + 1) % gradient_accumulation_steps != 0 -> (148 + 1) % 2 != 0\u001b[0m\n",
      "2025-06-16 13:55:29 - src.tasks.clm_training.fabric.base - \u001b[0;36mDEBUG\u001b[0m - \u001b[0;36miter 149 step 74: loss 5.7188, iter time: 122.39ms remaining time: \u001b[0m\n",
      "[\u001b[0;36mDEBUG\u001b[0m | src.tasks.clm_training.fabric.base]: \u001b[0;36miter 149 step 74: loss 5.7188, iter time: 122.39ms remaining time: \u001b[0m\n",
      "  3%|\u001b[34m█▎                                      \u001b[0m| 149/4682 [01:03<31:53,  2.37it/s]\u001b[0m2025-06-16 13:55:29 - src.tasks.clm_training.fabric.base - \u001b[0;36mDEBUG\u001b[0m - \u001b[0;36mgradient_accumulation_steps raw value: 2, type: <class 'int'>\u001b[0m\n",
      "[\u001b[0;36mDEBUG\u001b[0m | src.tasks.clm_training.fabric.base]: \u001b[0;36mgradient_accumulation_steps raw value: 2, type: <class 'int'>\u001b[0m\n",
      "2025-06-16 13:55:29 - src.tasks.clm_training.fabric.base - \u001b[0;36mDEBUG\u001b[0m - \u001b[0;36mgradient_accumulation_steps converted: 2, type: <class 'int'>\u001b[0m\n",
      "[\u001b[0;36mDEBUG\u001b[0m | src.tasks.clm_training.fabric.base]: \u001b[0;36mgradient_accumulation_steps converted: 2, type: <class 'int'>\u001b[0m\n",
      "2025-06-16 13:55:29 - src.tasks.clm_training.fabric.base - \u001b[0;36mDEBUG\u001b[0m - \u001b[0;36miter_num value: 149, type: <class 'int'>\u001b[0m\n",
      "[\u001b[0;36mDEBUG\u001b[0m | src.tasks.clm_training.fabric.base]: \u001b[0;36miter_num value: 149, type: <class 'int'>\u001b[0m\n",
      "2025-06-16 13:55:29 - src.tasks.clm_training.fabric.base - \u001b[0;36mDEBUG\u001b[0m - \u001b[0;36mAbout to check: (iter_num + 1) % gradient_accumulation_steps != 0 -> (149 + 1) % 2 != 0\u001b[0m\n",
      "[\u001b[0;36mDEBUG\u001b[0m | src.tasks.clm_training.fabric.base]: \u001b[0;36mAbout to check: (iter_num + 1) % gradient_accumulation_steps != 0 -> (149 + 1) % 2 != 0\u001b[0m\n",
      "2025-06-16 13:55:29 - src.tasks.clm_training.fabric.base - \u001b[0;36mDEBUG\u001b[0m - \u001b[0;36mGradient norm before clipping: 26.7500\u001b[0m\n",
      "[\u001b[0;36mDEBUG\u001b[0m | src.tasks.clm_training.fabric.base]: \u001b[0;36mGradient norm before clipping: 26.7500\u001b[0m\n",
      "2025-06-16 13:55:29 - src.tasks.clm_training.fabric.base - \u001b[0;36mDEBUG\u001b[0m - \u001b[0;36mvalidate_after_k_steps value: 1000, type: <class 'int'>\u001b[0m\n",
      "[\u001b[0;36mDEBUG\u001b[0m | src.tasks.clm_training.fabric.base]: \u001b[0;36mvalidate_after_k_steps value: 1000, type: <class 'int'>\u001b[0m\n",
      "2025-06-16 13:55:29 - src.tasks.clm_training.fabric.base - \u001b[0;36mDEBUG\u001b[0m - \u001b[0;36mstep_count value: 75, type: <class 'int'>\u001b[0m\n",
      "[\u001b[0;36mDEBUG\u001b[0m | src.tasks.clm_training.fabric.base]: \u001b[0;36mstep_count value: 75, type: <class 'int'>\u001b[0m\n",
      "2025-06-16 13:55:29 - src.tasks.clm_training.fabric.base - \u001b[0;36mDEBUG\u001b[0m - \u001b[0;36mValidation condition check - epochFinished: False, trainingFinished: False\u001b[0m\n",
      "[\u001b[0;36mDEBUG\u001b[0m | src.tasks.clm_training.fabric.base]: \u001b[0;36mValidation condition check - epochFinished: False, trainingFinished: False\u001b[0m\n",
      "2025-06-16 13:55:29 - src.tasks.clm_training.fabric.base - \u001b[0;36mDEBUG\u001b[0m - \u001b[0;36mAbout to check: step_count % validate_after_k_steps == 0 -> 75 % 1000 == 0\u001b[0m\n",
      "[\u001b[0;36mDEBUG\u001b[0m | src.tasks.clm_training.fabric.base]: \u001b[0;36mAbout to check: step_count % validate_after_k_steps == 0 -> 75 % 1000 == 0\u001b[0m\n",
      "2025-06-16 13:55:29 - src.tasks.clm_training.fabric.base - \u001b[0;36mDEBUG\u001b[0m - \u001b[0;36miter 150 step 75: loss 5.8125, iter time: 420.63ms remaining time: \u001b[0m\n",
      "[\u001b[0;36mDEBUG\u001b[0m | src.tasks.clm_training.fabric.base]: \u001b[0;36miter 150 step 75: loss 5.8125, iter time: 420.63ms remaining time: \u001b[0m\n",
      "  3%|\u001b[34m█▎                                      \u001b[0m| 150/4682 [01:03<32:04,  2.36it/s]\u001b[0m2025-06-16 13:55:29 - src.tasks.clm_training.fabric.base - \u001b[0;36mDEBUG\u001b[0m - \u001b[0;36mgradient_accumulation_steps raw value: 2, type: <class 'int'>\u001b[0m\n",
      "[\u001b[0;36mDEBUG\u001b[0m | src.tasks.clm_training.fabric.base]: \u001b[0;36mgradient_accumulation_steps raw value: 2, type: <class 'int'>\u001b[0m\n",
      "2025-06-16 13:55:29 - src.tasks.clm_training.fabric.base - \u001b[0;36mDEBUG\u001b[0m - \u001b[0;36mgradient_accumulation_steps converted: 2, type: <class 'int'>\u001b[0m\n",
      "[\u001b[0;36mDEBUG\u001b[0m | src.tasks.clm_training.fabric.base]: \u001b[0;36mgradient_accumulation_steps converted: 2, type: <class 'int'>\u001b[0m\n",
      "2025-06-16 13:55:29 - src.tasks.clm_training.fabric.base - \u001b[0;36mDEBUG\u001b[0m - \u001b[0;36miter_num value: 150, type: <class 'int'>\u001b[0m\n",
      "[\u001b[0;36mDEBUG\u001b[0m | src.tasks.clm_training.fabric.base]: \u001b[0;36miter_num value: 150, type: <class 'int'>\u001b[0m\n",
      "2025-06-16 13:55:29 - src.tasks.clm_training.fabric.base - \u001b[0;36mDEBUG\u001b[0m - \u001b[0;36mAbout to check: (iter_num + 1) % gradient_accumulation_steps != 0 -> (150 + 1) % 2 != 0\u001b[0m\n",
      "[\u001b[0;36mDEBUG\u001b[0m | src.tasks.clm_training.fabric.base]: \u001b[0;36mAbout to check: (iter_num + 1) % gradient_accumulation_steps != 0 -> (150 + 1) % 2 != 0\u001b[0m\n",
      "2025-06-16 13:55:30 - src.tasks.clm_training.fabric.base - \u001b[0;36mDEBUG\u001b[0m - \u001b[0;36miter 151 step 75: loss 6.0000, iter time: 122.52ms remaining time: \u001b[0m\n",
      "[\u001b[0;36mDEBUG\u001b[0m | src.tasks.clm_training.fabric.base]: \u001b[0;36miter 151 step 75: loss 6.0000, iter time: 122.52ms remaining time: \u001b[0m\n",
      "  3%|\u001b[34m█▎                                      \u001b[0m| 151/4682 [01:04<31:52,  2.37it/s]\u001b[0m2025-06-16 13:55:30 - src.tasks.clm_training.fabric.base - \u001b[0;36mDEBUG\u001b[0m - \u001b[0;36mgradient_accumulation_steps raw value: 2, type: <class 'int'>\u001b[0m\n",
      "[\u001b[0;36mDEBUG\u001b[0m | src.tasks.clm_training.fabric.base]: \u001b[0;36mgradient_accumulation_steps raw value: 2, type: <class 'int'>\u001b[0m\n",
      "2025-06-16 13:55:30 - src.tasks.clm_training.fabric.base - \u001b[0;36mDEBUG\u001b[0m - \u001b[0;36mgradient_accumulation_steps converted: 2, type: <class 'int'>\u001b[0m\n",
      "[\u001b[0;36mDEBUG\u001b[0m | src.tasks.clm_training.fabric.base]: \u001b[0;36mgradient_accumulation_steps converted: 2, type: <class 'int'>\u001b[0m\n",
      "2025-06-16 13:55:30 - src.tasks.clm_training.fabric.base - \u001b[0;36mDEBUG\u001b[0m - \u001b[0;36miter_num value: 151, type: <class 'int'>\u001b[0m\n",
      "[\u001b[0;36mDEBUG\u001b[0m | src.tasks.clm_training.fabric.base]: \u001b[0;36miter_num value: 151, type: <class 'int'>\u001b[0m\n",
      "2025-06-16 13:55:30 - src.tasks.clm_training.fabric.base - \u001b[0;36mDEBUG\u001b[0m - \u001b[0;36mAbout to check: (iter_num + 1) % gradient_accumulation_steps != 0 -> (151 + 1) % 2 != 0\u001b[0m\n",
      "[\u001b[0;36mDEBUG\u001b[0m | src.tasks.clm_training.fabric.base]: \u001b[0;36mAbout to check: (iter_num + 1) % gradient_accumulation_steps != 0 -> (151 + 1) % 2 != 0\u001b[0m\n",
      "2025-06-16 13:55:30 - src.tasks.clm_training.fabric.base - \u001b[0;36mDEBUG\u001b[0m - \u001b[0;36mGradient norm before clipping: 24.5000\u001b[0m\n",
      "[\u001b[0;36mDEBUG\u001b[0m | src.tasks.clm_training.fabric.base]: \u001b[0;36mGradient norm before clipping: 24.5000\u001b[0m\n",
      "2025-06-16 13:55:30 - src.tasks.clm_training.fabric.base - \u001b[0;36mDEBUG\u001b[0m - \u001b[0;36mvalidate_after_k_steps value: 1000, type: <class 'int'>\u001b[0m\n",
      "[\u001b[0;36mDEBUG\u001b[0m | src.tasks.clm_training.fabric.base]: \u001b[0;36mvalidate_after_k_steps value: 1000, type: <class 'int'>\u001b[0m\n",
      "2025-06-16 13:55:30 - src.tasks.clm_training.fabric.base - \u001b[0;36mDEBUG\u001b[0m - \u001b[0;36mstep_count value: 76, type: <class 'int'>\u001b[0m\n",
      "[\u001b[0;36mDEBUG\u001b[0m | src.tasks.clm_training.fabric.base]: \u001b[0;36mstep_count value: 76, type: <class 'int'>\u001b[0m\n",
      "2025-06-16 13:55:30 - src.tasks.clm_training.fabric.base - \u001b[0;36mDEBUG\u001b[0m - \u001b[0;36mValidation condition check - epochFinished: False, trainingFinished: False\u001b[0m\n",
      "[\u001b[0;36mDEBUG\u001b[0m | src.tasks.clm_training.fabric.base]: \u001b[0;36mValidation condition check - epochFinished: False, trainingFinished: False\u001b[0m\n",
      "2025-06-16 13:55:30 - src.tasks.clm_training.fabric.base - \u001b[0;36mDEBUG\u001b[0m - \u001b[0;36mAbout to check: step_count % validate_after_k_steps == 0 -> 76 % 1000 == 0\u001b[0m\n",
      "[\u001b[0;36mDEBUG\u001b[0m | src.tasks.clm_training.fabric.base]: \u001b[0;36mAbout to check: step_count % validate_after_k_steps == 0 -> 76 % 1000 == 0\u001b[0m\n",
      "2025-06-16 13:55:30 - src.tasks.clm_training.fabric.base - \u001b[0;36mDEBUG\u001b[0m - \u001b[0;36miter 152 step 76: loss 6.0312, iter time: 420.41ms remaining time: \u001b[0m\n",
      "[\u001b[0;36mDEBUG\u001b[0m | src.tasks.clm_training.fabric.base]: \u001b[0;36miter 152 step 76: loss 6.0312, iter time: 420.41ms remaining time: \u001b[0m\n",
      "  3%|\u001b[34m█▎                                      \u001b[0m| 152/4682 [01:04<32:03,  2.36it/s]\u001b[0m2025-06-16 13:55:30 - src.tasks.clm_training.fabric.base - \u001b[0;36mDEBUG\u001b[0m - \u001b[0;36mgradient_accumulation_steps raw value: 2, type: <class 'int'>\u001b[0m\n",
      "[\u001b[0;36mDEBUG\u001b[0m | src.tasks.clm_training.fabric.base]: \u001b[0;36mgradient_accumulation_steps raw value: 2, type: <class 'int'>\u001b[0m\n",
      "2025-06-16 13:55:30 - src.tasks.clm_training.fabric.base - \u001b[0;36mDEBUG\u001b[0m - \u001b[0;36mgradient_accumulation_steps converted: 2, type: <class 'int'>\u001b[0m\n",
      "[\u001b[0;36mDEBUG\u001b[0m | src.tasks.clm_training.fabric.base]: \u001b[0;36mgradient_accumulation_steps converted: 2, type: <class 'int'>\u001b[0m\n",
      "2025-06-16 13:55:30 - src.tasks.clm_training.fabric.base - \u001b[0;36mDEBUG\u001b[0m - \u001b[0;36miter_num value: 152, type: <class 'int'>\u001b[0m\n",
      "[\u001b[0;36mDEBUG\u001b[0m | src.tasks.clm_training.fabric.base]: \u001b[0;36miter_num value: 152, type: <class 'int'>\u001b[0m\n",
      "2025-06-16 13:55:30 - src.tasks.clm_training.fabric.base - \u001b[0;36mDEBUG\u001b[0m - \u001b[0;36mAbout to check: (iter_num + 1) % gradient_accumulation_steps != 0 -> (152 + 1) % 2 != 0\u001b[0m\n",
      "[\u001b[0;36mDEBUG\u001b[0m | src.tasks.clm_training.fabric.base]: \u001b[0;36mAbout to check: (iter_num + 1) % gradient_accumulation_steps != 0 -> (152 + 1) % 2 != 0\u001b[0m\n",
      "2025-06-16 13:55:31 - src.tasks.clm_training.fabric.base - \u001b[0;36mDEBUG\u001b[0m - \u001b[0;36miter 153 step 76: loss 5.8125, iter time: 122.52ms remaining time: \u001b[0m\n",
      "[\u001b[0;36mDEBUG\u001b[0m | src.tasks.clm_training.fabric.base]: \u001b[0;36miter 153 step 76: loss 5.8125, iter time: 122.52ms remaining time: \u001b[0m\n",
      "  3%|\u001b[34m█▎                                      \u001b[0m| 153/4682 [01:04<31:52,  2.37it/s]\u001b[0m2025-06-16 13:55:31 - src.tasks.clm_training.fabric.base - \u001b[0;36mDEBUG\u001b[0m - \u001b[0;36mgradient_accumulation_steps raw value: 2, type: <class 'int'>\u001b[0m\n",
      "[\u001b[0;36mDEBUG\u001b[0m | src.tasks.clm_training.fabric.base]: \u001b[0;36mgradient_accumulation_steps raw value: 2, type: <class 'int'>\u001b[0m\n",
      "2025-06-16 13:55:31 - src.tasks.clm_training.fabric.base - \u001b[0;36mDEBUG\u001b[0m - \u001b[0;36mgradient_accumulation_steps converted: 2, type: <class 'int'>\u001b[0m\n",
      "[\u001b[0;36mDEBUG\u001b[0m | src.tasks.clm_training.fabric.base]: \u001b[0;36mgradient_accumulation_steps converted: 2, type: <class 'int'>\u001b[0m\n",
      "2025-06-16 13:55:31 - src.tasks.clm_training.fabric.base - \u001b[0;36mDEBUG\u001b[0m - \u001b[0;36miter_num value: 153, type: <class 'int'>\u001b[0m\n",
      "[\u001b[0;36mDEBUG\u001b[0m | src.tasks.clm_training.fabric.base]: \u001b[0;36miter_num value: 153, type: <class 'int'>\u001b[0m\n",
      "2025-06-16 13:55:31 - src.tasks.clm_training.fabric.base - \u001b[0;36mDEBUG\u001b[0m - \u001b[0;36mAbout to check: (iter_num + 1) % gradient_accumulation_steps != 0 -> (153 + 1) % 2 != 0\u001b[0m\n",
      "[\u001b[0;36mDEBUG\u001b[0m | src.tasks.clm_training.fabric.base]: \u001b[0;36mAbout to check: (iter_num + 1) % gradient_accumulation_steps != 0 -> (153 + 1) % 2 != 0\u001b[0m\n",
      "2025-06-16 13:55:31 - src.tasks.clm_training.fabric.base - \u001b[0;36mDEBUG\u001b[0m - \u001b[0;36mGradient norm before clipping: 26.0000\u001b[0m\n",
      "[\u001b[0;36mDEBUG\u001b[0m | src.tasks.clm_training.fabric.base]: \u001b[0;36mGradient norm before clipping: 26.0000\u001b[0m\n",
      "2025-06-16 13:55:31 - src.tasks.clm_training.fabric.base - \u001b[0;36mDEBUG\u001b[0m - \u001b[0;36mvalidate_after_k_steps value: 1000, type: <class 'int'>\u001b[0m\n",
      "[\u001b[0;36mDEBUG\u001b[0m | src.tasks.clm_training.fabric.base]: \u001b[0;36mvalidate_after_k_steps value: 1000, type: <class 'int'>\u001b[0m\n",
      "2025-06-16 13:55:31 - src.tasks.clm_training.fabric.base - \u001b[0;36mDEBUG\u001b[0m - \u001b[0;36mstep_count value: 77, type: <class 'int'>\u001b[0m\n",
      "[\u001b[0;36mDEBUG\u001b[0m | src.tasks.clm_training.fabric.base]: \u001b[0;36mstep_count value: 77, type: <class 'int'>\u001b[0m\n",
      "2025-06-16 13:55:31 - src.tasks.clm_training.fabric.base - \u001b[0;36mDEBUG\u001b[0m - \u001b[0;36mValidation condition check - epochFinished: False, trainingFinished: False\u001b[0m\n",
      "[\u001b[0;36mDEBUG\u001b[0m | src.tasks.clm_training.fabric.base]: \u001b[0;36mValidation condition check - epochFinished: False, trainingFinished: False\u001b[0m\n",
      "2025-06-16 13:55:31 - src.tasks.clm_training.fabric.base - \u001b[0;36mDEBUG\u001b[0m - \u001b[0;36mAbout to check: step_count % validate_after_k_steps == 0 -> 77 % 1000 == 0\u001b[0m\n",
      "[\u001b[0;36mDEBUG\u001b[0m | src.tasks.clm_training.fabric.base]: \u001b[0;36mAbout to check: step_count % validate_after_k_steps == 0 -> 77 % 1000 == 0\u001b[0m\n",
      "2025-06-16 13:55:31 - src.tasks.clm_training.fabric.base - \u001b[0;36mDEBUG\u001b[0m - \u001b[0;36miter 154 step 77: loss 5.8125, iter time: 419.90ms remaining time: \u001b[0m\n",
      "[\u001b[0;36mDEBUG\u001b[0m | src.tasks.clm_training.fabric.base]: \u001b[0;36miter 154 step 77: loss 5.8125, iter time: 419.90ms remaining time: \u001b[0m\n",
      "  3%|\u001b[34m█▎                                      \u001b[0m| 154/4682 [01:05<32:02,  2.36it/s]\u001b[0m2025-06-16 13:55:31 - src.tasks.clm_training.fabric.base - \u001b[0;36mDEBUG\u001b[0m - \u001b[0;36mgradient_accumulation_steps raw value: 2, type: <class 'int'>\u001b[0m\n",
      "[\u001b[0;36mDEBUG\u001b[0m | src.tasks.clm_training.fabric.base]: \u001b[0;36mgradient_accumulation_steps raw value: 2, type: <class 'int'>\u001b[0m\n",
      "2025-06-16 13:55:31 - src.tasks.clm_training.fabric.base - \u001b[0;36mDEBUG\u001b[0m - \u001b[0;36mgradient_accumulation_steps converted: 2, type: <class 'int'>\u001b[0m\n",
      "[\u001b[0;36mDEBUG\u001b[0m | src.tasks.clm_training.fabric.base]: \u001b[0;36mgradient_accumulation_steps converted: 2, type: <class 'int'>\u001b[0m\n",
      "2025-06-16 13:55:31 - src.tasks.clm_training.fabric.base - \u001b[0;36mDEBUG\u001b[0m - \u001b[0;36miter_num value: 154, type: <class 'int'>\u001b[0m\n",
      "[\u001b[0;36mDEBUG\u001b[0m | src.tasks.clm_training.fabric.base]: \u001b[0;36miter_num value: 154, type: <class 'int'>\u001b[0m\n",
      "2025-06-16 13:55:31 - src.tasks.clm_training.fabric.base - \u001b[0;36mDEBUG\u001b[0m - \u001b[0;36mAbout to check: (iter_num + 1) % gradient_accumulation_steps != 0 -> (154 + 1) % 2 != 0\u001b[0m\n",
      "[\u001b[0;36mDEBUG\u001b[0m | src.tasks.clm_training.fabric.base]: \u001b[0;36mAbout to check: (iter_num + 1) % gradient_accumulation_steps != 0 -> (154 + 1) % 2 != 0\u001b[0m\n",
      "2025-06-16 13:55:31 - src.tasks.clm_training.fabric.base - \u001b[0;36mDEBUG\u001b[0m - \u001b[0;36miter 155 step 77: loss 6.0938, iter time: 122.22ms remaining time: \u001b[0m\n",
      "[\u001b[0;36mDEBUG\u001b[0m | src.tasks.clm_training.fabric.base]: \u001b[0;36miter 155 step 77: loss 6.0938, iter time: 122.22ms remaining time: \u001b[0m\n",
      "  3%|\u001b[34m█▎                                      \u001b[0m| 155/4682 [01:05<31:50,  2.37it/s]\u001b[0m2025-06-16 13:55:31 - src.tasks.clm_training.fabric.base - \u001b[0;36mDEBUG\u001b[0m - \u001b[0;36mgradient_accumulation_steps raw value: 2, type: <class 'int'>\u001b[0m\n",
      "[\u001b[0;36mDEBUG\u001b[0m | src.tasks.clm_training.fabric.base]: \u001b[0;36mgradient_accumulation_steps raw value: 2, type: <class 'int'>\u001b[0m\n",
      "2025-06-16 13:55:31 - src.tasks.clm_training.fabric.base - \u001b[0;36mDEBUG\u001b[0m - \u001b[0;36mgradient_accumulation_steps converted: 2, type: <class 'int'>\u001b[0m\n",
      "[\u001b[0;36mDEBUG\u001b[0m | src.tasks.clm_training.fabric.base]: \u001b[0;36mgradient_accumulation_steps converted: 2, type: <class 'int'>\u001b[0m\n",
      "2025-06-16 13:55:31 - src.tasks.clm_training.fabric.base - \u001b[0;36mDEBUG\u001b[0m - \u001b[0;36miter_num value: 155, type: <class 'int'>\u001b[0m\n",
      "[\u001b[0;36mDEBUG\u001b[0m | src.tasks.clm_training.fabric.base]: \u001b[0;36miter_num value: 155, type: <class 'int'>\u001b[0m\n",
      "2025-06-16 13:55:31 - src.tasks.clm_training.fabric.base - \u001b[0;36mDEBUG\u001b[0m - \u001b[0;36mAbout to check: (iter_num + 1) % gradient_accumulation_steps != 0 -> (155 + 1) % 2 != 0\u001b[0m\n",
      "[\u001b[0;36mDEBUG\u001b[0m | src.tasks.clm_training.fabric.base]: \u001b[0;36mAbout to check: (iter_num + 1) % gradient_accumulation_steps != 0 -> (155 + 1) % 2 != 0\u001b[0m\n",
      "2025-06-16 13:55:32 - src.tasks.clm_training.fabric.base - \u001b[0;36mDEBUG\u001b[0m - \u001b[0;36mGradient norm before clipping: 24.5000\u001b[0m\n",
      "[\u001b[0;36mDEBUG\u001b[0m | src.tasks.clm_training.fabric.base]: \u001b[0;36mGradient norm before clipping: 24.5000\u001b[0m\n",
      "2025-06-16 13:55:32 - src.tasks.clm_training.fabric.base - \u001b[0;36mDEBUG\u001b[0m - \u001b[0;36mvalidate_after_k_steps value: 1000, type: <class 'int'>\u001b[0m\n",
      "[\u001b[0;36mDEBUG\u001b[0m | src.tasks.clm_training.fabric.base]: \u001b[0;36mvalidate_after_k_steps value: 1000, type: <class 'int'>\u001b[0m\n",
      "2025-06-16 13:55:32 - src.tasks.clm_training.fabric.base - \u001b[0;36mDEBUG\u001b[0m - \u001b[0;36mstep_count value: 78, type: <class 'int'>\u001b[0m\n",
      "[\u001b[0;36mDEBUG\u001b[0m | src.tasks.clm_training.fabric.base]: \u001b[0;36mstep_count value: 78, type: <class 'int'>\u001b[0m\n",
      "2025-06-16 13:55:32 - src.tasks.clm_training.fabric.base - \u001b[0;36mDEBUG\u001b[0m - \u001b[0;36mValidation condition check - epochFinished: False, trainingFinished: False\u001b[0m\n",
      "[\u001b[0;36mDEBUG\u001b[0m | src.tasks.clm_training.fabric.base]: \u001b[0;36mValidation condition check - epochFinished: False, trainingFinished: False\u001b[0m\n",
      "2025-06-16 13:55:32 - src.tasks.clm_training.fabric.base - \u001b[0;36mDEBUG\u001b[0m - \u001b[0;36mAbout to check: step_count % validate_after_k_steps == 0 -> 78 % 1000 == 0\u001b[0m\n",
      "[\u001b[0;36mDEBUG\u001b[0m | src.tasks.clm_training.fabric.base]: \u001b[0;36mAbout to check: step_count % validate_after_k_steps == 0 -> 78 % 1000 == 0\u001b[0m\n",
      "2025-06-16 13:55:32 - src.tasks.clm_training.fabric.base - \u001b[0;36mDEBUG\u001b[0m - \u001b[0;36miter 156 step 78: loss 5.7500, iter time: 420.78ms remaining time: \u001b[0m\n",
      "[\u001b[0;36mDEBUG\u001b[0m | src.tasks.clm_training.fabric.base]: \u001b[0;36miter 156 step 78: loss 5.7500, iter time: 420.78ms remaining time: \u001b[0m\n",
      "  3%|\u001b[34m█▎                                      \u001b[0m| 156/4682 [01:06<32:01,  2.35it/s]\u001b[0m2025-06-16 13:55:32 - src.tasks.clm_training.fabric.base - \u001b[0;36mDEBUG\u001b[0m - \u001b[0;36mgradient_accumulation_steps raw value: 2, type: <class 'int'>\u001b[0m\n",
      "[\u001b[0;36mDEBUG\u001b[0m | src.tasks.clm_training.fabric.base]: \u001b[0;36mgradient_accumulation_steps raw value: 2, type: <class 'int'>\u001b[0m\n",
      "2025-06-16 13:55:32 - src.tasks.clm_training.fabric.base - \u001b[0;36mDEBUG\u001b[0m - \u001b[0;36mgradient_accumulation_steps converted: 2, type: <class 'int'>\u001b[0m\n",
      "[\u001b[0;36mDEBUG\u001b[0m | src.tasks.clm_training.fabric.base]: \u001b[0;36mgradient_accumulation_steps converted: 2, type: <class 'int'>\u001b[0m\n",
      "2025-06-16 13:55:32 - src.tasks.clm_training.fabric.base - \u001b[0;36mDEBUG\u001b[0m - \u001b[0;36miter_num value: 156, type: <class 'int'>\u001b[0m\n",
      "[\u001b[0;36mDEBUG\u001b[0m | src.tasks.clm_training.fabric.base]: \u001b[0;36miter_num value: 156, type: <class 'int'>\u001b[0m\n",
      "2025-06-16 13:55:32 - src.tasks.clm_training.fabric.base - \u001b[0;36mDEBUG\u001b[0m - \u001b[0;36mAbout to check: (iter_num + 1) % gradient_accumulation_steps != 0 -> (156 + 1) % 2 != 0\u001b[0m\n",
      "[\u001b[0;36mDEBUG\u001b[0m | src.tasks.clm_training.fabric.base]: \u001b[0;36mAbout to check: (iter_num + 1) % gradient_accumulation_steps != 0 -> (156 + 1) % 2 != 0\u001b[0m\n",
      "2025-06-16 13:55:32 - src.tasks.clm_training.fabric.base - \u001b[0;36mDEBUG\u001b[0m - \u001b[0;36miter 157 step 78: loss 6.1250, iter time: 122.33ms remaining time: \u001b[0m\n",
      "[\u001b[0;36mDEBUG\u001b[0m | src.tasks.clm_training.fabric.base]: \u001b[0;36miter 157 step 78: loss 6.1250, iter time: 122.33ms remaining time: \u001b[0m\n",
      "  3%|\u001b[34m█▎                                      \u001b[0m| 157/4682 [01:06<31:48,  2.37it/s]\u001b[0m2025-06-16 13:55:32 - src.tasks.clm_training.fabric.base - \u001b[0;36mDEBUG\u001b[0m - \u001b[0;36mgradient_accumulation_steps raw value: 2, type: <class 'int'>\u001b[0m\n",
      "[\u001b[0;36mDEBUG\u001b[0m | src.tasks.clm_training.fabric.base]: \u001b[0;36mgradient_accumulation_steps raw value: 2, type: <class 'int'>\u001b[0m\n",
      "2025-06-16 13:55:32 - src.tasks.clm_training.fabric.base - \u001b[0;36mDEBUG\u001b[0m - \u001b[0;36mgradient_accumulation_steps converted: 2, type: <class 'int'>\u001b[0m\n",
      "[\u001b[0;36mDEBUG\u001b[0m | src.tasks.clm_training.fabric.base]: \u001b[0;36mgradient_accumulation_steps converted: 2, type: <class 'int'>\u001b[0m\n",
      "2025-06-16 13:55:32 - src.tasks.clm_training.fabric.base - \u001b[0;36mDEBUG\u001b[0m - \u001b[0;36miter_num value: 157, type: <class 'int'>\u001b[0m\n",
      "[\u001b[0;36mDEBUG\u001b[0m | src.tasks.clm_training.fabric.base]: \u001b[0;36miter_num value: 157, type: <class 'int'>\u001b[0m\n",
      "2025-06-16 13:55:32 - src.tasks.clm_training.fabric.base - \u001b[0;36mDEBUG\u001b[0m - \u001b[0;36mAbout to check: (iter_num + 1) % gradient_accumulation_steps != 0 -> (157 + 1) % 2 != 0\u001b[0m\n",
      "[\u001b[0;36mDEBUG\u001b[0m | src.tasks.clm_training.fabric.base]: \u001b[0;36mAbout to check: (iter_num + 1) % gradient_accumulation_steps != 0 -> (157 + 1) % 2 != 0\u001b[0m\n",
      "2025-06-16 13:55:33 - src.tasks.clm_training.fabric.base - \u001b[0;36mDEBUG\u001b[0m - \u001b[0;36mGradient norm before clipping: 22.1250\u001b[0m\n",
      "[\u001b[0;36mDEBUG\u001b[0m | src.tasks.clm_training.fabric.base]: \u001b[0;36mGradient norm before clipping: 22.1250\u001b[0m\n",
      "2025-06-16 13:55:33 - src.tasks.clm_training.fabric.base - \u001b[0;36mDEBUG\u001b[0m - \u001b[0;36mvalidate_after_k_steps value: 1000, type: <class 'int'>\u001b[0m\n",
      "[\u001b[0;36mDEBUG\u001b[0m | src.tasks.clm_training.fabric.base]: \u001b[0;36mvalidate_after_k_steps value: 1000, type: <class 'int'>\u001b[0m\n",
      "2025-06-16 13:55:33 - src.tasks.clm_training.fabric.base - \u001b[0;36mDEBUG\u001b[0m - \u001b[0;36mstep_count value: 79, type: <class 'int'>\u001b[0m\n",
      "[\u001b[0;36mDEBUG\u001b[0m | src.tasks.clm_training.fabric.base]: \u001b[0;36mstep_count value: 79, type: <class 'int'>\u001b[0m\n",
      "2025-06-16 13:55:33 - src.tasks.clm_training.fabric.base - \u001b[0;36mDEBUG\u001b[0m - \u001b[0;36mValidation condition check - epochFinished: False, trainingFinished: False\u001b[0m\n",
      "[\u001b[0;36mDEBUG\u001b[0m | src.tasks.clm_training.fabric.base]: \u001b[0;36mValidation condition check - epochFinished: False, trainingFinished: False\u001b[0m\n",
      "2025-06-16 13:55:33 - src.tasks.clm_training.fabric.base - \u001b[0;36mDEBUG\u001b[0m - \u001b[0;36mAbout to check: step_count % validate_after_k_steps == 0 -> 79 % 1000 == 0\u001b[0m\n",
      "[\u001b[0;36mDEBUG\u001b[0m | src.tasks.clm_training.fabric.base]: \u001b[0;36mAbout to check: step_count % validate_after_k_steps == 0 -> 79 % 1000 == 0\u001b[0m\n",
      "2025-06-16 13:55:33 - src.tasks.clm_training.fabric.base - \u001b[0;36mDEBUG\u001b[0m - \u001b[0;36miter 158 step 79: loss 5.4375, iter time: 420.36ms remaining time: \u001b[0m\n",
      "[\u001b[0;36mDEBUG\u001b[0m | src.tasks.clm_training.fabric.base]: \u001b[0;36miter 158 step 79: loss 5.4375, iter time: 420.36ms remaining time: \u001b[0m\n",
      "  3%|\u001b[34m█▎                                      \u001b[0m| 158/4682 [01:07<31:59,  2.36it/s]\u001b[0m2025-06-16 13:55:33 - src.tasks.clm_training.fabric.base - \u001b[0;36mDEBUG\u001b[0m - \u001b[0;36mgradient_accumulation_steps raw value: 2, type: <class 'int'>\u001b[0m\n",
      "[\u001b[0;36mDEBUG\u001b[0m | src.tasks.clm_training.fabric.base]: \u001b[0;36mgradient_accumulation_steps raw value: 2, type: <class 'int'>\u001b[0m\n",
      "2025-06-16 13:55:33 - src.tasks.clm_training.fabric.base - \u001b[0;36mDEBUG\u001b[0m - \u001b[0;36mgradient_accumulation_steps converted: 2, type: <class 'int'>\u001b[0m\n",
      "[\u001b[0;36mDEBUG\u001b[0m | src.tasks.clm_training.fabric.base]: \u001b[0;36mgradient_accumulation_steps converted: 2, type: <class 'int'>\u001b[0m\n",
      "2025-06-16 13:55:33 - src.tasks.clm_training.fabric.base - \u001b[0;36mDEBUG\u001b[0m - \u001b[0;36miter_num value: 158, type: <class 'int'>\u001b[0m\n",
      "[\u001b[0;36mDEBUG\u001b[0m | src.tasks.clm_training.fabric.base]: \u001b[0;36miter_num value: 158, type: <class 'int'>\u001b[0m\n",
      "2025-06-16 13:55:33 - src.tasks.clm_training.fabric.base - \u001b[0;36mDEBUG\u001b[0m - \u001b[0;36mAbout to check: (iter_num + 1) % gradient_accumulation_steps != 0 -> (158 + 1) % 2 != 0\u001b[0m\n",
      "[\u001b[0;36mDEBUG\u001b[0m | src.tasks.clm_training.fabric.base]: \u001b[0;36mAbout to check: (iter_num + 1) % gradient_accumulation_steps != 0 -> (158 + 1) % 2 != 0\u001b[0m\n",
      "2025-06-16 13:55:33 - src.tasks.clm_training.fabric.base - \u001b[0;36mDEBUG\u001b[0m - \u001b[0;36miter 159 step 79: loss 5.8750, iter time: 122.71ms remaining time: \u001b[0m\n",
      "[\u001b[0;36mDEBUG\u001b[0m | src.tasks.clm_training.fabric.base]: \u001b[0;36miter 159 step 79: loss 5.8750, iter time: 122.71ms remaining time: \u001b[0m\n",
      "  3%|\u001b[34m█▎                                      \u001b[0m| 159/4682 [01:07<31:48,  2.37it/s]\u001b[0m2025-06-16 13:55:33 - src.tasks.clm_training.fabric.base - \u001b[0;36mDEBUG\u001b[0m - \u001b[0;36mgradient_accumulation_steps raw value: 2, type: <class 'int'>\u001b[0m\n",
      "[\u001b[0;36mDEBUG\u001b[0m | src.tasks.clm_training.fabric.base]: \u001b[0;36mgradient_accumulation_steps raw value: 2, type: <class 'int'>\u001b[0m\n",
      "2025-06-16 13:55:33 - src.tasks.clm_training.fabric.base - \u001b[0;36mDEBUG\u001b[0m - \u001b[0;36mgradient_accumulation_steps converted: 2, type: <class 'int'>\u001b[0m\n",
      "[\u001b[0;36mDEBUG\u001b[0m | src.tasks.clm_training.fabric.base]: \u001b[0;36mgradient_accumulation_steps converted: 2, type: <class 'int'>\u001b[0m\n",
      "2025-06-16 13:55:33 - src.tasks.clm_training.fabric.base - \u001b[0;36mDEBUG\u001b[0m - \u001b[0;36miter_num value: 159, type: <class 'int'>\u001b[0m\n",
      "[\u001b[0;36mDEBUG\u001b[0m | src.tasks.clm_training.fabric.base]: \u001b[0;36miter_num value: 159, type: <class 'int'>\u001b[0m\n",
      "2025-06-16 13:55:33 - src.tasks.clm_training.fabric.base - \u001b[0;36mDEBUG\u001b[0m - \u001b[0;36mAbout to check: (iter_num + 1) % gradient_accumulation_steps != 0 -> (159 + 1) % 2 != 0\u001b[0m\n",
      "[\u001b[0;36mDEBUG\u001b[0m | src.tasks.clm_training.fabric.base]: \u001b[0;36mAbout to check: (iter_num + 1) % gradient_accumulation_steps != 0 -> (159 + 1) % 2 != 0\u001b[0m\n",
      "2025-06-16 13:55:33 - src.tasks.clm_training.fabric.base - \u001b[0;36mDEBUG\u001b[0m - \u001b[0;36mGradient norm before clipping: 23.0000\u001b[0m\n",
      "[\u001b[0;36mDEBUG\u001b[0m | src.tasks.clm_training.fabric.base]: \u001b[0;36mGradient norm before clipping: 23.0000\u001b[0m\n",
      "2025-06-16 13:55:33 - src.tasks.clm_training.fabric.base - \u001b[0;36mDEBUG\u001b[0m - \u001b[0;36mvalidate_after_k_steps value: 1000, type: <class 'int'>\u001b[0m\n",
      "[\u001b[0;36mDEBUG\u001b[0m | src.tasks.clm_training.fabric.base]: \u001b[0;36mvalidate_after_k_steps value: 1000, type: <class 'int'>\u001b[0m\n",
      "2025-06-16 13:55:33 - src.tasks.clm_training.fabric.base - \u001b[0;36mDEBUG\u001b[0m - \u001b[0;36mstep_count value: 80, type: <class 'int'>\u001b[0m\n",
      "[\u001b[0;36mDEBUG\u001b[0m | src.tasks.clm_training.fabric.base]: \u001b[0;36mstep_count value: 80, type: <class 'int'>\u001b[0m\n",
      "2025-06-16 13:55:33 - src.tasks.clm_training.fabric.base - \u001b[0;36mDEBUG\u001b[0m - \u001b[0;36mValidation condition check - epochFinished: False, trainingFinished: False\u001b[0m\n",
      "[\u001b[0;36mDEBUG\u001b[0m | src.tasks.clm_training.fabric.base]: \u001b[0;36mValidation condition check - epochFinished: False, trainingFinished: False\u001b[0m\n",
      "2025-06-16 13:55:33 - src.tasks.clm_training.fabric.base - \u001b[0;36mDEBUG\u001b[0m - \u001b[0;36mAbout to check: step_count % validate_after_k_steps == 0 -> 80 % 1000 == 0\u001b[0m\n",
      "[\u001b[0;36mDEBUG\u001b[0m | src.tasks.clm_training.fabric.base]: \u001b[0;36mAbout to check: step_count % validate_after_k_steps == 0 -> 80 % 1000 == 0\u001b[0m\n",
      "2025-06-16 13:55:34 - src.tasks.clm_training.fabric.base - \u001b[0;36mDEBUG\u001b[0m - \u001b[0;36miter 160 step 80: loss 5.4688, iter time: 420.95ms remaining time: \u001b[0m\n",
      "[\u001b[0;36mDEBUG\u001b[0m | src.tasks.clm_training.fabric.base]: \u001b[0;36miter 160 step 80: loss 5.4688, iter time: 420.95ms remaining time: \u001b[0m\n",
      "  3%|\u001b[34m█▎                                      \u001b[0m| 160/4682 [01:07<32:00,  2.35it/s]\u001b[0m2025-06-16 13:55:34 - src.tasks.clm_training.fabric.base - \u001b[0;36mDEBUG\u001b[0m - \u001b[0;36mgradient_accumulation_steps raw value: 2, type: <class 'int'>\u001b[0m\n",
      "[\u001b[0;36mDEBUG\u001b[0m | src.tasks.clm_training.fabric.base]: \u001b[0;36mgradient_accumulation_steps raw value: 2, type: <class 'int'>\u001b[0m\n",
      "2025-06-16 13:55:34 - src.tasks.clm_training.fabric.base - \u001b[0;36mDEBUG\u001b[0m - \u001b[0;36mgradient_accumulation_steps converted: 2, type: <class 'int'>\u001b[0m\n",
      "[\u001b[0;36mDEBUG\u001b[0m | src.tasks.clm_training.fabric.base]: \u001b[0;36mgradient_accumulation_steps converted: 2, type: <class 'int'>\u001b[0m\n",
      "2025-06-16 13:55:34 - src.tasks.clm_training.fabric.base - \u001b[0;36mDEBUG\u001b[0m - \u001b[0;36miter_num value: 160, type: <class 'int'>\u001b[0m\n",
      "[\u001b[0;36mDEBUG\u001b[0m | src.tasks.clm_training.fabric.base]: \u001b[0;36miter_num value: 160, type: <class 'int'>\u001b[0m\n",
      "2025-06-16 13:55:34 - src.tasks.clm_training.fabric.base - \u001b[0;36mDEBUG\u001b[0m - \u001b[0;36mAbout to check: (iter_num + 1) % gradient_accumulation_steps != 0 -> (160 + 1) % 2 != 0\u001b[0m\n",
      "[\u001b[0;36mDEBUG\u001b[0m | src.tasks.clm_training.fabric.base]: \u001b[0;36mAbout to check: (iter_num + 1) % gradient_accumulation_steps != 0 -> (160 + 1) % 2 != 0\u001b[0m\n",
      "2025-06-16 13:55:34 - src.tasks.clm_training.fabric.base - \u001b[0;36mDEBUG\u001b[0m - \u001b[0;36miter 161 step 80: loss 6.0312, iter time: 122.38ms remaining time: \u001b[0m\n",
      "[\u001b[0;36mDEBUG\u001b[0m | src.tasks.clm_training.fabric.base]: \u001b[0;36miter 161 step 80: loss 6.0312, iter time: 122.38ms remaining time: \u001b[0m\n",
      "  3%|\u001b[34m█▍                                      \u001b[0m| 161/4682 [01:08<31:47,  2.37it/s]\u001b[0m2025-06-16 13:55:34 - src.tasks.clm_training.fabric.base - \u001b[0;36mDEBUG\u001b[0m - \u001b[0;36mgradient_accumulation_steps raw value: 2, type: <class 'int'>\u001b[0m\n",
      "[\u001b[0;36mDEBUG\u001b[0m | src.tasks.clm_training.fabric.base]: \u001b[0;36mgradient_accumulation_steps raw value: 2, type: <class 'int'>\u001b[0m\n",
      "2025-06-16 13:55:34 - src.tasks.clm_training.fabric.base - \u001b[0;36mDEBUG\u001b[0m - \u001b[0;36mgradient_accumulation_steps converted: 2, type: <class 'int'>\u001b[0m\n",
      "[\u001b[0;36mDEBUG\u001b[0m | src.tasks.clm_training.fabric.base]: \u001b[0;36mgradient_accumulation_steps converted: 2, type: <class 'int'>\u001b[0m\n",
      "2025-06-16 13:55:34 - src.tasks.clm_training.fabric.base - \u001b[0;36mDEBUG\u001b[0m - \u001b[0;36miter_num value: 161, type: <class 'int'>\u001b[0m\n",
      "[\u001b[0;36mDEBUG\u001b[0m | src.tasks.clm_training.fabric.base]: \u001b[0;36miter_num value: 161, type: <class 'int'>\u001b[0m\n",
      "2025-06-16 13:55:34 - src.tasks.clm_training.fabric.base - \u001b[0;36mDEBUG\u001b[0m - \u001b[0;36mAbout to check: (iter_num + 1) % gradient_accumulation_steps != 0 -> (161 + 1) % 2 != 0\u001b[0m\n",
      "[\u001b[0;36mDEBUG\u001b[0m | src.tasks.clm_training.fabric.base]: \u001b[0;36mAbout to check: (iter_num + 1) % gradient_accumulation_steps != 0 -> (161 + 1) % 2 != 0\u001b[0m\n",
      "2025-06-16 13:55:34 - src.tasks.clm_training.fabric.base - \u001b[0;36mDEBUG\u001b[0m - \u001b[0;36mGradient norm before clipping: 28.7500\u001b[0m\n",
      "[\u001b[0;36mDEBUG\u001b[0m | src.tasks.clm_training.fabric.base]: \u001b[0;36mGradient norm before clipping: 28.7500\u001b[0m\n",
      "2025-06-16 13:55:34 - src.tasks.clm_training.fabric.base - \u001b[0;36mDEBUG\u001b[0m - \u001b[0;36mvalidate_after_k_steps value: 1000, type: <class 'int'>\u001b[0m\n",
      "[\u001b[0;36mDEBUG\u001b[0m | src.tasks.clm_training.fabric.base]: \u001b[0;36mvalidate_after_k_steps value: 1000, type: <class 'int'>\u001b[0m\n",
      "2025-06-16 13:55:34 - src.tasks.clm_training.fabric.base - \u001b[0;36mDEBUG\u001b[0m - \u001b[0;36mstep_count value: 81, type: <class 'int'>\u001b[0m\n",
      "[\u001b[0;36mDEBUG\u001b[0m | src.tasks.clm_training.fabric.base]: \u001b[0;36mstep_count value: 81, type: <class 'int'>\u001b[0m\n",
      "2025-06-16 13:55:34 - src.tasks.clm_training.fabric.base - \u001b[0;36mDEBUG\u001b[0m - \u001b[0;36mValidation condition check - epochFinished: False, trainingFinished: False\u001b[0m\n",
      "[\u001b[0;36mDEBUG\u001b[0m | src.tasks.clm_training.fabric.base]: \u001b[0;36mValidation condition check - epochFinished: False, trainingFinished: False\u001b[0m\n",
      "2025-06-16 13:55:34 - src.tasks.clm_training.fabric.base - \u001b[0;36mDEBUG\u001b[0m - \u001b[0;36mAbout to check: step_count % validate_after_k_steps == 0 -> 81 % 1000 == 0\u001b[0m\n",
      "[\u001b[0;36mDEBUG\u001b[0m | src.tasks.clm_training.fabric.base]: \u001b[0;36mAbout to check: step_count % validate_after_k_steps == 0 -> 81 % 1000 == 0\u001b[0m\n",
      "2025-06-16 13:55:34 - src.tasks.clm_training.fabric.base - \u001b[0;36mDEBUG\u001b[0m - \u001b[0;36miter 162 step 81: loss 5.5312, iter time: 421.00ms remaining time: \u001b[0m\n",
      "[\u001b[0;36mDEBUG\u001b[0m | src.tasks.clm_training.fabric.base]: \u001b[0;36miter 162 step 81: loss 5.5312, iter time: 421.00ms remaining time: \u001b[0m\n",
      "  3%|\u001b[34m█▍                                      \u001b[0m| 162/4682 [01:08<31:59,  2.35it/s]\u001b[0m2025-06-16 13:55:34 - src.tasks.clm_training.fabric.base - \u001b[0;36mDEBUG\u001b[0m - \u001b[0;36mgradient_accumulation_steps raw value: 2, type: <class 'int'>\u001b[0m\n",
      "[\u001b[0;36mDEBUG\u001b[0m | src.tasks.clm_training.fabric.base]: \u001b[0;36mgradient_accumulation_steps raw value: 2, type: <class 'int'>\u001b[0m\n",
      "2025-06-16 13:55:34 - src.tasks.clm_training.fabric.base - \u001b[0;36mDEBUG\u001b[0m - \u001b[0;36mgradient_accumulation_steps converted: 2, type: <class 'int'>\u001b[0m\n",
      "[\u001b[0;36mDEBUG\u001b[0m | src.tasks.clm_training.fabric.base]: \u001b[0;36mgradient_accumulation_steps converted: 2, type: <class 'int'>\u001b[0m\n",
      "2025-06-16 13:55:34 - src.tasks.clm_training.fabric.base - \u001b[0;36mDEBUG\u001b[0m - \u001b[0;36miter_num value: 162, type: <class 'int'>\u001b[0m\n",
      "[\u001b[0;36mDEBUG\u001b[0m | src.tasks.clm_training.fabric.base]: \u001b[0;36miter_num value: 162, type: <class 'int'>\u001b[0m\n",
      "2025-06-16 13:55:34 - src.tasks.clm_training.fabric.base - \u001b[0;36mDEBUG\u001b[0m - \u001b[0;36mAbout to check: (iter_num + 1) % gradient_accumulation_steps != 0 -> (162 + 1) % 2 != 0\u001b[0m\n",
      "[\u001b[0;36mDEBUG\u001b[0m | src.tasks.clm_training.fabric.base]: \u001b[0;36mAbout to check: (iter_num + 1) % gradient_accumulation_steps != 0 -> (162 + 1) % 2 != 0\u001b[0m\n",
      "2025-06-16 13:55:35 - src.tasks.clm_training.fabric.base - \u001b[0;36mDEBUG\u001b[0m - \u001b[0;36miter 163 step 81: loss 5.7812, iter time: 122.45ms remaining time: \u001b[0m\n",
      "[\u001b[0;36mDEBUG\u001b[0m | src.tasks.clm_training.fabric.base]: \u001b[0;36miter 163 step 81: loss 5.7812, iter time: 122.45ms remaining time: \u001b[0m\n",
      "  3%|\u001b[34m█▍                                      \u001b[0m| 163/4682 [01:09<31:47,  2.37it/s]\u001b[0m2025-06-16 13:55:35 - src.tasks.clm_training.fabric.base - \u001b[0;36mDEBUG\u001b[0m - \u001b[0;36mgradient_accumulation_steps raw value: 2, type: <class 'int'>\u001b[0m\n",
      "[\u001b[0;36mDEBUG\u001b[0m | src.tasks.clm_training.fabric.base]: \u001b[0;36mgradient_accumulation_steps raw value: 2, type: <class 'int'>\u001b[0m\n",
      "2025-06-16 13:55:35 - src.tasks.clm_training.fabric.base - \u001b[0;36mDEBUG\u001b[0m - \u001b[0;36mgradient_accumulation_steps converted: 2, type: <class 'int'>\u001b[0m\n",
      "[\u001b[0;36mDEBUG\u001b[0m | src.tasks.clm_training.fabric.base]: \u001b[0;36mgradient_accumulation_steps converted: 2, type: <class 'int'>\u001b[0m\n",
      "2025-06-16 13:55:35 - src.tasks.clm_training.fabric.base - \u001b[0;36mDEBUG\u001b[0m - \u001b[0;36miter_num value: 163, type: <class 'int'>\u001b[0m\n",
      "[\u001b[0;36mDEBUG\u001b[0m | src.tasks.clm_training.fabric.base]: \u001b[0;36miter_num value: 163, type: <class 'int'>\u001b[0m\n",
      "2025-06-16 13:55:35 - src.tasks.clm_training.fabric.base - \u001b[0;36mDEBUG\u001b[0m - \u001b[0;36mAbout to check: (iter_num + 1) % gradient_accumulation_steps != 0 -> (163 + 1) % 2 != 0\u001b[0m\n",
      "[\u001b[0;36mDEBUG\u001b[0m | src.tasks.clm_training.fabric.base]: \u001b[0;36mAbout to check: (iter_num + 1) % gradient_accumulation_steps != 0 -> (163 + 1) % 2 != 0\u001b[0m\n",
      "2025-06-16 13:55:35 - src.tasks.clm_training.fabric.base - \u001b[0;36mDEBUG\u001b[0m - \u001b[0;36mGradient norm before clipping: 25.2500\u001b[0m\n",
      "[\u001b[0;36mDEBUG\u001b[0m | src.tasks.clm_training.fabric.base]: \u001b[0;36mGradient norm before clipping: 25.2500\u001b[0m\n",
      "2025-06-16 13:55:35 - src.tasks.clm_training.fabric.base - \u001b[0;36mDEBUG\u001b[0m - \u001b[0;36mvalidate_after_k_steps value: 1000, type: <class 'int'>\u001b[0m\n",
      "[\u001b[0;36mDEBUG\u001b[0m | src.tasks.clm_training.fabric.base]: \u001b[0;36mvalidate_after_k_steps value: 1000, type: <class 'int'>\u001b[0m\n",
      "2025-06-16 13:55:35 - src.tasks.clm_training.fabric.base - \u001b[0;36mDEBUG\u001b[0m - \u001b[0;36mstep_count value: 82, type: <class 'int'>\u001b[0m\n",
      "[\u001b[0;36mDEBUG\u001b[0m | src.tasks.clm_training.fabric.base]: \u001b[0;36mstep_count value: 82, type: <class 'int'>\u001b[0m\n",
      "2025-06-16 13:55:35 - src.tasks.clm_training.fabric.base - \u001b[0;36mDEBUG\u001b[0m - \u001b[0;36mValidation condition check - epochFinished: False, trainingFinished: False\u001b[0m\n",
      "[\u001b[0;36mDEBUG\u001b[0m | src.tasks.clm_training.fabric.base]: \u001b[0;36mValidation condition check - epochFinished: False, trainingFinished: False\u001b[0m\n",
      "2025-06-16 13:55:35 - src.tasks.clm_training.fabric.base - \u001b[0;36mDEBUG\u001b[0m - \u001b[0;36mAbout to check: step_count % validate_after_k_steps == 0 -> 82 % 1000 == 0\u001b[0m\n",
      "[\u001b[0;36mDEBUG\u001b[0m | src.tasks.clm_training.fabric.base]: \u001b[0;36mAbout to check: step_count % validate_after_k_steps == 0 -> 82 % 1000 == 0\u001b[0m\n",
      "2025-06-16 13:55:35 - src.tasks.clm_training.fabric.base - \u001b[0;36mDEBUG\u001b[0m - \u001b[0;36miter 164 step 82: loss 5.3438, iter time: 420.57ms remaining time: \u001b[0m\n",
      "[\u001b[0;36mDEBUG\u001b[0m | src.tasks.clm_training.fabric.base]: \u001b[0;36miter 164 step 82: loss 5.3438, iter time: 420.57ms remaining time: \u001b[0m\n",
      "  4%|\u001b[34m█▍                                      \u001b[0m| 164/4682 [01:09<31:58,  2.35it/s]\u001b[0m2025-06-16 13:55:35 - src.tasks.clm_training.fabric.base - \u001b[0;36mDEBUG\u001b[0m - \u001b[0;36mgradient_accumulation_steps raw value: 2, type: <class 'int'>\u001b[0m\n",
      "[\u001b[0;36mDEBUG\u001b[0m | src.tasks.clm_training.fabric.base]: \u001b[0;36mgradient_accumulation_steps raw value: 2, type: <class 'int'>\u001b[0m\n",
      "2025-06-16 13:55:35 - src.tasks.clm_training.fabric.base - \u001b[0;36mDEBUG\u001b[0m - \u001b[0;36mgradient_accumulation_steps converted: 2, type: <class 'int'>\u001b[0m\n",
      "[\u001b[0;36mDEBUG\u001b[0m | src.tasks.clm_training.fabric.base]: \u001b[0;36mgradient_accumulation_steps converted: 2, type: <class 'int'>\u001b[0m\n",
      "2025-06-16 13:55:35 - src.tasks.clm_training.fabric.base - \u001b[0;36mDEBUG\u001b[0m - \u001b[0;36miter_num value: 164, type: <class 'int'>\u001b[0m\n",
      "[\u001b[0;36mDEBUG\u001b[0m | src.tasks.clm_training.fabric.base]: \u001b[0;36miter_num value: 164, type: <class 'int'>\u001b[0m\n",
      "2025-06-16 13:55:35 - src.tasks.clm_training.fabric.base - \u001b[0;36mDEBUG\u001b[0m - \u001b[0;36mAbout to check: (iter_num + 1) % gradient_accumulation_steps != 0 -> (164 + 1) % 2 != 0\u001b[0m\n",
      "[\u001b[0;36mDEBUG\u001b[0m | src.tasks.clm_training.fabric.base]: \u001b[0;36mAbout to check: (iter_num + 1) % gradient_accumulation_steps != 0 -> (164 + 1) % 2 != 0\u001b[0m\n",
      "2025-06-16 13:55:36 - src.tasks.clm_training.fabric.base - \u001b[0;36mDEBUG\u001b[0m - \u001b[0;36miter 165 step 82: loss 6.1562, iter time: 122.45ms remaining time: \u001b[0m\n",
      "[\u001b[0;36mDEBUG\u001b[0m | src.tasks.clm_training.fabric.base]: \u001b[0;36miter 165 step 82: loss 6.1562, iter time: 122.45ms remaining time: \u001b[0m\n",
      "  4%|\u001b[34m█▍                                      \u001b[0m| 165/4682 [01:10<31:47,  2.37it/s]\u001b[0m2025-06-16 13:55:36 - src.tasks.clm_training.fabric.base - \u001b[0;36mDEBUG\u001b[0m - \u001b[0;36mgradient_accumulation_steps raw value: 2, type: <class 'int'>\u001b[0m\n",
      "[\u001b[0;36mDEBUG\u001b[0m | src.tasks.clm_training.fabric.base]: \u001b[0;36mgradient_accumulation_steps raw value: 2, type: <class 'int'>\u001b[0m\n",
      "2025-06-16 13:55:36 - src.tasks.clm_training.fabric.base - \u001b[0;36mDEBUG\u001b[0m - \u001b[0;36mgradient_accumulation_steps converted: 2, type: <class 'int'>\u001b[0m\n",
      "[\u001b[0;36mDEBUG\u001b[0m | src.tasks.clm_training.fabric.base]: \u001b[0;36mgradient_accumulation_steps converted: 2, type: <class 'int'>\u001b[0m\n",
      "2025-06-16 13:55:36 - src.tasks.clm_training.fabric.base - \u001b[0;36mDEBUG\u001b[0m - \u001b[0;36miter_num value: 165, type: <class 'int'>\u001b[0m\n",
      "[\u001b[0;36mDEBUG\u001b[0m | src.tasks.clm_training.fabric.base]: \u001b[0;36miter_num value: 165, type: <class 'int'>\u001b[0m\n",
      "2025-06-16 13:55:36 - src.tasks.clm_training.fabric.base - \u001b[0;36mDEBUG\u001b[0m - \u001b[0;36mAbout to check: (iter_num + 1) % gradient_accumulation_steps != 0 -> (165 + 1) % 2 != 0\u001b[0m\n",
      "[\u001b[0;36mDEBUG\u001b[0m | src.tasks.clm_training.fabric.base]: \u001b[0;36mAbout to check: (iter_num + 1) % gradient_accumulation_steps != 0 -> (165 + 1) % 2 != 0\u001b[0m\n",
      "2025-06-16 13:55:36 - src.tasks.clm_training.fabric.base - \u001b[0;36mDEBUG\u001b[0m - \u001b[0;36mGradient norm before clipping: 26.5000\u001b[0m\n",
      "[\u001b[0;36mDEBUG\u001b[0m | src.tasks.clm_training.fabric.base]: \u001b[0;36mGradient norm before clipping: 26.5000\u001b[0m\n",
      "2025-06-16 13:55:36 - src.tasks.clm_training.fabric.base - \u001b[0;36mDEBUG\u001b[0m - \u001b[0;36mvalidate_after_k_steps value: 1000, type: <class 'int'>\u001b[0m\n",
      "[\u001b[0;36mDEBUG\u001b[0m | src.tasks.clm_training.fabric.base]: \u001b[0;36mvalidate_after_k_steps value: 1000, type: <class 'int'>\u001b[0m\n",
      "2025-06-16 13:55:36 - src.tasks.clm_training.fabric.base - \u001b[0;36mDEBUG\u001b[0m - \u001b[0;36mstep_count value: 83, type: <class 'int'>\u001b[0m\n",
      "[\u001b[0;36mDEBUG\u001b[0m | src.tasks.clm_training.fabric.base]: \u001b[0;36mstep_count value: 83, type: <class 'int'>\u001b[0m\n",
      "2025-06-16 13:55:36 - src.tasks.clm_training.fabric.base - \u001b[0;36mDEBUG\u001b[0m - \u001b[0;36mValidation condition check - epochFinished: False, trainingFinished: False\u001b[0m\n",
      "[\u001b[0;36mDEBUG\u001b[0m | src.tasks.clm_training.fabric.base]: \u001b[0;36mValidation condition check - epochFinished: False, trainingFinished: False\u001b[0m\n",
      "2025-06-16 13:55:36 - src.tasks.clm_training.fabric.base - \u001b[0;36mDEBUG\u001b[0m - \u001b[0;36mAbout to check: step_count % validate_after_k_steps == 0 -> 83 % 1000 == 0\u001b[0m\n",
      "[\u001b[0;36mDEBUG\u001b[0m | src.tasks.clm_training.fabric.base]: \u001b[0;36mAbout to check: step_count % validate_after_k_steps == 0 -> 83 % 1000 == 0\u001b[0m\n",
      "2025-06-16 13:55:36 - src.tasks.clm_training.fabric.base - \u001b[0;36mDEBUG\u001b[0m - \u001b[0;36miter 166 step 83: loss 5.9375, iter time: 420.78ms remaining time: \u001b[0m\n",
      "[\u001b[0;36mDEBUG\u001b[0m | src.tasks.clm_training.fabric.base]: \u001b[0;36miter 166 step 83: loss 5.9375, iter time: 420.78ms remaining time: \u001b[0m\n",
      "  4%|\u001b[34m█▍                                      \u001b[0m| 166/4682 [01:10<31:58,  2.35it/s]\u001b[0m2025-06-16 13:55:36 - src.tasks.clm_training.fabric.base - \u001b[0;36mDEBUG\u001b[0m - \u001b[0;36mgradient_accumulation_steps raw value: 2, type: <class 'int'>\u001b[0m\n",
      "[\u001b[0;36mDEBUG\u001b[0m | src.tasks.clm_training.fabric.base]: \u001b[0;36mgradient_accumulation_steps raw value: 2, type: <class 'int'>\u001b[0m\n",
      "2025-06-16 13:55:36 - src.tasks.clm_training.fabric.base - \u001b[0;36mDEBUG\u001b[0m - \u001b[0;36mgradient_accumulation_steps converted: 2, type: <class 'int'>\u001b[0m\n",
      "[\u001b[0;36mDEBUG\u001b[0m | src.tasks.clm_training.fabric.base]: \u001b[0;36mgradient_accumulation_steps converted: 2, type: <class 'int'>\u001b[0m\n",
      "2025-06-16 13:55:36 - src.tasks.clm_training.fabric.base - \u001b[0;36mDEBUG\u001b[0m - \u001b[0;36miter_num value: 166, type: <class 'int'>\u001b[0m\n",
      "[\u001b[0;36mDEBUG\u001b[0m | src.tasks.clm_training.fabric.base]: \u001b[0;36miter_num value: 166, type: <class 'int'>\u001b[0m\n",
      "2025-06-16 13:55:36 - src.tasks.clm_training.fabric.base - \u001b[0;36mDEBUG\u001b[0m - \u001b[0;36mAbout to check: (iter_num + 1) % gradient_accumulation_steps != 0 -> (166 + 1) % 2 != 0\u001b[0m\n",
      "[\u001b[0;36mDEBUG\u001b[0m | src.tasks.clm_training.fabric.base]: \u001b[0;36mAbout to check: (iter_num + 1) % gradient_accumulation_steps != 0 -> (166 + 1) % 2 != 0\u001b[0m\n",
      "2025-06-16 13:55:36 - src.tasks.clm_training.fabric.base - \u001b[0;36mDEBUG\u001b[0m - \u001b[0;36miter 167 step 83: loss 5.6562, iter time: 121.84ms remaining time: \u001b[0m\n",
      "[\u001b[0;36mDEBUG\u001b[0m | src.tasks.clm_training.fabric.base]: \u001b[0;36miter 167 step 83: loss 5.6562, iter time: 121.84ms remaining time: \u001b[0m\n",
      "  4%|\u001b[34m█▍                                      \u001b[0m| 167/4682 [01:10<31:45,  2.37it/s]\u001b[0m2025-06-16 13:55:36 - src.tasks.clm_training.fabric.base - \u001b[0;36mDEBUG\u001b[0m - \u001b[0;36mgradient_accumulation_steps raw value: 2, type: <class 'int'>\u001b[0m\n",
      "[\u001b[0;36mDEBUG\u001b[0m | src.tasks.clm_training.fabric.base]: \u001b[0;36mgradient_accumulation_steps raw value: 2, type: <class 'int'>\u001b[0m\n",
      "2025-06-16 13:55:36 - src.tasks.clm_training.fabric.base - \u001b[0;36mDEBUG\u001b[0m - \u001b[0;36mgradient_accumulation_steps converted: 2, type: <class 'int'>\u001b[0m\n",
      "[\u001b[0;36mDEBUG\u001b[0m | src.tasks.clm_training.fabric.base]: \u001b[0;36mgradient_accumulation_steps converted: 2, type: <class 'int'>\u001b[0m\n",
      "2025-06-16 13:55:36 - src.tasks.clm_training.fabric.base - \u001b[0;36mDEBUG\u001b[0m - \u001b[0;36miter_num value: 167, type: <class 'int'>\u001b[0m\n",
      "[\u001b[0;36mDEBUG\u001b[0m | src.tasks.clm_training.fabric.base]: \u001b[0;36miter_num value: 167, type: <class 'int'>\u001b[0m\n",
      "2025-06-16 13:55:36 - src.tasks.clm_training.fabric.base - \u001b[0;36mDEBUG\u001b[0m - \u001b[0;36mAbout to check: (iter_num + 1) % gradient_accumulation_steps != 0 -> (167 + 1) % 2 != 0\u001b[0m\n",
      "[\u001b[0;36mDEBUG\u001b[0m | src.tasks.clm_training.fabric.base]: \u001b[0;36mAbout to check: (iter_num + 1) % gradient_accumulation_steps != 0 -> (167 + 1) % 2 != 0\u001b[0m\n",
      "2025-06-16 13:55:37 - src.tasks.clm_training.fabric.base - \u001b[0;36mDEBUG\u001b[0m - \u001b[0;36mGradient norm before clipping: 23.5000\u001b[0m\n",
      "[\u001b[0;36mDEBUG\u001b[0m | src.tasks.clm_training.fabric.base]: \u001b[0;36mGradient norm before clipping: 23.5000\u001b[0m\n",
      "2025-06-16 13:55:37 - src.tasks.clm_training.fabric.base - \u001b[0;36mDEBUG\u001b[0m - \u001b[0;36mvalidate_after_k_steps value: 1000, type: <class 'int'>\u001b[0m\n",
      "[\u001b[0;36mDEBUG\u001b[0m | src.tasks.clm_training.fabric.base]: \u001b[0;36mvalidate_after_k_steps value: 1000, type: <class 'int'>\u001b[0m\n",
      "2025-06-16 13:55:37 - src.tasks.clm_training.fabric.base - \u001b[0;36mDEBUG\u001b[0m - \u001b[0;36mstep_count value: 84, type: <class 'int'>\u001b[0m\n",
      "[\u001b[0;36mDEBUG\u001b[0m | src.tasks.clm_training.fabric.base]: \u001b[0;36mstep_count value: 84, type: <class 'int'>\u001b[0m\n",
      "2025-06-16 13:55:37 - src.tasks.clm_training.fabric.base - \u001b[0;36mDEBUG\u001b[0m - \u001b[0;36mValidation condition check - epochFinished: False, trainingFinished: False\u001b[0m\n",
      "[\u001b[0;36mDEBUG\u001b[0m | src.tasks.clm_training.fabric.base]: \u001b[0;36mValidation condition check - epochFinished: False, trainingFinished: False\u001b[0m\n",
      "2025-06-16 13:55:37 - src.tasks.clm_training.fabric.base - \u001b[0;36mDEBUG\u001b[0m - \u001b[0;36mAbout to check: step_count % validate_after_k_steps == 0 -> 84 % 1000 == 0\u001b[0m\n",
      "[\u001b[0;36mDEBUG\u001b[0m | src.tasks.clm_training.fabric.base]: \u001b[0;36mAbout to check: step_count % validate_after_k_steps == 0 -> 84 % 1000 == 0\u001b[0m\n",
      "2025-06-16 13:55:37 - src.tasks.clm_training.fabric.base - \u001b[0;36mDEBUG\u001b[0m - \u001b[0;36miter 168 step 84: loss 5.7188, iter time: 420.39ms remaining time: \u001b[0m\n",
      "[\u001b[0;36mDEBUG\u001b[0m | src.tasks.clm_training.fabric.base]: \u001b[0;36miter 168 step 84: loss 5.7188, iter time: 420.39ms remaining time: \u001b[0m\n",
      "  4%|\u001b[34m█▍                                      \u001b[0m| 168/4682 [01:11<31:56,  2.36it/s]\u001b[0m2025-06-16 13:55:37 - src.tasks.clm_training.fabric.base - \u001b[0;36mDEBUG\u001b[0m - \u001b[0;36mgradient_accumulation_steps raw value: 2, type: <class 'int'>\u001b[0m\n",
      "[\u001b[0;36mDEBUG\u001b[0m | src.tasks.clm_training.fabric.base]: \u001b[0;36mgradient_accumulation_steps raw value: 2, type: <class 'int'>\u001b[0m\n",
      "2025-06-16 13:55:37 - src.tasks.clm_training.fabric.base - \u001b[0;36mDEBUG\u001b[0m - \u001b[0;36mgradient_accumulation_steps converted: 2, type: <class 'int'>\u001b[0m\n",
      "[\u001b[0;36mDEBUG\u001b[0m | src.tasks.clm_training.fabric.base]: \u001b[0;36mgradient_accumulation_steps converted: 2, type: <class 'int'>\u001b[0m\n",
      "2025-06-16 13:55:37 - src.tasks.clm_training.fabric.base - \u001b[0;36mDEBUG\u001b[0m - \u001b[0;36miter_num value: 168, type: <class 'int'>\u001b[0m\n",
      "[\u001b[0;36mDEBUG\u001b[0m | src.tasks.clm_training.fabric.base]: \u001b[0;36miter_num value: 168, type: <class 'int'>\u001b[0m\n",
      "2025-06-16 13:55:37 - src.tasks.clm_training.fabric.base - \u001b[0;36mDEBUG\u001b[0m - \u001b[0;36mAbout to check: (iter_num + 1) % gradient_accumulation_steps != 0 -> (168 + 1) % 2 != 0\u001b[0m\n",
      "[\u001b[0;36mDEBUG\u001b[0m | src.tasks.clm_training.fabric.base]: \u001b[0;36mAbout to check: (iter_num + 1) % gradient_accumulation_steps != 0 -> (168 + 1) % 2 != 0\u001b[0m\n",
      "2025-06-16 13:55:37 - src.tasks.clm_training.fabric.base - \u001b[0;36mDEBUG\u001b[0m - \u001b[0;36miter 169 step 84: loss 5.5312, iter time: 122.60ms remaining time: \u001b[0m\n",
      "[\u001b[0;36mDEBUG\u001b[0m | src.tasks.clm_training.fabric.base]: \u001b[0;36miter 169 step 84: loss 5.5312, iter time: 122.60ms remaining time: \u001b[0m\n",
      "  4%|\u001b[34m█▍                                      \u001b[0m| 169/4682 [01:11<31:44,  2.37it/s]\u001b[0m2025-06-16 13:55:37 - src.tasks.clm_training.fabric.base - \u001b[0;36mDEBUG\u001b[0m - \u001b[0;36mgradient_accumulation_steps raw value: 2, type: <class 'int'>\u001b[0m\n",
      "[\u001b[0;36mDEBUG\u001b[0m | src.tasks.clm_training.fabric.base]: \u001b[0;36mgradient_accumulation_steps raw value: 2, type: <class 'int'>\u001b[0m\n",
      "2025-06-16 13:55:37 - src.tasks.clm_training.fabric.base - \u001b[0;36mDEBUG\u001b[0m - \u001b[0;36mgradient_accumulation_steps converted: 2, type: <class 'int'>\u001b[0m\n",
      "[\u001b[0;36mDEBUG\u001b[0m | src.tasks.clm_training.fabric.base]: \u001b[0;36mgradient_accumulation_steps converted: 2, type: <class 'int'>\u001b[0m\n",
      "2025-06-16 13:55:37 - src.tasks.clm_training.fabric.base - \u001b[0;36mDEBUG\u001b[0m - \u001b[0;36miter_num value: 169, type: <class 'int'>\u001b[0m\n",
      "[\u001b[0;36mDEBUG\u001b[0m | src.tasks.clm_training.fabric.base]: \u001b[0;36miter_num value: 169, type: <class 'int'>\u001b[0m\n",
      "2025-06-16 13:55:37 - src.tasks.clm_training.fabric.base - \u001b[0;36mDEBUG\u001b[0m - \u001b[0;36mAbout to check: (iter_num + 1) % gradient_accumulation_steps != 0 -> (169 + 1) % 2 != 0\u001b[0m\n",
      "[\u001b[0;36mDEBUG\u001b[0m | src.tasks.clm_training.fabric.base]: \u001b[0;36mAbout to check: (iter_num + 1) % gradient_accumulation_steps != 0 -> (169 + 1) % 2 != 0\u001b[0m\n",
      "2025-06-16 13:55:38 - src.tasks.clm_training.fabric.base - \u001b[0;36mDEBUG\u001b[0m - \u001b[0;36mGradient norm before clipping: 24.6250\u001b[0m\n",
      "[\u001b[0;36mDEBUG\u001b[0m | src.tasks.clm_training.fabric.base]: \u001b[0;36mGradient norm before clipping: 24.6250\u001b[0m\n",
      "2025-06-16 13:55:38 - src.tasks.clm_training.fabric.base - \u001b[0;36mDEBUG\u001b[0m - \u001b[0;36mvalidate_after_k_steps value: 1000, type: <class 'int'>\u001b[0m\n",
      "[\u001b[0;36mDEBUG\u001b[0m | src.tasks.clm_training.fabric.base]: \u001b[0;36mvalidate_after_k_steps value: 1000, type: <class 'int'>\u001b[0m\n",
      "2025-06-16 13:55:38 - src.tasks.clm_training.fabric.base - \u001b[0;36mDEBUG\u001b[0m - \u001b[0;36mstep_count value: 85, type: <class 'int'>\u001b[0m\n",
      "[\u001b[0;36mDEBUG\u001b[0m | src.tasks.clm_training.fabric.base]: \u001b[0;36mstep_count value: 85, type: <class 'int'>\u001b[0m\n",
      "2025-06-16 13:55:38 - src.tasks.clm_training.fabric.base - \u001b[0;36mDEBUG\u001b[0m - \u001b[0;36mValidation condition check - epochFinished: False, trainingFinished: False\u001b[0m\n",
      "[\u001b[0;36mDEBUG\u001b[0m | src.tasks.clm_training.fabric.base]: \u001b[0;36mValidation condition check - epochFinished: False, trainingFinished: False\u001b[0m\n",
      "2025-06-16 13:55:38 - src.tasks.clm_training.fabric.base - \u001b[0;36mDEBUG\u001b[0m - \u001b[0;36mAbout to check: step_count % validate_after_k_steps == 0 -> 85 % 1000 == 0\u001b[0m\n",
      "[\u001b[0;36mDEBUG\u001b[0m | src.tasks.clm_training.fabric.base]: \u001b[0;36mAbout to check: step_count % validate_after_k_steps == 0 -> 85 % 1000 == 0\u001b[0m\n",
      "2025-06-16 13:55:38 - src.tasks.clm_training.fabric.base - \u001b[0;36mDEBUG\u001b[0m - \u001b[0;36miter 170 step 85: loss 5.7500, iter time: 420.49ms remaining time: \u001b[0m\n",
      "[\u001b[0;36mDEBUG\u001b[0m | src.tasks.clm_training.fabric.base]: \u001b[0;36miter 170 step 85: loss 5.7500, iter time: 420.49ms remaining time: \u001b[0m\n",
      "  4%|\u001b[34m█▍                                      \u001b[0m| 170/4682 [01:12<31:55,  2.36it/s]\u001b[0m2025-06-16 13:55:38 - src.tasks.clm_training.fabric.base - \u001b[0;36mDEBUG\u001b[0m - \u001b[0;36mgradient_accumulation_steps raw value: 2, type: <class 'int'>\u001b[0m\n",
      "[\u001b[0;36mDEBUG\u001b[0m | src.tasks.clm_training.fabric.base]: \u001b[0;36mgradient_accumulation_steps raw value: 2, type: <class 'int'>\u001b[0m\n",
      "2025-06-16 13:55:38 - src.tasks.clm_training.fabric.base - \u001b[0;36mDEBUG\u001b[0m - \u001b[0;36mgradient_accumulation_steps converted: 2, type: <class 'int'>\u001b[0m\n",
      "[\u001b[0;36mDEBUG\u001b[0m | src.tasks.clm_training.fabric.base]: \u001b[0;36mgradient_accumulation_steps converted: 2, type: <class 'int'>\u001b[0m\n",
      "2025-06-16 13:55:38 - src.tasks.clm_training.fabric.base - \u001b[0;36mDEBUG\u001b[0m - \u001b[0;36miter_num value: 170, type: <class 'int'>\u001b[0m\n",
      "[\u001b[0;36mDEBUG\u001b[0m | src.tasks.clm_training.fabric.base]: \u001b[0;36miter_num value: 170, type: <class 'int'>\u001b[0m\n",
      "2025-06-16 13:55:38 - src.tasks.clm_training.fabric.base - \u001b[0;36mDEBUG\u001b[0m - \u001b[0;36mAbout to check: (iter_num + 1) % gradient_accumulation_steps != 0 -> (170 + 1) % 2 != 0\u001b[0m\n",
      "[\u001b[0;36mDEBUG\u001b[0m | src.tasks.clm_training.fabric.base]: \u001b[0;36mAbout to check: (iter_num + 1) % gradient_accumulation_steps != 0 -> (170 + 1) % 2 != 0\u001b[0m\n",
      "2025-06-16 13:55:38 - src.tasks.clm_training.fabric.base - \u001b[0;36mDEBUG\u001b[0m - \u001b[0;36miter 171 step 85: loss 5.5625, iter time: 122.57ms remaining time: \u001b[0m\n",
      "[\u001b[0;36mDEBUG\u001b[0m | src.tasks.clm_training.fabric.base]: \u001b[0;36miter 171 step 85: loss 5.5625, iter time: 122.57ms remaining time: \u001b[0m\n",
      "  4%|\u001b[34m█▍                                      \u001b[0m| 171/4682 [01:12<31:44,  2.37it/s]\u001b[0m2025-06-16 13:55:38 - src.tasks.clm_training.fabric.base - \u001b[0;36mDEBUG\u001b[0m - \u001b[0;36mgradient_accumulation_steps raw value: 2, type: <class 'int'>\u001b[0m\n",
      "[\u001b[0;36mDEBUG\u001b[0m | src.tasks.clm_training.fabric.base]: \u001b[0;36mgradient_accumulation_steps raw value: 2, type: <class 'int'>\u001b[0m\n",
      "2025-06-16 13:55:38 - src.tasks.clm_training.fabric.base - \u001b[0;36mDEBUG\u001b[0m - \u001b[0;36mgradient_accumulation_steps converted: 2, type: <class 'int'>\u001b[0m\n",
      "[\u001b[0;36mDEBUG\u001b[0m | src.tasks.clm_training.fabric.base]: \u001b[0;36mgradient_accumulation_steps converted: 2, type: <class 'int'>\u001b[0m\n",
      "2025-06-16 13:55:38 - src.tasks.clm_training.fabric.base - \u001b[0;36mDEBUG\u001b[0m - \u001b[0;36miter_num value: 171, type: <class 'int'>\u001b[0m\n",
      "[\u001b[0;36mDEBUG\u001b[0m | src.tasks.clm_training.fabric.base]: \u001b[0;36miter_num value: 171, type: <class 'int'>\u001b[0m\n",
      "2025-06-16 13:55:38 - src.tasks.clm_training.fabric.base - \u001b[0;36mDEBUG\u001b[0m - \u001b[0;36mAbout to check: (iter_num + 1) % gradient_accumulation_steps != 0 -> (171 + 1) % 2 != 0\u001b[0m\n",
      "[\u001b[0;36mDEBUG\u001b[0m | src.tasks.clm_training.fabric.base]: \u001b[0;36mAbout to check: (iter_num + 1) % gradient_accumulation_steps != 0 -> (171 + 1) % 2 != 0\u001b[0m\n",
      "2025-06-16 13:55:39 - src.tasks.clm_training.fabric.base - \u001b[0;36mDEBUG\u001b[0m - \u001b[0;36mGradient norm before clipping: 23.3750\u001b[0m\n",
      "[\u001b[0;36mDEBUG\u001b[0m | src.tasks.clm_training.fabric.base]: \u001b[0;36mGradient norm before clipping: 23.3750\u001b[0m\n",
      "2025-06-16 13:55:39 - src.tasks.clm_training.fabric.base - \u001b[0;36mDEBUG\u001b[0m - \u001b[0;36mvalidate_after_k_steps value: 1000, type: <class 'int'>\u001b[0m\n",
      "[\u001b[0;36mDEBUG\u001b[0m | src.tasks.clm_training.fabric.base]: \u001b[0;36mvalidate_after_k_steps value: 1000, type: <class 'int'>\u001b[0m\n",
      "2025-06-16 13:55:39 - src.tasks.clm_training.fabric.base - \u001b[0;36mDEBUG\u001b[0m - \u001b[0;36mstep_count value: 86, type: <class 'int'>\u001b[0m\n",
      "[\u001b[0;36mDEBUG\u001b[0m | src.tasks.clm_training.fabric.base]: \u001b[0;36mstep_count value: 86, type: <class 'int'>\u001b[0m\n",
      "2025-06-16 13:55:39 - src.tasks.clm_training.fabric.base - \u001b[0;36mDEBUG\u001b[0m - \u001b[0;36mValidation condition check - epochFinished: False, trainingFinished: False\u001b[0m\n",
      "[\u001b[0;36mDEBUG\u001b[0m | src.tasks.clm_training.fabric.base]: \u001b[0;36mValidation condition check - epochFinished: False, trainingFinished: False\u001b[0m\n",
      "2025-06-16 13:55:39 - src.tasks.clm_training.fabric.base - \u001b[0;36mDEBUG\u001b[0m - \u001b[0;36mAbout to check: step_count % validate_after_k_steps == 0 -> 86 % 1000 == 0\u001b[0m\n",
      "[\u001b[0;36mDEBUG\u001b[0m | src.tasks.clm_training.fabric.base]: \u001b[0;36mAbout to check: step_count % validate_after_k_steps == 0 -> 86 % 1000 == 0\u001b[0m\n",
      "2025-06-16 13:55:39 - src.tasks.clm_training.fabric.base - \u001b[0;36mDEBUG\u001b[0m - \u001b[0;36miter 172 step 86: loss 5.7188, iter time: 420.73ms remaining time: \u001b[0m\n",
      "[\u001b[0;36mDEBUG\u001b[0m | src.tasks.clm_training.fabric.base]: \u001b[0;36miter 172 step 86: loss 5.7188, iter time: 420.73ms remaining time: \u001b[0m\n",
      "  4%|\u001b[34m█▍                                      \u001b[0m| 172/4682 [01:13<31:55,  2.35it/s]\u001b[0m2025-06-16 13:55:39 - src.tasks.clm_training.fabric.base - \u001b[0;36mDEBUG\u001b[0m - \u001b[0;36mgradient_accumulation_steps raw value: 2, type: <class 'int'>\u001b[0m\n",
      "[\u001b[0;36mDEBUG\u001b[0m | src.tasks.clm_training.fabric.base]: \u001b[0;36mgradient_accumulation_steps raw value: 2, type: <class 'int'>\u001b[0m\n",
      "2025-06-16 13:55:39 - src.tasks.clm_training.fabric.base - \u001b[0;36mDEBUG\u001b[0m - \u001b[0;36mgradient_accumulation_steps converted: 2, type: <class 'int'>\u001b[0m\n",
      "[\u001b[0;36mDEBUG\u001b[0m | src.tasks.clm_training.fabric.base]: \u001b[0;36mgradient_accumulation_steps converted: 2, type: <class 'int'>\u001b[0m\n",
      "2025-06-16 13:55:39 - src.tasks.clm_training.fabric.base - \u001b[0;36mDEBUG\u001b[0m - \u001b[0;36miter_num value: 172, type: <class 'int'>\u001b[0m\n",
      "[\u001b[0;36mDEBUG\u001b[0m | src.tasks.clm_training.fabric.base]: \u001b[0;36miter_num value: 172, type: <class 'int'>\u001b[0m\n",
      "2025-06-16 13:55:39 - src.tasks.clm_training.fabric.base - \u001b[0;36mDEBUG\u001b[0m - \u001b[0;36mAbout to check: (iter_num + 1) % gradient_accumulation_steps != 0 -> (172 + 1) % 2 != 0\u001b[0m\n",
      "[\u001b[0;36mDEBUG\u001b[0m | src.tasks.clm_training.fabric.base]: \u001b[0;36mAbout to check: (iter_num + 1) % gradient_accumulation_steps != 0 -> (172 + 1) % 2 != 0\u001b[0m\n",
      "2025-06-16 13:55:39 - src.tasks.clm_training.fabric.base - \u001b[0;36mDEBUG\u001b[0m - \u001b[0;36miter 173 step 86: loss 5.5000, iter time: 122.22ms remaining time: \u001b[0m\n",
      "[\u001b[0;36mDEBUG\u001b[0m | src.tasks.clm_training.fabric.base]: \u001b[0;36miter 173 step 86: loss 5.5000, iter time: 122.22ms remaining time: \u001b[0m\n",
      "  4%|\u001b[34m█▍                                      \u001b[0m| 173/4682 [01:13<31:44,  2.37it/s]\u001b[0m2025-06-16 13:55:39 - src.tasks.clm_training.fabric.base - \u001b[0;36mDEBUG\u001b[0m - \u001b[0;36mgradient_accumulation_steps raw value: 2, type: <class 'int'>\u001b[0m\n",
      "[\u001b[0;36mDEBUG\u001b[0m | src.tasks.clm_training.fabric.base]: \u001b[0;36mgradient_accumulation_steps raw value: 2, type: <class 'int'>\u001b[0m\n",
      "2025-06-16 13:55:39 - src.tasks.clm_training.fabric.base - \u001b[0;36mDEBUG\u001b[0m - \u001b[0;36mgradient_accumulation_steps converted: 2, type: <class 'int'>\u001b[0m\n",
      "[\u001b[0;36mDEBUG\u001b[0m | src.tasks.clm_training.fabric.base]: \u001b[0;36mgradient_accumulation_steps converted: 2, type: <class 'int'>\u001b[0m\n",
      "2025-06-16 13:55:39 - src.tasks.clm_training.fabric.base - \u001b[0;36mDEBUG\u001b[0m - \u001b[0;36miter_num value: 173, type: <class 'int'>\u001b[0m\n",
      "[\u001b[0;36mDEBUG\u001b[0m | src.tasks.clm_training.fabric.base]: \u001b[0;36miter_num value: 173, type: <class 'int'>\u001b[0m\n",
      "2025-06-16 13:55:39 - src.tasks.clm_training.fabric.base - \u001b[0;36mDEBUG\u001b[0m - \u001b[0;36mAbout to check: (iter_num + 1) % gradient_accumulation_steps != 0 -> (173 + 1) % 2 != 0\u001b[0m\n",
      "[\u001b[0;36mDEBUG\u001b[0m | src.tasks.clm_training.fabric.base]: \u001b[0;36mAbout to check: (iter_num + 1) % gradient_accumulation_steps != 0 -> (173 + 1) % 2 != 0\u001b[0m\n",
      "2025-06-16 13:55:39 - src.tasks.clm_training.fabric.base - \u001b[0;36mDEBUG\u001b[0m - \u001b[0;36mGradient norm before clipping: 24.3750\u001b[0m\n",
      "[\u001b[0;36mDEBUG\u001b[0m | src.tasks.clm_training.fabric.base]: \u001b[0;36mGradient norm before clipping: 24.3750\u001b[0m\n",
      "2025-06-16 13:55:39 - src.tasks.clm_training.fabric.base - \u001b[0;36mDEBUG\u001b[0m - \u001b[0;36mvalidate_after_k_steps value: 1000, type: <class 'int'>\u001b[0m\n",
      "[\u001b[0;36mDEBUG\u001b[0m | src.tasks.clm_training.fabric.base]: \u001b[0;36mvalidate_after_k_steps value: 1000, type: <class 'int'>\u001b[0m\n",
      "2025-06-16 13:55:39 - src.tasks.clm_training.fabric.base - \u001b[0;36mDEBUG\u001b[0m - \u001b[0;36mstep_count value: 87, type: <class 'int'>\u001b[0m\n",
      "[\u001b[0;36mDEBUG\u001b[0m | src.tasks.clm_training.fabric.base]: \u001b[0;36mstep_count value: 87, type: <class 'int'>\u001b[0m\n",
      "2025-06-16 13:55:39 - src.tasks.clm_training.fabric.base - \u001b[0;36mDEBUG\u001b[0m - \u001b[0;36mValidation condition check - epochFinished: False, trainingFinished: False\u001b[0m\n",
      "[\u001b[0;36mDEBUG\u001b[0m | src.tasks.clm_training.fabric.base]: \u001b[0;36mValidation condition check - epochFinished: False, trainingFinished: False\u001b[0m\n",
      "2025-06-16 13:55:39 - src.tasks.clm_training.fabric.base - \u001b[0;36mDEBUG\u001b[0m - \u001b[0;36mAbout to check: step_count % validate_after_k_steps == 0 -> 87 % 1000 == 0\u001b[0m\n",
      "[\u001b[0;36mDEBUG\u001b[0m | src.tasks.clm_training.fabric.base]: \u001b[0;36mAbout to check: step_count % validate_after_k_steps == 0 -> 87 % 1000 == 0\u001b[0m\n",
      "2025-06-16 13:55:39 - src.tasks.clm_training.fabric.base - \u001b[0;36mDEBUG\u001b[0m - \u001b[0;36miter 174 step 87: loss 5.3750, iter time: 420.91ms remaining time: \u001b[0m\n",
      "[\u001b[0;36mDEBUG\u001b[0m | src.tasks.clm_training.fabric.base]: \u001b[0;36miter 174 step 87: loss 5.3750, iter time: 420.91ms remaining time: \u001b[0m\n",
      "  4%|\u001b[34m█▍                                      \u001b[0m| 174/4682 [01:13<31:55,  2.35it/s]\u001b[0m2025-06-16 13:55:39 - src.tasks.clm_training.fabric.base - \u001b[0;36mDEBUG\u001b[0m - \u001b[0;36mgradient_accumulation_steps raw value: 2, type: <class 'int'>\u001b[0m\n",
      "[\u001b[0;36mDEBUG\u001b[0m | src.tasks.clm_training.fabric.base]: \u001b[0;36mgradient_accumulation_steps raw value: 2, type: <class 'int'>\u001b[0m\n",
      "2025-06-16 13:55:39 - src.tasks.clm_training.fabric.base - \u001b[0;36mDEBUG\u001b[0m - \u001b[0;36mgradient_accumulation_steps converted: 2, type: <class 'int'>\u001b[0m\n",
      "[\u001b[0;36mDEBUG\u001b[0m | src.tasks.clm_training.fabric.base]: \u001b[0;36mgradient_accumulation_steps converted: 2, type: <class 'int'>\u001b[0m\n",
      "2025-06-16 13:55:39 - src.tasks.clm_training.fabric.base - \u001b[0;36mDEBUG\u001b[0m - \u001b[0;36miter_num value: 174, type: <class 'int'>\u001b[0m\n",
      "[\u001b[0;36mDEBUG\u001b[0m | src.tasks.clm_training.fabric.base]: \u001b[0;36miter_num value: 174, type: <class 'int'>\u001b[0m\n",
      "2025-06-16 13:55:39 - src.tasks.clm_training.fabric.base - \u001b[0;36mDEBUG\u001b[0m - \u001b[0;36mAbout to check: (iter_num + 1) % gradient_accumulation_steps != 0 -> (174 + 1) % 2 != 0\u001b[0m\n",
      "[\u001b[0;36mDEBUG\u001b[0m | src.tasks.clm_training.fabric.base]: \u001b[0;36mAbout to check: (iter_num + 1) % gradient_accumulation_steps != 0 -> (174 + 1) % 2 != 0\u001b[0m\n",
      "2025-06-16 13:55:40 - src.tasks.clm_training.fabric.base - \u001b[0;36mDEBUG\u001b[0m - \u001b[0;36miter 175 step 87: loss 5.6875, iter time: 122.26ms remaining time: \u001b[0m\n",
      "[\u001b[0;36mDEBUG\u001b[0m | src.tasks.clm_training.fabric.base]: \u001b[0;36miter 175 step 87: loss 5.6875, iter time: 122.26ms remaining time: \u001b[0m\n",
      "  4%|\u001b[34m█▍                                      \u001b[0m| 175/4682 [01:14<31:42,  2.37it/s]\u001b[0m2025-06-16 13:55:40 - src.tasks.clm_training.fabric.base - \u001b[0;36mDEBUG\u001b[0m - \u001b[0;36mgradient_accumulation_steps raw value: 2, type: <class 'int'>\u001b[0m\n",
      "[\u001b[0;36mDEBUG\u001b[0m | src.tasks.clm_training.fabric.base]: \u001b[0;36mgradient_accumulation_steps raw value: 2, type: <class 'int'>\u001b[0m\n",
      "2025-06-16 13:55:40 - src.tasks.clm_training.fabric.base - \u001b[0;36mDEBUG\u001b[0m - \u001b[0;36mgradient_accumulation_steps converted: 2, type: <class 'int'>\u001b[0m\n",
      "[\u001b[0;36mDEBUG\u001b[0m | src.tasks.clm_training.fabric.base]: \u001b[0;36mgradient_accumulation_steps converted: 2, type: <class 'int'>\u001b[0m\n",
      "2025-06-16 13:55:40 - src.tasks.clm_training.fabric.base - \u001b[0;36mDEBUG\u001b[0m - \u001b[0;36miter_num value: 175, type: <class 'int'>\u001b[0m\n",
      "[\u001b[0;36mDEBUG\u001b[0m | src.tasks.clm_training.fabric.base]: \u001b[0;36miter_num value: 175, type: <class 'int'>\u001b[0m\n",
      "2025-06-16 13:55:40 - src.tasks.clm_training.fabric.base - \u001b[0;36mDEBUG\u001b[0m - \u001b[0;36mAbout to check: (iter_num + 1) % gradient_accumulation_steps != 0 -> (175 + 1) % 2 != 0\u001b[0m\n",
      "[\u001b[0;36mDEBUG\u001b[0m | src.tasks.clm_training.fabric.base]: \u001b[0;36mAbout to check: (iter_num + 1) % gradient_accumulation_steps != 0 -> (175 + 1) % 2 != 0\u001b[0m\n",
      "2025-06-16 13:55:40 - src.tasks.clm_training.fabric.base - \u001b[0;36mDEBUG\u001b[0m - \u001b[0;36mGradient norm before clipping: 22.6250\u001b[0m\n",
      "[\u001b[0;36mDEBUG\u001b[0m | src.tasks.clm_training.fabric.base]: \u001b[0;36mGradient norm before clipping: 22.6250\u001b[0m\n",
      "2025-06-16 13:55:40 - src.tasks.clm_training.fabric.base - \u001b[0;36mDEBUG\u001b[0m - \u001b[0;36mvalidate_after_k_steps value: 1000, type: <class 'int'>\u001b[0m\n",
      "[\u001b[0;36mDEBUG\u001b[0m | src.tasks.clm_training.fabric.base]: \u001b[0;36mvalidate_after_k_steps value: 1000, type: <class 'int'>\u001b[0m\n",
      "2025-06-16 13:55:40 - src.tasks.clm_training.fabric.base - \u001b[0;36mDEBUG\u001b[0m - \u001b[0;36mstep_count value: 88, type: <class 'int'>\u001b[0m\n",
      "[\u001b[0;36mDEBUG\u001b[0m | src.tasks.clm_training.fabric.base]: \u001b[0;36mstep_count value: 88, type: <class 'int'>\u001b[0m\n",
      "2025-06-16 13:55:40 - src.tasks.clm_training.fabric.base - \u001b[0;36mDEBUG\u001b[0m - \u001b[0;36mValidation condition check - epochFinished: False, trainingFinished: False\u001b[0m\n",
      "[\u001b[0;36mDEBUG\u001b[0m | src.tasks.clm_training.fabric.base]: \u001b[0;36mValidation condition check - epochFinished: False, trainingFinished: False\u001b[0m\n",
      "2025-06-16 13:55:40 - src.tasks.clm_training.fabric.base - \u001b[0;36mDEBUG\u001b[0m - \u001b[0;36mAbout to check: step_count % validate_after_k_steps == 0 -> 88 % 1000 == 0\u001b[0m\n",
      "[\u001b[0;36mDEBUG\u001b[0m | src.tasks.clm_training.fabric.base]: \u001b[0;36mAbout to check: step_count % validate_after_k_steps == 0 -> 88 % 1000 == 0\u001b[0m\n",
      "2025-06-16 13:55:40 - src.tasks.clm_training.fabric.base - \u001b[0;36mDEBUG\u001b[0m - \u001b[0;36miter 176 step 88: loss 5.6562, iter time: 420.72ms remaining time: \u001b[0m\n",
      "[\u001b[0;36mDEBUG\u001b[0m | src.tasks.clm_training.fabric.base]: \u001b[0;36miter 176 step 88: loss 5.6562, iter time: 420.72ms remaining time: \u001b[0m\n",
      "  4%|\u001b[34m█▌                                      \u001b[0m| 176/4682 [01:14<31:54,  2.35it/s]\u001b[0m2025-06-16 13:55:40 - src.tasks.clm_training.fabric.base - \u001b[0;36mDEBUG\u001b[0m - \u001b[0;36mgradient_accumulation_steps raw value: 2, type: <class 'int'>\u001b[0m\n",
      "[\u001b[0;36mDEBUG\u001b[0m | src.tasks.clm_training.fabric.base]: \u001b[0;36mgradient_accumulation_steps raw value: 2, type: <class 'int'>\u001b[0m\n",
      "2025-06-16 13:55:40 - src.tasks.clm_training.fabric.base - \u001b[0;36mDEBUG\u001b[0m - \u001b[0;36mgradient_accumulation_steps converted: 2, type: <class 'int'>\u001b[0m\n",
      "[\u001b[0;36mDEBUG\u001b[0m | src.tasks.clm_training.fabric.base]: \u001b[0;36mgradient_accumulation_steps converted: 2, type: <class 'int'>\u001b[0m\n",
      "2025-06-16 13:55:40 - src.tasks.clm_training.fabric.base - \u001b[0;36mDEBUG\u001b[0m - \u001b[0;36miter_num value: 176, type: <class 'int'>\u001b[0m\n",
      "[\u001b[0;36mDEBUG\u001b[0m | src.tasks.clm_training.fabric.base]: \u001b[0;36miter_num value: 176, type: <class 'int'>\u001b[0m\n",
      "2025-06-16 13:55:40 - src.tasks.clm_training.fabric.base - \u001b[0;36mDEBUG\u001b[0m - \u001b[0;36mAbout to check: (iter_num + 1) % gradient_accumulation_steps != 0 -> (176 + 1) % 2 != 0\u001b[0m\n",
      "[\u001b[0;36mDEBUG\u001b[0m | src.tasks.clm_training.fabric.base]: \u001b[0;36mAbout to check: (iter_num + 1) % gradient_accumulation_steps != 0 -> (176 + 1) % 2 != 0\u001b[0m\n",
      "2025-06-16 13:55:41 - src.tasks.clm_training.fabric.base - \u001b[0;36mDEBUG\u001b[0m - \u001b[0;36miter 177 step 88: loss 6.0938, iter time: 122.52ms remaining time: \u001b[0m\n",
      "[\u001b[0;36mDEBUG\u001b[0m | src.tasks.clm_training.fabric.base]: \u001b[0;36miter 177 step 88: loss 6.0938, iter time: 122.52ms remaining time: \u001b[0m\n",
      "  4%|\u001b[34m█▌                                      \u001b[0m| 177/4682 [01:15<31:43,  2.37it/s]\u001b[0m2025-06-16 13:55:41 - src.tasks.clm_training.fabric.base - \u001b[0;36mDEBUG\u001b[0m - \u001b[0;36mgradient_accumulation_steps raw value: 2, type: <class 'int'>\u001b[0m\n",
      "[\u001b[0;36mDEBUG\u001b[0m | src.tasks.clm_training.fabric.base]: \u001b[0;36mgradient_accumulation_steps raw value: 2, type: <class 'int'>\u001b[0m\n",
      "2025-06-16 13:55:41 - src.tasks.clm_training.fabric.base - \u001b[0;36mDEBUG\u001b[0m - \u001b[0;36mgradient_accumulation_steps converted: 2, type: <class 'int'>\u001b[0m\n",
      "[\u001b[0;36mDEBUG\u001b[0m | src.tasks.clm_training.fabric.base]: \u001b[0;36mgradient_accumulation_steps converted: 2, type: <class 'int'>\u001b[0m\n",
      "2025-06-16 13:55:41 - src.tasks.clm_training.fabric.base - \u001b[0;36mDEBUG\u001b[0m - \u001b[0;36miter_num value: 177, type: <class 'int'>\u001b[0m\n",
      "[\u001b[0;36mDEBUG\u001b[0m | src.tasks.clm_training.fabric.base]: \u001b[0;36miter_num value: 177, type: <class 'int'>\u001b[0m\n",
      "2025-06-16 13:55:41 - src.tasks.clm_training.fabric.base - \u001b[0;36mDEBUG\u001b[0m - \u001b[0;36mAbout to check: (iter_num + 1) % gradient_accumulation_steps != 0 -> (177 + 1) % 2 != 0\u001b[0m\n",
      "[\u001b[0;36mDEBUG\u001b[0m | src.tasks.clm_training.fabric.base]: \u001b[0;36mAbout to check: (iter_num + 1) % gradient_accumulation_steps != 0 -> (177 + 1) % 2 != 0\u001b[0m\n",
      "2025-06-16 13:55:41 - src.tasks.clm_training.fabric.base - \u001b[0;36mDEBUG\u001b[0m - \u001b[0;36mGradient norm before clipping: 28.6250\u001b[0m\n",
      "[\u001b[0;36mDEBUG\u001b[0m | src.tasks.clm_training.fabric.base]: \u001b[0;36mGradient norm before clipping: 28.6250\u001b[0m\n",
      "2025-06-16 13:55:41 - src.tasks.clm_training.fabric.base - \u001b[0;36mDEBUG\u001b[0m - \u001b[0;36mvalidate_after_k_steps value: 1000, type: <class 'int'>\u001b[0m\n",
      "[\u001b[0;36mDEBUG\u001b[0m | src.tasks.clm_training.fabric.base]: \u001b[0;36mvalidate_after_k_steps value: 1000, type: <class 'int'>\u001b[0m\n",
      "2025-06-16 13:55:41 - src.tasks.clm_training.fabric.base - \u001b[0;36mDEBUG\u001b[0m - \u001b[0;36mstep_count value: 89, type: <class 'int'>\u001b[0m\n",
      "[\u001b[0;36mDEBUG\u001b[0m | src.tasks.clm_training.fabric.base]: \u001b[0;36mstep_count value: 89, type: <class 'int'>\u001b[0m\n",
      "2025-06-16 13:55:41 - src.tasks.clm_training.fabric.base - \u001b[0;36mDEBUG\u001b[0m - \u001b[0;36mValidation condition check - epochFinished: False, trainingFinished: False\u001b[0m\n",
      "[\u001b[0;36mDEBUG\u001b[0m | src.tasks.clm_training.fabric.base]: \u001b[0;36mValidation condition check - epochFinished: False, trainingFinished: False\u001b[0m\n",
      "2025-06-16 13:55:41 - src.tasks.clm_training.fabric.base - \u001b[0;36mDEBUG\u001b[0m - \u001b[0;36mAbout to check: step_count % validate_after_k_steps == 0 -> 89 % 1000 == 0\u001b[0m\n",
      "[\u001b[0;36mDEBUG\u001b[0m | src.tasks.clm_training.fabric.base]: \u001b[0;36mAbout to check: step_count % validate_after_k_steps == 0 -> 89 % 1000 == 0\u001b[0m\n",
      "2025-06-16 13:55:41 - src.tasks.clm_training.fabric.base - \u001b[0;36mDEBUG\u001b[0m - \u001b[0;36miter 178 step 89: loss 5.5938, iter time: 420.31ms remaining time: \u001b[0m\n",
      "[\u001b[0;36mDEBUG\u001b[0m | src.tasks.clm_training.fabric.base]: \u001b[0;36miter 178 step 89: loss 5.5938, iter time: 420.31ms remaining time: \u001b[0m\n",
      "  4%|\u001b[34m█▌                                      \u001b[0m| 178/4682 [01:15<31:53,  2.35it/s]\u001b[0m2025-06-16 13:55:41 - src.tasks.clm_training.fabric.base - \u001b[0;36mDEBUG\u001b[0m - \u001b[0;36mgradient_accumulation_steps raw value: 2, type: <class 'int'>\u001b[0m\n",
      "[\u001b[0;36mDEBUG\u001b[0m | src.tasks.clm_training.fabric.base]: \u001b[0;36mgradient_accumulation_steps raw value: 2, type: <class 'int'>\u001b[0m\n",
      "2025-06-16 13:55:41 - src.tasks.clm_training.fabric.base - \u001b[0;36mDEBUG\u001b[0m - \u001b[0;36mgradient_accumulation_steps converted: 2, type: <class 'int'>\u001b[0m\n",
      "[\u001b[0;36mDEBUG\u001b[0m | src.tasks.clm_training.fabric.base]: \u001b[0;36mgradient_accumulation_steps converted: 2, type: <class 'int'>\u001b[0m\n",
      "2025-06-16 13:55:41 - src.tasks.clm_training.fabric.base - \u001b[0;36mDEBUG\u001b[0m - \u001b[0;36miter_num value: 178, type: <class 'int'>\u001b[0m\n",
      "[\u001b[0;36mDEBUG\u001b[0m | src.tasks.clm_training.fabric.base]: \u001b[0;36miter_num value: 178, type: <class 'int'>\u001b[0m\n",
      "2025-06-16 13:55:41 - src.tasks.clm_training.fabric.base - \u001b[0;36mDEBUG\u001b[0m - \u001b[0;36mAbout to check: (iter_num + 1) % gradient_accumulation_steps != 0 -> (178 + 1) % 2 != 0\u001b[0m\n",
      "[\u001b[0;36mDEBUG\u001b[0m | src.tasks.clm_training.fabric.base]: \u001b[0;36mAbout to check: (iter_num + 1) % gradient_accumulation_steps != 0 -> (178 + 1) % 2 != 0\u001b[0m\n",
      "2025-06-16 13:55:42 - src.tasks.clm_training.fabric.base - \u001b[0;36mDEBUG\u001b[0m - \u001b[0;36miter 179 step 89: loss 5.9062, iter time: 122.15ms remaining time: \u001b[0m\n",
      "[\u001b[0;36mDEBUG\u001b[0m | src.tasks.clm_training.fabric.base]: \u001b[0;36miter 179 step 89: loss 5.9062, iter time: 122.15ms remaining time: \u001b[0m\n",
      "  4%|\u001b[34m█▌                                      \u001b[0m| 179/4682 [01:15<31:41,  2.37it/s]\u001b[0m2025-06-16 13:55:42 - src.tasks.clm_training.fabric.base - \u001b[0;36mDEBUG\u001b[0m - \u001b[0;36mgradient_accumulation_steps raw value: 2, type: <class 'int'>\u001b[0m\n",
      "[\u001b[0;36mDEBUG\u001b[0m | src.tasks.clm_training.fabric.base]: \u001b[0;36mgradient_accumulation_steps raw value: 2, type: <class 'int'>\u001b[0m\n",
      "2025-06-16 13:55:42 - src.tasks.clm_training.fabric.base - \u001b[0;36mDEBUG\u001b[0m - \u001b[0;36mgradient_accumulation_steps converted: 2, type: <class 'int'>\u001b[0m\n",
      "[\u001b[0;36mDEBUG\u001b[0m | src.tasks.clm_training.fabric.base]: \u001b[0;36mgradient_accumulation_steps converted: 2, type: <class 'int'>\u001b[0m\n",
      "2025-06-16 13:55:42 - src.tasks.clm_training.fabric.base - \u001b[0;36mDEBUG\u001b[0m - \u001b[0;36miter_num value: 179, type: <class 'int'>\u001b[0m\n",
      "[\u001b[0;36mDEBUG\u001b[0m | src.tasks.clm_training.fabric.base]: \u001b[0;36miter_num value: 179, type: <class 'int'>\u001b[0m\n",
      "2025-06-16 13:55:42 - src.tasks.clm_training.fabric.base - \u001b[0;36mDEBUG\u001b[0m - \u001b[0;36mAbout to check: (iter_num + 1) % gradient_accumulation_steps != 0 -> (179 + 1) % 2 != 0\u001b[0m\n",
      "[\u001b[0;36mDEBUG\u001b[0m | src.tasks.clm_training.fabric.base]: \u001b[0;36mAbout to check: (iter_num + 1) % gradient_accumulation_steps != 0 -> (179 + 1) % 2 != 0\u001b[0m\n",
      "2025-06-16 13:55:42 - src.tasks.clm_training.fabric.base - \u001b[0;36mDEBUG\u001b[0m - \u001b[0;36mGradient norm before clipping: 23.5000\u001b[0m\n",
      "[\u001b[0;36mDEBUG\u001b[0m | src.tasks.clm_training.fabric.base]: \u001b[0;36mGradient norm before clipping: 23.5000\u001b[0m\n",
      "2025-06-16 13:55:42 - src.tasks.clm_training.fabric.base - \u001b[0;36mDEBUG\u001b[0m - \u001b[0;36mvalidate_after_k_steps value: 1000, type: <class 'int'>\u001b[0m\n",
      "[\u001b[0;36mDEBUG\u001b[0m | src.tasks.clm_training.fabric.base]: \u001b[0;36mvalidate_after_k_steps value: 1000, type: <class 'int'>\u001b[0m\n",
      "2025-06-16 13:55:42 - src.tasks.clm_training.fabric.base - \u001b[0;36mDEBUG\u001b[0m - \u001b[0;36mstep_count value: 90, type: <class 'int'>\u001b[0m\n",
      "[\u001b[0;36mDEBUG\u001b[0m | src.tasks.clm_training.fabric.base]: \u001b[0;36mstep_count value: 90, type: <class 'int'>\u001b[0m\n",
      "2025-06-16 13:55:42 - src.tasks.clm_training.fabric.base - \u001b[0;36mDEBUG\u001b[0m - \u001b[0;36mValidation condition check - epochFinished: False, trainingFinished: False\u001b[0m\n",
      "[\u001b[0;36mDEBUG\u001b[0m | src.tasks.clm_training.fabric.base]: \u001b[0;36mValidation condition check - epochFinished: False, trainingFinished: False\u001b[0m\n",
      "2025-06-16 13:55:42 - src.tasks.clm_training.fabric.base - \u001b[0;36mDEBUG\u001b[0m - \u001b[0;36mAbout to check: step_count % validate_after_k_steps == 0 -> 90 % 1000 == 0\u001b[0m\n",
      "[\u001b[0;36mDEBUG\u001b[0m | src.tasks.clm_training.fabric.base]: \u001b[0;36mAbout to check: step_count % validate_after_k_steps == 0 -> 90 % 1000 == 0\u001b[0m\n",
      "2025-06-16 13:55:42 - src.tasks.clm_training.fabric.base - \u001b[0;36mDEBUG\u001b[0m - \u001b[0;36miter 180 step 90: loss 5.9375, iter time: 421.35ms remaining time: \u001b[0m\n",
      "[\u001b[0;36mDEBUG\u001b[0m | src.tasks.clm_training.fabric.base]: \u001b[0;36miter 180 step 90: loss 5.9375, iter time: 421.35ms remaining time: \u001b[0m\n",
      "  4%|\u001b[34m█▌                                      \u001b[0m| 180/4682 [01:16<31:52,  2.35it/s]\u001b[0m2025-06-16 13:55:42 - src.tasks.clm_training.fabric.base - \u001b[0;36mDEBUG\u001b[0m - \u001b[0;36mgradient_accumulation_steps raw value: 2, type: <class 'int'>\u001b[0m\n",
      "[\u001b[0;36mDEBUG\u001b[0m | src.tasks.clm_training.fabric.base]: \u001b[0;36mgradient_accumulation_steps raw value: 2, type: <class 'int'>\u001b[0m\n",
      "2025-06-16 13:55:42 - src.tasks.clm_training.fabric.base - \u001b[0;36mDEBUG\u001b[0m - \u001b[0;36mgradient_accumulation_steps converted: 2, type: <class 'int'>\u001b[0m\n",
      "[\u001b[0;36mDEBUG\u001b[0m | src.tasks.clm_training.fabric.base]: \u001b[0;36mgradient_accumulation_steps converted: 2, type: <class 'int'>\u001b[0m\n",
      "2025-06-16 13:55:42 - src.tasks.clm_training.fabric.base - \u001b[0;36mDEBUG\u001b[0m - \u001b[0;36miter_num value: 180, type: <class 'int'>\u001b[0m\n",
      "[\u001b[0;36mDEBUG\u001b[0m | src.tasks.clm_training.fabric.base]: \u001b[0;36miter_num value: 180, type: <class 'int'>\u001b[0m\n",
      "2025-06-16 13:55:42 - src.tasks.clm_training.fabric.base - \u001b[0;36mDEBUG\u001b[0m - \u001b[0;36mAbout to check: (iter_num + 1) % gradient_accumulation_steps != 0 -> (180 + 1) % 2 != 0\u001b[0m\n",
      "[\u001b[0;36mDEBUG\u001b[0m | src.tasks.clm_training.fabric.base]: \u001b[0;36mAbout to check: (iter_num + 1) % gradient_accumulation_steps != 0 -> (180 + 1) % 2 != 0\u001b[0m\n",
      "2025-06-16 13:55:42 - src.tasks.clm_training.fabric.base - \u001b[0;36mDEBUG\u001b[0m - \u001b[0;36miter 181 step 90: loss 6.2188, iter time: 122.88ms remaining time: \u001b[0m\n",
      "[\u001b[0;36mDEBUG\u001b[0m | src.tasks.clm_training.fabric.base]: \u001b[0;36miter 181 step 90: loss 6.2188, iter time: 122.88ms remaining time: \u001b[0m\n",
      "  4%|\u001b[34m█▌                                      \u001b[0m| 181/4682 [01:16<31:41,  2.37it/s]\u001b[0m2025-06-16 13:55:42 - src.tasks.clm_training.fabric.base - \u001b[0;36mDEBUG\u001b[0m - \u001b[0;36mgradient_accumulation_steps raw value: 2, type: <class 'int'>\u001b[0m\n",
      "[\u001b[0;36mDEBUG\u001b[0m | src.tasks.clm_training.fabric.base]: \u001b[0;36mgradient_accumulation_steps raw value: 2, type: <class 'int'>\u001b[0m\n",
      "2025-06-16 13:55:42 - src.tasks.clm_training.fabric.base - \u001b[0;36mDEBUG\u001b[0m - \u001b[0;36mgradient_accumulation_steps converted: 2, type: <class 'int'>\u001b[0m\n",
      "[\u001b[0;36mDEBUG\u001b[0m | src.tasks.clm_training.fabric.base]: \u001b[0;36mgradient_accumulation_steps converted: 2, type: <class 'int'>\u001b[0m\n",
      "2025-06-16 13:55:42 - src.tasks.clm_training.fabric.base - \u001b[0;36mDEBUG\u001b[0m - \u001b[0;36miter_num value: 181, type: <class 'int'>\u001b[0m\n",
      "[\u001b[0;36mDEBUG\u001b[0m | src.tasks.clm_training.fabric.base]: \u001b[0;36miter_num value: 181, type: <class 'int'>\u001b[0m\n",
      "2025-06-16 13:55:42 - src.tasks.clm_training.fabric.base - \u001b[0;36mDEBUG\u001b[0m - \u001b[0;36mAbout to check: (iter_num + 1) % gradient_accumulation_steps != 0 -> (181 + 1) % 2 != 0\u001b[0m\n",
      "[\u001b[0;36mDEBUG\u001b[0m | src.tasks.clm_training.fabric.base]: \u001b[0;36mAbout to check: (iter_num + 1) % gradient_accumulation_steps != 0 -> (181 + 1) % 2 != 0\u001b[0m\n",
      "2025-06-16 13:55:43 - src.tasks.clm_training.fabric.base - \u001b[0;36mDEBUG\u001b[0m - \u001b[0;36mGradient norm before clipping: 25.2500\u001b[0m\n",
      "[\u001b[0;36mDEBUG\u001b[0m | src.tasks.clm_training.fabric.base]: \u001b[0;36mGradient norm before clipping: 25.2500\u001b[0m\n",
      "2025-06-16 13:55:43 - src.tasks.clm_training.fabric.base - \u001b[0;36mDEBUG\u001b[0m - \u001b[0;36mvalidate_after_k_steps value: 1000, type: <class 'int'>\u001b[0m\n",
      "[\u001b[0;36mDEBUG\u001b[0m | src.tasks.clm_training.fabric.base]: \u001b[0;36mvalidate_after_k_steps value: 1000, type: <class 'int'>\u001b[0m\n",
      "2025-06-16 13:55:43 - src.tasks.clm_training.fabric.base - \u001b[0;36mDEBUG\u001b[0m - \u001b[0;36mstep_count value: 91, type: <class 'int'>\u001b[0m\n",
      "[\u001b[0;36mDEBUG\u001b[0m | src.tasks.clm_training.fabric.base]: \u001b[0;36mstep_count value: 91, type: <class 'int'>\u001b[0m\n",
      "2025-06-16 13:55:43 - src.tasks.clm_training.fabric.base - \u001b[0;36mDEBUG\u001b[0m - \u001b[0;36mValidation condition check - epochFinished: False, trainingFinished: False\u001b[0m\n",
      "[\u001b[0;36mDEBUG\u001b[0m | src.tasks.clm_training.fabric.base]: \u001b[0;36mValidation condition check - epochFinished: False, trainingFinished: False\u001b[0m\n",
      "2025-06-16 13:55:43 - src.tasks.clm_training.fabric.base - \u001b[0;36mDEBUG\u001b[0m - \u001b[0;36mAbout to check: step_count % validate_after_k_steps == 0 -> 91 % 1000 == 0\u001b[0m\n",
      "[\u001b[0;36mDEBUG\u001b[0m | src.tasks.clm_training.fabric.base]: \u001b[0;36mAbout to check: step_count % validate_after_k_steps == 0 -> 91 % 1000 == 0\u001b[0m\n",
      "2025-06-16 13:55:43 - src.tasks.clm_training.fabric.base - \u001b[0;36mDEBUG\u001b[0m - \u001b[0;36miter 182 step 91: loss 5.3125, iter time: 421.06ms remaining time: \u001b[0m\n",
      "[\u001b[0;36mDEBUG\u001b[0m | src.tasks.clm_training.fabric.base]: \u001b[0;36miter 182 step 91: loss 5.3125, iter time: 421.06ms remaining time: \u001b[0m\n",
      "  4%|\u001b[34m█▌                                      \u001b[0m| 182/4682 [01:17<31:51,  2.35it/s]\u001b[0m2025-06-16 13:55:43 - src.tasks.clm_training.fabric.base - \u001b[0;36mDEBUG\u001b[0m - \u001b[0;36mgradient_accumulation_steps raw value: 2, type: <class 'int'>\u001b[0m\n",
      "[\u001b[0;36mDEBUG\u001b[0m | src.tasks.clm_training.fabric.base]: \u001b[0;36mgradient_accumulation_steps raw value: 2, type: <class 'int'>\u001b[0m\n",
      "2025-06-16 13:55:43 - src.tasks.clm_training.fabric.base - \u001b[0;36mDEBUG\u001b[0m - \u001b[0;36mgradient_accumulation_steps converted: 2, type: <class 'int'>\u001b[0m\n",
      "[\u001b[0;36mDEBUG\u001b[0m | src.tasks.clm_training.fabric.base]: \u001b[0;36mgradient_accumulation_steps converted: 2, type: <class 'int'>\u001b[0m\n",
      "2025-06-16 13:55:43 - src.tasks.clm_training.fabric.base - \u001b[0;36mDEBUG\u001b[0m - \u001b[0;36miter_num value: 182, type: <class 'int'>\u001b[0m\n",
      "[\u001b[0;36mDEBUG\u001b[0m | src.tasks.clm_training.fabric.base]: \u001b[0;36miter_num value: 182, type: <class 'int'>\u001b[0m\n",
      "2025-06-16 13:55:43 - src.tasks.clm_training.fabric.base - \u001b[0;36mDEBUG\u001b[0m - \u001b[0;36mAbout to check: (iter_num + 1) % gradient_accumulation_steps != 0 -> (182 + 1) % 2 != 0\u001b[0m\n",
      "[\u001b[0;36mDEBUG\u001b[0m | src.tasks.clm_training.fabric.base]: \u001b[0;36mAbout to check: (iter_num + 1) % gradient_accumulation_steps != 0 -> (182 + 1) % 2 != 0\u001b[0m\n",
      "2025-06-16 13:55:43 - src.tasks.clm_training.fabric.base - \u001b[0;36mDEBUG\u001b[0m - \u001b[0;36miter 183 step 91: loss 5.9688, iter time: 122.64ms remaining time: \u001b[0m\n",
      "[\u001b[0;36mDEBUG\u001b[0m | src.tasks.clm_training.fabric.base]: \u001b[0;36miter 183 step 91: loss 5.9688, iter time: 122.64ms remaining time: \u001b[0m\n",
      "  4%|\u001b[34m█▌                                      \u001b[0m| 183/4682 [01:17<31:40,  2.37it/s]\u001b[0m2025-06-16 13:55:43 - src.tasks.clm_training.fabric.base - \u001b[0;36mDEBUG\u001b[0m - \u001b[0;36mgradient_accumulation_steps raw value: 2, type: <class 'int'>\u001b[0m\n",
      "[\u001b[0;36mDEBUG\u001b[0m | src.tasks.clm_training.fabric.base]: \u001b[0;36mgradient_accumulation_steps raw value: 2, type: <class 'int'>\u001b[0m\n",
      "2025-06-16 13:55:43 - src.tasks.clm_training.fabric.base - \u001b[0;36mDEBUG\u001b[0m - \u001b[0;36mgradient_accumulation_steps converted: 2, type: <class 'int'>\u001b[0m\n",
      "[\u001b[0;36mDEBUG\u001b[0m | src.tasks.clm_training.fabric.base]: \u001b[0;36mgradient_accumulation_steps converted: 2, type: <class 'int'>\u001b[0m\n",
      "2025-06-16 13:55:43 - src.tasks.clm_training.fabric.base - \u001b[0;36mDEBUG\u001b[0m - \u001b[0;36miter_num value: 183, type: <class 'int'>\u001b[0m\n",
      "[\u001b[0;36mDEBUG\u001b[0m | src.tasks.clm_training.fabric.base]: \u001b[0;36miter_num value: 183, type: <class 'int'>\u001b[0m\n",
      "2025-06-16 13:55:43 - src.tasks.clm_training.fabric.base - \u001b[0;36mDEBUG\u001b[0m - \u001b[0;36mAbout to check: (iter_num + 1) % gradient_accumulation_steps != 0 -> (183 + 1) % 2 != 0\u001b[0m\n",
      "[\u001b[0;36mDEBUG\u001b[0m | src.tasks.clm_training.fabric.base]: \u001b[0;36mAbout to check: (iter_num + 1) % gradient_accumulation_steps != 0 -> (183 + 1) % 2 != 0\u001b[0m\n",
      "2025-06-16 13:55:44 - src.tasks.clm_training.fabric.base - \u001b[0;36mDEBUG\u001b[0m - \u001b[0;36mGradient norm before clipping: 24.3750\u001b[0m\n",
      "[\u001b[0;36mDEBUG\u001b[0m | src.tasks.clm_training.fabric.base]: \u001b[0;36mGradient norm before clipping: 24.3750\u001b[0m\n",
      "2025-06-16 13:55:44 - src.tasks.clm_training.fabric.base - \u001b[0;36mDEBUG\u001b[0m - \u001b[0;36mvalidate_after_k_steps value: 1000, type: <class 'int'>\u001b[0m\n",
      "[\u001b[0;36mDEBUG\u001b[0m | src.tasks.clm_training.fabric.base]: \u001b[0;36mvalidate_after_k_steps value: 1000, type: <class 'int'>\u001b[0m\n",
      "2025-06-16 13:55:44 - src.tasks.clm_training.fabric.base - \u001b[0;36mDEBUG\u001b[0m - \u001b[0;36mstep_count value: 92, type: <class 'int'>\u001b[0m\n",
      "[\u001b[0;36mDEBUG\u001b[0m | src.tasks.clm_training.fabric.base]: \u001b[0;36mstep_count value: 92, type: <class 'int'>\u001b[0m\n",
      "2025-06-16 13:55:44 - src.tasks.clm_training.fabric.base - \u001b[0;36mDEBUG\u001b[0m - \u001b[0;36mValidation condition check - epochFinished: False, trainingFinished: False\u001b[0m\n",
      "[\u001b[0;36mDEBUG\u001b[0m | src.tasks.clm_training.fabric.base]: \u001b[0;36mValidation condition check - epochFinished: False, trainingFinished: False\u001b[0m\n",
      "2025-06-16 13:55:44 - src.tasks.clm_training.fabric.base - \u001b[0;36mDEBUG\u001b[0m - \u001b[0;36mAbout to check: step_count % validate_after_k_steps == 0 -> 92 % 1000 == 0\u001b[0m\n",
      "[\u001b[0;36mDEBUG\u001b[0m | src.tasks.clm_training.fabric.base]: \u001b[0;36mAbout to check: step_count % validate_after_k_steps == 0 -> 92 % 1000 == 0\u001b[0m\n",
      "2025-06-16 13:55:44 - src.tasks.clm_training.fabric.base - \u001b[0;36mDEBUG\u001b[0m - \u001b[0;36miter 184 step 92: loss 5.8125, iter time: 420.61ms remaining time: \u001b[0m\n",
      "[\u001b[0;36mDEBUG\u001b[0m | src.tasks.clm_training.fabric.base]: \u001b[0;36miter 184 step 92: loss 5.8125, iter time: 420.61ms remaining time: \u001b[0m\n",
      "  4%|\u001b[34m█▌                                      \u001b[0m| 184/4682 [01:18<31:50,  2.35it/s]\u001b[0m2025-06-16 13:55:44 - src.tasks.clm_training.fabric.base - \u001b[0;36mDEBUG\u001b[0m - \u001b[0;36mgradient_accumulation_steps raw value: 2, type: <class 'int'>\u001b[0m\n",
      "[\u001b[0;36mDEBUG\u001b[0m | src.tasks.clm_training.fabric.base]: \u001b[0;36mgradient_accumulation_steps raw value: 2, type: <class 'int'>\u001b[0m\n",
      "2025-06-16 13:55:44 - src.tasks.clm_training.fabric.base - \u001b[0;36mDEBUG\u001b[0m - \u001b[0;36mgradient_accumulation_steps converted: 2, type: <class 'int'>\u001b[0m\n",
      "[\u001b[0;36mDEBUG\u001b[0m | src.tasks.clm_training.fabric.base]: \u001b[0;36mgradient_accumulation_steps converted: 2, type: <class 'int'>\u001b[0m\n",
      "2025-06-16 13:55:44 - src.tasks.clm_training.fabric.base - \u001b[0;36mDEBUG\u001b[0m - \u001b[0;36miter_num value: 184, type: <class 'int'>\u001b[0m\n",
      "[\u001b[0;36mDEBUG\u001b[0m | src.tasks.clm_training.fabric.base]: \u001b[0;36miter_num value: 184, type: <class 'int'>\u001b[0m\n",
      "2025-06-16 13:55:44 - src.tasks.clm_training.fabric.base - \u001b[0;36mDEBUG\u001b[0m - \u001b[0;36mAbout to check: (iter_num + 1) % gradient_accumulation_steps != 0 -> (184 + 1) % 2 != 0\u001b[0m\n",
      "[\u001b[0;36mDEBUG\u001b[0m | src.tasks.clm_training.fabric.base]: \u001b[0;36mAbout to check: (iter_num + 1) % gradient_accumulation_steps != 0 -> (184 + 1) % 2 != 0\u001b[0m\n",
      "2025-06-16 13:55:44 - src.tasks.clm_training.fabric.base - \u001b[0;36mDEBUG\u001b[0m - \u001b[0;36miter 185 step 92: loss 5.6250, iter time: 122.40ms remaining time: \u001b[0m\n",
      "[\u001b[0;36mDEBUG\u001b[0m | src.tasks.clm_training.fabric.base]: \u001b[0;36miter 185 step 92: loss 5.6250, iter time: 122.40ms remaining time: \u001b[0m\n",
      "  4%|\u001b[34m█▌                                      \u001b[0m| 185/4682 [01:18<31:39,  2.37it/s]\u001b[0m2025-06-16 13:55:44 - src.tasks.clm_training.fabric.base - \u001b[0;36mDEBUG\u001b[0m - \u001b[0;36mgradient_accumulation_steps raw value: 2, type: <class 'int'>\u001b[0m\n",
      "[\u001b[0;36mDEBUG\u001b[0m | src.tasks.clm_training.fabric.base]: \u001b[0;36mgradient_accumulation_steps raw value: 2, type: <class 'int'>\u001b[0m\n",
      "2025-06-16 13:55:44 - src.tasks.clm_training.fabric.base - \u001b[0;36mDEBUG\u001b[0m - \u001b[0;36mgradient_accumulation_steps converted: 2, type: <class 'int'>\u001b[0m\n",
      "[\u001b[0;36mDEBUG\u001b[0m | src.tasks.clm_training.fabric.base]: \u001b[0;36mgradient_accumulation_steps converted: 2, type: <class 'int'>\u001b[0m\n",
      "2025-06-16 13:55:44 - src.tasks.clm_training.fabric.base - \u001b[0;36mDEBUG\u001b[0m - \u001b[0;36miter_num value: 185, type: <class 'int'>\u001b[0m\n",
      "[\u001b[0;36mDEBUG\u001b[0m | src.tasks.clm_training.fabric.base]: \u001b[0;36miter_num value: 185, type: <class 'int'>\u001b[0m\n",
      "2025-06-16 13:55:44 - src.tasks.clm_training.fabric.base - \u001b[0;36mDEBUG\u001b[0m - \u001b[0;36mAbout to check: (iter_num + 1) % gradient_accumulation_steps != 0 -> (185 + 1) % 2 != 0\u001b[0m\n",
      "[\u001b[0;36mDEBUG\u001b[0m | src.tasks.clm_training.fabric.base]: \u001b[0;36mAbout to check: (iter_num + 1) % gradient_accumulation_steps != 0 -> (185 + 1) % 2 != 0\u001b[0m\n",
      "2025-06-16 13:55:45 - src.tasks.clm_training.fabric.base - \u001b[0;36mDEBUG\u001b[0m - \u001b[0;36mGradient norm before clipping: 27.2500\u001b[0m\n",
      "[\u001b[0;36mDEBUG\u001b[0m | src.tasks.clm_training.fabric.base]: \u001b[0;36mGradient norm before clipping: 27.2500\u001b[0m\n",
      "2025-06-16 13:55:45 - src.tasks.clm_training.fabric.base - \u001b[0;36mDEBUG\u001b[0m - \u001b[0;36mvalidate_after_k_steps value: 1000, type: <class 'int'>\u001b[0m\n",
      "[\u001b[0;36mDEBUG\u001b[0m | src.tasks.clm_training.fabric.base]: \u001b[0;36mvalidate_after_k_steps value: 1000, type: <class 'int'>\u001b[0m\n",
      "2025-06-16 13:55:45 - src.tasks.clm_training.fabric.base - \u001b[0;36mDEBUG\u001b[0m - \u001b[0;36mstep_count value: 93, type: <class 'int'>\u001b[0m\n",
      "[\u001b[0;36mDEBUG\u001b[0m | src.tasks.clm_training.fabric.base]: \u001b[0;36mstep_count value: 93, type: <class 'int'>\u001b[0m\n",
      "2025-06-16 13:55:45 - src.tasks.clm_training.fabric.base - \u001b[0;36mDEBUG\u001b[0m - \u001b[0;36mValidation condition check - epochFinished: False, trainingFinished: False\u001b[0m\n",
      "[\u001b[0;36mDEBUG\u001b[0m | src.tasks.clm_training.fabric.base]: \u001b[0;36mValidation condition check - epochFinished: False, trainingFinished: False\u001b[0m\n",
      "2025-06-16 13:55:45 - src.tasks.clm_training.fabric.base - \u001b[0;36mDEBUG\u001b[0m - \u001b[0;36mAbout to check: step_count % validate_after_k_steps == 0 -> 93 % 1000 == 0\u001b[0m\n",
      "[\u001b[0;36mDEBUG\u001b[0m | src.tasks.clm_training.fabric.base]: \u001b[0;36mAbout to check: step_count % validate_after_k_steps == 0 -> 93 % 1000 == 0\u001b[0m\n",
      "2025-06-16 13:55:45 - src.tasks.clm_training.fabric.base - \u001b[0;36mDEBUG\u001b[0m - \u001b[0;36miter 186 step 93: loss 5.9062, iter time: 420.78ms remaining time: \u001b[0m\n",
      "[\u001b[0;36mDEBUG\u001b[0m | src.tasks.clm_training.fabric.base]: \u001b[0;36miter 186 step 93: loss 5.9062, iter time: 420.78ms remaining time: \u001b[0m\n",
      "  4%|\u001b[34m█▌                                      \u001b[0m| 186/4682 [01:18<31:50,  2.35it/s]\u001b[0m2025-06-16 13:55:45 - src.tasks.clm_training.fabric.base - \u001b[0;36mDEBUG\u001b[0m - \u001b[0;36mgradient_accumulation_steps raw value: 2, type: <class 'int'>\u001b[0m\n",
      "[\u001b[0;36mDEBUG\u001b[0m | src.tasks.clm_training.fabric.base]: \u001b[0;36mgradient_accumulation_steps raw value: 2, type: <class 'int'>\u001b[0m\n",
      "2025-06-16 13:55:45 - src.tasks.clm_training.fabric.base - \u001b[0;36mDEBUG\u001b[0m - \u001b[0;36mgradient_accumulation_steps converted: 2, type: <class 'int'>\u001b[0m\n",
      "[\u001b[0;36mDEBUG\u001b[0m | src.tasks.clm_training.fabric.base]: \u001b[0;36mgradient_accumulation_steps converted: 2, type: <class 'int'>\u001b[0m\n",
      "2025-06-16 13:55:45 - src.tasks.clm_training.fabric.base - \u001b[0;36mDEBUG\u001b[0m - \u001b[0;36miter_num value: 186, type: <class 'int'>\u001b[0m\n",
      "[\u001b[0;36mDEBUG\u001b[0m | src.tasks.clm_training.fabric.base]: \u001b[0;36miter_num value: 186, type: <class 'int'>\u001b[0m\n",
      "2025-06-16 13:55:45 - src.tasks.clm_training.fabric.base - \u001b[0;36mDEBUG\u001b[0m - \u001b[0;36mAbout to check: (iter_num + 1) % gradient_accumulation_steps != 0 -> (186 + 1) % 2 != 0\u001b[0m\n",
      "[\u001b[0;36mDEBUG\u001b[0m | src.tasks.clm_training.fabric.base]: \u001b[0;36mAbout to check: (iter_num + 1) % gradient_accumulation_steps != 0 -> (186 + 1) % 2 != 0\u001b[0m\n",
      "2025-06-16 13:55:45 - src.tasks.clm_training.fabric.base - \u001b[0;36mDEBUG\u001b[0m - \u001b[0;36miter 187 step 93: loss 5.8438, iter time: 122.69ms remaining time: \u001b[0m\n",
      "[\u001b[0;36mDEBUG\u001b[0m | src.tasks.clm_training.fabric.base]: \u001b[0;36miter 187 step 93: loss 5.8438, iter time: 122.69ms remaining time: \u001b[0m\n",
      "  4%|\u001b[34m█▌                                      \u001b[0m| 187/4682 [01:19<31:39,  2.37it/s]\u001b[0m2025-06-16 13:55:45 - src.tasks.clm_training.fabric.base - \u001b[0;36mDEBUG\u001b[0m - \u001b[0;36mgradient_accumulation_steps raw value: 2, type: <class 'int'>\u001b[0m\n",
      "[\u001b[0;36mDEBUG\u001b[0m | src.tasks.clm_training.fabric.base]: \u001b[0;36mgradient_accumulation_steps raw value: 2, type: <class 'int'>\u001b[0m\n",
      "2025-06-16 13:55:45 - src.tasks.clm_training.fabric.base - \u001b[0;36mDEBUG\u001b[0m - \u001b[0;36mgradient_accumulation_steps converted: 2, type: <class 'int'>\u001b[0m\n",
      "[\u001b[0;36mDEBUG\u001b[0m | src.tasks.clm_training.fabric.base]: \u001b[0;36mgradient_accumulation_steps converted: 2, type: <class 'int'>\u001b[0m\n",
      "2025-06-16 13:55:45 - src.tasks.clm_training.fabric.base - \u001b[0;36mDEBUG\u001b[0m - \u001b[0;36miter_num value: 187, type: <class 'int'>\u001b[0m\n",
      "[\u001b[0;36mDEBUG\u001b[0m | src.tasks.clm_training.fabric.base]: \u001b[0;36miter_num value: 187, type: <class 'int'>\u001b[0m\n",
      "2025-06-16 13:55:45 - src.tasks.clm_training.fabric.base - \u001b[0;36mDEBUG\u001b[0m - \u001b[0;36mAbout to check: (iter_num + 1) % gradient_accumulation_steps != 0 -> (187 + 1) % 2 != 0\u001b[0m\n",
      "[\u001b[0;36mDEBUG\u001b[0m | src.tasks.clm_training.fabric.base]: \u001b[0;36mAbout to check: (iter_num + 1) % gradient_accumulation_steps != 0 -> (187 + 1) % 2 != 0\u001b[0m\n",
      "2025-06-16 13:55:45 - src.tasks.clm_training.fabric.base - \u001b[0;36mDEBUG\u001b[0m - \u001b[0;36mGradient norm before clipping: 22.0000\u001b[0m\n",
      "[\u001b[0;36mDEBUG\u001b[0m | src.tasks.clm_training.fabric.base]: \u001b[0;36mGradient norm before clipping: 22.0000\u001b[0m\n",
      "2025-06-16 13:55:45 - src.tasks.clm_training.fabric.base - \u001b[0;36mDEBUG\u001b[0m - \u001b[0;36mvalidate_after_k_steps value: 1000, type: <class 'int'>\u001b[0m\n",
      "[\u001b[0;36mDEBUG\u001b[0m | src.tasks.clm_training.fabric.base]: \u001b[0;36mvalidate_after_k_steps value: 1000, type: <class 'int'>\u001b[0m\n",
      "2025-06-16 13:55:45 - src.tasks.clm_training.fabric.base - \u001b[0;36mDEBUG\u001b[0m - \u001b[0;36mstep_count value: 94, type: <class 'int'>\u001b[0m\n",
      "[\u001b[0;36mDEBUG\u001b[0m | src.tasks.clm_training.fabric.base]: \u001b[0;36mstep_count value: 94, type: <class 'int'>\u001b[0m\n",
      "2025-06-16 13:55:45 - src.tasks.clm_training.fabric.base - \u001b[0;36mDEBUG\u001b[0m - \u001b[0;36mValidation condition check - epochFinished: False, trainingFinished: False\u001b[0m\n",
      "[\u001b[0;36mDEBUG\u001b[0m | src.tasks.clm_training.fabric.base]: \u001b[0;36mValidation condition check - epochFinished: False, trainingFinished: False\u001b[0m\n",
      "2025-06-16 13:55:45 - src.tasks.clm_training.fabric.base - \u001b[0;36mDEBUG\u001b[0m - \u001b[0;36mAbout to check: step_count % validate_after_k_steps == 0 -> 94 % 1000 == 0\u001b[0m\n",
      "[\u001b[0;36mDEBUG\u001b[0m | src.tasks.clm_training.fabric.base]: \u001b[0;36mAbout to check: step_count % validate_after_k_steps == 0 -> 94 % 1000 == 0\u001b[0m\n",
      "2025-06-16 13:55:45 - src.tasks.clm_training.fabric.base - \u001b[0;36mDEBUG\u001b[0m - \u001b[0;36miter 188 step 94: loss 5.8750, iter time: 421.18ms remaining time: \u001b[0m\n",
      "[\u001b[0;36mDEBUG\u001b[0m | src.tasks.clm_training.fabric.base]: \u001b[0;36miter 188 step 94: loss 5.8750, iter time: 421.18ms remaining time: \u001b[0m\n",
      "  4%|\u001b[34m█▌                                      \u001b[0m| 188/4682 [01:19<31:51,  2.35it/s]\u001b[0m2025-06-16 13:55:45 - src.tasks.clm_training.fabric.base - \u001b[0;36mDEBUG\u001b[0m - \u001b[0;36mgradient_accumulation_steps raw value: 2, type: <class 'int'>\u001b[0m\n",
      "[\u001b[0;36mDEBUG\u001b[0m | src.tasks.clm_training.fabric.base]: \u001b[0;36mgradient_accumulation_steps raw value: 2, type: <class 'int'>\u001b[0m\n",
      "2025-06-16 13:55:45 - src.tasks.clm_training.fabric.base - \u001b[0;36mDEBUG\u001b[0m - \u001b[0;36mgradient_accumulation_steps converted: 2, type: <class 'int'>\u001b[0m\n",
      "[\u001b[0;36mDEBUG\u001b[0m | src.tasks.clm_training.fabric.base]: \u001b[0;36mgradient_accumulation_steps converted: 2, type: <class 'int'>\u001b[0m\n",
      "2025-06-16 13:55:45 - src.tasks.clm_training.fabric.base - \u001b[0;36mDEBUG\u001b[0m - \u001b[0;36miter_num value: 188, type: <class 'int'>\u001b[0m\n",
      "[\u001b[0;36mDEBUG\u001b[0m | src.tasks.clm_training.fabric.base]: \u001b[0;36miter_num value: 188, type: <class 'int'>\u001b[0m\n",
      "2025-06-16 13:55:45 - src.tasks.clm_training.fabric.base - \u001b[0;36mDEBUG\u001b[0m - \u001b[0;36mAbout to check: (iter_num + 1) % gradient_accumulation_steps != 0 -> (188 + 1) % 2 != 0\u001b[0m\n",
      "[\u001b[0;36mDEBUG\u001b[0m | src.tasks.clm_training.fabric.base]: \u001b[0;36mAbout to check: (iter_num + 1) % gradient_accumulation_steps != 0 -> (188 + 1) % 2 != 0\u001b[0m\n",
      "2025-06-16 13:55:46 - src.tasks.clm_training.fabric.base - \u001b[0;36mDEBUG\u001b[0m - \u001b[0;36miter 189 step 94: loss 5.9375, iter time: 122.70ms remaining time: \u001b[0m\n",
      "[\u001b[0;36mDEBUG\u001b[0m | src.tasks.clm_training.fabric.base]: \u001b[0;36miter 189 step 94: loss 5.9375, iter time: 122.70ms remaining time: \u001b[0m\n",
      "  4%|\u001b[34m█▌                                      \u001b[0m| 189/4682 [01:20<31:38,  2.37it/s]\u001b[0m2025-06-16 13:55:46 - src.tasks.clm_training.fabric.base - \u001b[0;36mDEBUG\u001b[0m - \u001b[0;36mgradient_accumulation_steps raw value: 2, type: <class 'int'>\u001b[0m\n",
      "[\u001b[0;36mDEBUG\u001b[0m | src.tasks.clm_training.fabric.base]: \u001b[0;36mgradient_accumulation_steps raw value: 2, type: <class 'int'>\u001b[0m\n",
      "2025-06-16 13:55:46 - src.tasks.clm_training.fabric.base - \u001b[0;36mDEBUG\u001b[0m - \u001b[0;36mgradient_accumulation_steps converted: 2, type: <class 'int'>\u001b[0m\n",
      "[\u001b[0;36mDEBUG\u001b[0m | src.tasks.clm_training.fabric.base]: \u001b[0;36mgradient_accumulation_steps converted: 2, type: <class 'int'>\u001b[0m\n",
      "2025-06-16 13:55:46 - src.tasks.clm_training.fabric.base - \u001b[0;36mDEBUG\u001b[0m - \u001b[0;36miter_num value: 189, type: <class 'int'>\u001b[0m\n",
      "[\u001b[0;36mDEBUG\u001b[0m | src.tasks.clm_training.fabric.base]: \u001b[0;36miter_num value: 189, type: <class 'int'>\u001b[0m\n",
      "2025-06-16 13:55:46 - src.tasks.clm_training.fabric.base - \u001b[0;36mDEBUG\u001b[0m - \u001b[0;36mAbout to check: (iter_num + 1) % gradient_accumulation_steps != 0 -> (189 + 1) % 2 != 0\u001b[0m\n",
      "[\u001b[0;36mDEBUG\u001b[0m | src.tasks.clm_training.fabric.base]: \u001b[0;36mAbout to check: (iter_num + 1) % gradient_accumulation_steps != 0 -> (189 + 1) % 2 != 0\u001b[0m\n",
      "2025-06-16 13:55:46 - src.tasks.clm_training.fabric.base - \u001b[0;36mDEBUG\u001b[0m - \u001b[0;36mGradient norm before clipping: 24.3750\u001b[0m\n",
      "[\u001b[0;36mDEBUG\u001b[0m | src.tasks.clm_training.fabric.base]: \u001b[0;36mGradient norm before clipping: 24.3750\u001b[0m\n",
      "2025-06-16 13:55:46 - src.tasks.clm_training.fabric.base - \u001b[0;36mDEBUG\u001b[0m - \u001b[0;36mvalidate_after_k_steps value: 1000, type: <class 'int'>\u001b[0m\n",
      "[\u001b[0;36mDEBUG\u001b[0m | src.tasks.clm_training.fabric.base]: \u001b[0;36mvalidate_after_k_steps value: 1000, type: <class 'int'>\u001b[0m\n",
      "2025-06-16 13:55:46 - src.tasks.clm_training.fabric.base - \u001b[0;36mDEBUG\u001b[0m - \u001b[0;36mstep_count value: 95, type: <class 'int'>\u001b[0m\n",
      "[\u001b[0;36mDEBUG\u001b[0m | src.tasks.clm_training.fabric.base]: \u001b[0;36mstep_count value: 95, type: <class 'int'>\u001b[0m\n",
      "2025-06-16 13:55:46 - src.tasks.clm_training.fabric.base - \u001b[0;36mDEBUG\u001b[0m - \u001b[0;36mValidation condition check - epochFinished: False, trainingFinished: False\u001b[0m\n",
      "[\u001b[0;36mDEBUG\u001b[0m | src.tasks.clm_training.fabric.base]: \u001b[0;36mValidation condition check - epochFinished: False, trainingFinished: False\u001b[0m\n",
      "2025-06-16 13:55:46 - src.tasks.clm_training.fabric.base - \u001b[0;36mDEBUG\u001b[0m - \u001b[0;36mAbout to check: step_count % validate_after_k_steps == 0 -> 95 % 1000 == 0\u001b[0m\n",
      "[\u001b[0;36mDEBUG\u001b[0m | src.tasks.clm_training.fabric.base]: \u001b[0;36mAbout to check: step_count % validate_after_k_steps == 0 -> 95 % 1000 == 0\u001b[0m\n",
      "2025-06-16 13:55:46 - src.tasks.clm_training.fabric.base - \u001b[0;36mDEBUG\u001b[0m - \u001b[0;36miter 190 step 95: loss 5.7812, iter time: 421.32ms remaining time: \u001b[0m\n",
      "[\u001b[0;36mDEBUG\u001b[0m | src.tasks.clm_training.fabric.base]: \u001b[0;36miter 190 step 95: loss 5.7812, iter time: 421.32ms remaining time: \u001b[0m\n",
      "  4%|\u001b[34m█▌                                      \u001b[0m| 190/4682 [01:20<31:50,  2.35it/s]\u001b[0m2025-06-16 13:55:46 - src.tasks.clm_training.fabric.base - \u001b[0;36mDEBUG\u001b[0m - \u001b[0;36mgradient_accumulation_steps raw value: 2, type: <class 'int'>\u001b[0m\n",
      "[\u001b[0;36mDEBUG\u001b[0m | src.tasks.clm_training.fabric.base]: \u001b[0;36mgradient_accumulation_steps raw value: 2, type: <class 'int'>\u001b[0m\n",
      "2025-06-16 13:55:46 - src.tasks.clm_training.fabric.base - \u001b[0;36mDEBUG\u001b[0m - \u001b[0;36mgradient_accumulation_steps converted: 2, type: <class 'int'>\u001b[0m\n",
      "[\u001b[0;36mDEBUG\u001b[0m | src.tasks.clm_training.fabric.base]: \u001b[0;36mgradient_accumulation_steps converted: 2, type: <class 'int'>\u001b[0m\n",
      "2025-06-16 13:55:46 - src.tasks.clm_training.fabric.base - \u001b[0;36mDEBUG\u001b[0m - \u001b[0;36miter_num value: 190, type: <class 'int'>\u001b[0m\n",
      "[\u001b[0;36mDEBUG\u001b[0m | src.tasks.clm_training.fabric.base]: \u001b[0;36miter_num value: 190, type: <class 'int'>\u001b[0m\n",
      "2025-06-16 13:55:46 - src.tasks.clm_training.fabric.base - \u001b[0;36mDEBUG\u001b[0m - \u001b[0;36mAbout to check: (iter_num + 1) % gradient_accumulation_steps != 0 -> (190 + 1) % 2 != 0\u001b[0m\n",
      "[\u001b[0;36mDEBUG\u001b[0m | src.tasks.clm_training.fabric.base]: \u001b[0;36mAbout to check: (iter_num + 1) % gradient_accumulation_steps != 0 -> (190 + 1) % 2 != 0\u001b[0m\n",
      "2025-06-16 13:55:47 - src.tasks.clm_training.fabric.base - \u001b[0;36mDEBUG\u001b[0m - \u001b[0;36miter 191 step 95: loss 5.6875, iter time: 122.06ms remaining time: \u001b[0m\n",
      "[\u001b[0;36mDEBUG\u001b[0m | src.tasks.clm_training.fabric.base]: \u001b[0;36miter 191 step 95: loss 5.6875, iter time: 122.06ms remaining time: \u001b[0m\n",
      "  4%|\u001b[34m█▋                                      \u001b[0m| 191/4682 [01:21<31:37,  2.37it/s]\u001b[0m2025-06-16 13:55:47 - src.tasks.clm_training.fabric.base - \u001b[0;36mDEBUG\u001b[0m - \u001b[0;36mgradient_accumulation_steps raw value: 2, type: <class 'int'>\u001b[0m\n",
      "[\u001b[0;36mDEBUG\u001b[0m | src.tasks.clm_training.fabric.base]: \u001b[0;36mgradient_accumulation_steps raw value: 2, type: <class 'int'>\u001b[0m\n",
      "2025-06-16 13:55:47 - src.tasks.clm_training.fabric.base - \u001b[0;36mDEBUG\u001b[0m - \u001b[0;36mgradient_accumulation_steps converted: 2, type: <class 'int'>\u001b[0m\n",
      "[\u001b[0;36mDEBUG\u001b[0m | src.tasks.clm_training.fabric.base]: \u001b[0;36mgradient_accumulation_steps converted: 2, type: <class 'int'>\u001b[0m\n",
      "2025-06-16 13:55:47 - src.tasks.clm_training.fabric.base - \u001b[0;36mDEBUG\u001b[0m - \u001b[0;36miter_num value: 191, type: <class 'int'>\u001b[0m\n",
      "[\u001b[0;36mDEBUG\u001b[0m | src.tasks.clm_training.fabric.base]: \u001b[0;36miter_num value: 191, type: <class 'int'>\u001b[0m\n",
      "2025-06-16 13:55:47 - src.tasks.clm_training.fabric.base - \u001b[0;36mDEBUG\u001b[0m - \u001b[0;36mAbout to check: (iter_num + 1) % gradient_accumulation_steps != 0 -> (191 + 1) % 2 != 0\u001b[0m\n",
      "[\u001b[0;36mDEBUG\u001b[0m | src.tasks.clm_training.fabric.base]: \u001b[0;36mAbout to check: (iter_num + 1) % gradient_accumulation_steps != 0 -> (191 + 1) % 2 != 0\u001b[0m\n",
      "2025-06-16 13:55:47 - src.tasks.clm_training.fabric.base - \u001b[0;36mDEBUG\u001b[0m - \u001b[0;36mGradient norm before clipping: 24.7500\u001b[0m\n",
      "[\u001b[0;36mDEBUG\u001b[0m | src.tasks.clm_training.fabric.base]: \u001b[0;36mGradient norm before clipping: 24.7500\u001b[0m\n",
      "2025-06-16 13:55:47 - src.tasks.clm_training.fabric.base - \u001b[0;36mDEBUG\u001b[0m - \u001b[0;36mvalidate_after_k_steps value: 1000, type: <class 'int'>\u001b[0m\n",
      "[\u001b[0;36mDEBUG\u001b[0m | src.tasks.clm_training.fabric.base]: \u001b[0;36mvalidate_after_k_steps value: 1000, type: <class 'int'>\u001b[0m\n",
      "2025-06-16 13:55:47 - src.tasks.clm_training.fabric.base - \u001b[0;36mDEBUG\u001b[0m - \u001b[0;36mstep_count value: 96, type: <class 'int'>\u001b[0m\n",
      "[\u001b[0;36mDEBUG\u001b[0m | src.tasks.clm_training.fabric.base]: \u001b[0;36mstep_count value: 96, type: <class 'int'>\u001b[0m\n",
      "2025-06-16 13:55:47 - src.tasks.clm_training.fabric.base - \u001b[0;36mDEBUG\u001b[0m - \u001b[0;36mValidation condition check - epochFinished: False, trainingFinished: False\u001b[0m\n",
      "[\u001b[0;36mDEBUG\u001b[0m | src.tasks.clm_training.fabric.base]: \u001b[0;36mValidation condition check - epochFinished: False, trainingFinished: False\u001b[0m\n",
      "2025-06-16 13:55:47 - src.tasks.clm_training.fabric.base - \u001b[0;36mDEBUG\u001b[0m - \u001b[0;36mAbout to check: step_count % validate_after_k_steps == 0 -> 96 % 1000 == 0\u001b[0m\n",
      "[\u001b[0;36mDEBUG\u001b[0m | src.tasks.clm_training.fabric.base]: \u001b[0;36mAbout to check: step_count % validate_after_k_steps == 0 -> 96 % 1000 == 0\u001b[0m\n",
      "2025-06-16 13:55:47 - src.tasks.clm_training.fabric.base - \u001b[0;36mDEBUG\u001b[0m - \u001b[0;36miter 192 step 96: loss 6.0000, iter time: 420.69ms remaining time: \u001b[0m\n",
      "[\u001b[0;36mDEBUG\u001b[0m | src.tasks.clm_training.fabric.base]: \u001b[0;36miter 192 step 96: loss 6.0000, iter time: 420.69ms remaining time: \u001b[0m\n",
      "  4%|\u001b[34m█▋                                      \u001b[0m| 192/4682 [01:21<31:47,  2.35it/s]\u001b[0m2025-06-16 13:55:47 - src.tasks.clm_training.fabric.base - \u001b[0;36mDEBUG\u001b[0m - \u001b[0;36mgradient_accumulation_steps raw value: 2, type: <class 'int'>\u001b[0m\n",
      "[\u001b[0;36mDEBUG\u001b[0m | src.tasks.clm_training.fabric.base]: \u001b[0;36mgradient_accumulation_steps raw value: 2, type: <class 'int'>\u001b[0m\n",
      "2025-06-16 13:55:47 - src.tasks.clm_training.fabric.base - \u001b[0;36mDEBUG\u001b[0m - \u001b[0;36mgradient_accumulation_steps converted: 2, type: <class 'int'>\u001b[0m\n",
      "[\u001b[0;36mDEBUG\u001b[0m | src.tasks.clm_training.fabric.base]: \u001b[0;36mgradient_accumulation_steps converted: 2, type: <class 'int'>\u001b[0m\n",
      "2025-06-16 13:55:47 - src.tasks.clm_training.fabric.base - \u001b[0;36mDEBUG\u001b[0m - \u001b[0;36miter_num value: 192, type: <class 'int'>\u001b[0m\n",
      "[\u001b[0;36mDEBUG\u001b[0m | src.tasks.clm_training.fabric.base]: \u001b[0;36miter_num value: 192, type: <class 'int'>\u001b[0m\n",
      "2025-06-16 13:55:47 - src.tasks.clm_training.fabric.base - \u001b[0;36mDEBUG\u001b[0m - \u001b[0;36mAbout to check: (iter_num + 1) % gradient_accumulation_steps != 0 -> (192 + 1) % 2 != 0\u001b[0m\n",
      "[\u001b[0;36mDEBUG\u001b[0m | src.tasks.clm_training.fabric.base]: \u001b[0;36mAbout to check: (iter_num + 1) % gradient_accumulation_steps != 0 -> (192 + 1) % 2 != 0\u001b[0m\n",
      "2025-06-16 13:55:47 - src.tasks.clm_training.fabric.base - \u001b[0;36mDEBUG\u001b[0m - \u001b[0;36miter 193 step 96: loss 5.9062, iter time: 122.27ms remaining time: \u001b[0m\n",
      "[\u001b[0;36mDEBUG\u001b[0m | src.tasks.clm_training.fabric.base]: \u001b[0;36miter 193 step 96: loss 5.9062, iter time: 122.27ms remaining time: \u001b[0m\n",
      "  4%|\u001b[34m█▋                                      \u001b[0m| 193/4682 [01:21<31:35,  2.37it/s]\u001b[0m2025-06-16 13:55:47 - src.tasks.clm_training.fabric.base - \u001b[0;36mDEBUG\u001b[0m - \u001b[0;36mgradient_accumulation_steps raw value: 2, type: <class 'int'>\u001b[0m\n",
      "[\u001b[0;36mDEBUG\u001b[0m | src.tasks.clm_training.fabric.base]: \u001b[0;36mgradient_accumulation_steps raw value: 2, type: <class 'int'>\u001b[0m\n",
      "2025-06-16 13:55:47 - src.tasks.clm_training.fabric.base - \u001b[0;36mDEBUG\u001b[0m - \u001b[0;36mgradient_accumulation_steps converted: 2, type: <class 'int'>\u001b[0m\n",
      "[\u001b[0;36mDEBUG\u001b[0m | src.tasks.clm_training.fabric.base]: \u001b[0;36mgradient_accumulation_steps converted: 2, type: <class 'int'>\u001b[0m\n",
      "2025-06-16 13:55:47 - src.tasks.clm_training.fabric.base - \u001b[0;36mDEBUG\u001b[0m - \u001b[0;36miter_num value: 193, type: <class 'int'>\u001b[0m\n",
      "[\u001b[0;36mDEBUG\u001b[0m | src.tasks.clm_training.fabric.base]: \u001b[0;36miter_num value: 193, type: <class 'int'>\u001b[0m\n",
      "2025-06-16 13:55:47 - src.tasks.clm_training.fabric.base - \u001b[0;36mDEBUG\u001b[0m - \u001b[0;36mAbout to check: (iter_num + 1) % gradient_accumulation_steps != 0 -> (193 + 1) % 2 != 0\u001b[0m\n",
      "[\u001b[0;36mDEBUG\u001b[0m | src.tasks.clm_training.fabric.base]: \u001b[0;36mAbout to check: (iter_num + 1) % gradient_accumulation_steps != 0 -> (193 + 1) % 2 != 0\u001b[0m\n",
      "2025-06-16 13:55:48 - src.tasks.clm_training.fabric.base - \u001b[0;36mDEBUG\u001b[0m - \u001b[0;36mGradient norm before clipping: 23.5000\u001b[0m\n",
      "[\u001b[0;36mDEBUG\u001b[0m | src.tasks.clm_training.fabric.base]: \u001b[0;36mGradient norm before clipping: 23.5000\u001b[0m\n",
      "2025-06-16 13:55:48 - src.tasks.clm_training.fabric.base - \u001b[0;36mDEBUG\u001b[0m - \u001b[0;36mvalidate_after_k_steps value: 1000, type: <class 'int'>\u001b[0m\n",
      "[\u001b[0;36mDEBUG\u001b[0m | src.tasks.clm_training.fabric.base]: \u001b[0;36mvalidate_after_k_steps value: 1000, type: <class 'int'>\u001b[0m\n",
      "2025-06-16 13:55:48 - src.tasks.clm_training.fabric.base - \u001b[0;36mDEBUG\u001b[0m - \u001b[0;36mstep_count value: 97, type: <class 'int'>\u001b[0m\n",
      "[\u001b[0;36mDEBUG\u001b[0m | src.tasks.clm_training.fabric.base]: \u001b[0;36mstep_count value: 97, type: <class 'int'>\u001b[0m\n",
      "2025-06-16 13:55:48 - src.tasks.clm_training.fabric.base - \u001b[0;36mDEBUG\u001b[0m - \u001b[0;36mValidation condition check - epochFinished: False, trainingFinished: False\u001b[0m\n",
      "[\u001b[0;36mDEBUG\u001b[0m | src.tasks.clm_training.fabric.base]: \u001b[0;36mValidation condition check - epochFinished: False, trainingFinished: False\u001b[0m\n",
      "2025-06-16 13:55:48 - src.tasks.clm_training.fabric.base - \u001b[0;36mDEBUG\u001b[0m - \u001b[0;36mAbout to check: step_count % validate_after_k_steps == 0 -> 97 % 1000 == 0\u001b[0m\n",
      "[\u001b[0;36mDEBUG\u001b[0m | src.tasks.clm_training.fabric.base]: \u001b[0;36mAbout to check: step_count % validate_after_k_steps == 0 -> 97 % 1000 == 0\u001b[0m\n",
      "2025-06-16 13:55:48 - src.tasks.clm_training.fabric.base - \u001b[0;36mDEBUG\u001b[0m - \u001b[0;36miter 194 step 97: loss 6.0312, iter time: 421.54ms remaining time: \u001b[0m\n",
      "[\u001b[0;36mDEBUG\u001b[0m | src.tasks.clm_training.fabric.base]: \u001b[0;36miter 194 step 97: loss 6.0312, iter time: 421.54ms remaining time: \u001b[0m\n",
      "  4%|\u001b[34m█▋                                      \u001b[0m| 194/4682 [01:22<31:47,  2.35it/s]\u001b[0m2025-06-16 13:55:48 - src.tasks.clm_training.fabric.base - \u001b[0;36mDEBUG\u001b[0m - \u001b[0;36mgradient_accumulation_steps raw value: 2, type: <class 'int'>\u001b[0m\n",
      "[\u001b[0;36mDEBUG\u001b[0m | src.tasks.clm_training.fabric.base]: \u001b[0;36mgradient_accumulation_steps raw value: 2, type: <class 'int'>\u001b[0m\n",
      "2025-06-16 13:55:48 - src.tasks.clm_training.fabric.base - \u001b[0;36mDEBUG\u001b[0m - \u001b[0;36mgradient_accumulation_steps converted: 2, type: <class 'int'>\u001b[0m\n",
      "[\u001b[0;36mDEBUG\u001b[0m | src.tasks.clm_training.fabric.base]: \u001b[0;36mgradient_accumulation_steps converted: 2, type: <class 'int'>\u001b[0m\n",
      "2025-06-16 13:55:48 - src.tasks.clm_training.fabric.base - \u001b[0;36mDEBUG\u001b[0m - \u001b[0;36miter_num value: 194, type: <class 'int'>\u001b[0m\n",
      "[\u001b[0;36mDEBUG\u001b[0m | src.tasks.clm_training.fabric.base]: \u001b[0;36miter_num value: 194, type: <class 'int'>\u001b[0m\n",
      "2025-06-16 13:55:48 - src.tasks.clm_training.fabric.base - \u001b[0;36mDEBUG\u001b[0m - \u001b[0;36mAbout to check: (iter_num + 1) % gradient_accumulation_steps != 0 -> (194 + 1) % 2 != 0\u001b[0m\n",
      "[\u001b[0;36mDEBUG\u001b[0m | src.tasks.clm_training.fabric.base]: \u001b[0;36mAbout to check: (iter_num + 1) % gradient_accumulation_steps != 0 -> (194 + 1) % 2 != 0\u001b[0m\n",
      "2025-06-16 13:55:48 - src.tasks.clm_training.fabric.base - \u001b[0;36mDEBUG\u001b[0m - \u001b[0;36miter 195 step 97: loss 5.8438, iter time: 122.76ms remaining time: \u001b[0m\n",
      "[\u001b[0;36mDEBUG\u001b[0m | src.tasks.clm_training.fabric.base]: \u001b[0;36miter 195 step 97: loss 5.8438, iter time: 122.76ms remaining time: \u001b[0m\n",
      "  4%|\u001b[34m█▋                                      \u001b[0m| 195/4682 [01:22<31:35,  2.37it/s]\u001b[0m2025-06-16 13:55:48 - src.tasks.clm_training.fabric.base - \u001b[0;36mDEBUG\u001b[0m - \u001b[0;36mgradient_accumulation_steps raw value: 2, type: <class 'int'>\u001b[0m\n",
      "[\u001b[0;36mDEBUG\u001b[0m | src.tasks.clm_training.fabric.base]: \u001b[0;36mgradient_accumulation_steps raw value: 2, type: <class 'int'>\u001b[0m\n",
      "2025-06-16 13:55:48 - src.tasks.clm_training.fabric.base - \u001b[0;36mDEBUG\u001b[0m - \u001b[0;36mgradient_accumulation_steps converted: 2, type: <class 'int'>\u001b[0m\n",
      "[\u001b[0;36mDEBUG\u001b[0m | src.tasks.clm_training.fabric.base]: \u001b[0;36mgradient_accumulation_steps converted: 2, type: <class 'int'>\u001b[0m\n",
      "2025-06-16 13:55:48 - src.tasks.clm_training.fabric.base - \u001b[0;36mDEBUG\u001b[0m - \u001b[0;36miter_num value: 195, type: <class 'int'>\u001b[0m\n",
      "[\u001b[0;36mDEBUG\u001b[0m | src.tasks.clm_training.fabric.base]: \u001b[0;36miter_num value: 195, type: <class 'int'>\u001b[0m\n",
      "2025-06-16 13:55:48 - src.tasks.clm_training.fabric.base - \u001b[0;36mDEBUG\u001b[0m - \u001b[0;36mAbout to check: (iter_num + 1) % gradient_accumulation_steps != 0 -> (195 + 1) % 2 != 0\u001b[0m\n",
      "[\u001b[0;36mDEBUG\u001b[0m | src.tasks.clm_training.fabric.base]: \u001b[0;36mAbout to check: (iter_num + 1) % gradient_accumulation_steps != 0 -> (195 + 1) % 2 != 0\u001b[0m\n",
      "2025-06-16 13:55:49 - src.tasks.clm_training.fabric.base - \u001b[0;36mDEBUG\u001b[0m - \u001b[0;36mGradient norm before clipping: 21.6250\u001b[0m\n",
      "[\u001b[0;36mDEBUG\u001b[0m | src.tasks.clm_training.fabric.base]: \u001b[0;36mGradient norm before clipping: 21.6250\u001b[0m\n",
      "2025-06-16 13:55:49 - src.tasks.clm_training.fabric.base - \u001b[0;36mDEBUG\u001b[0m - \u001b[0;36mvalidate_after_k_steps value: 1000, type: <class 'int'>\u001b[0m\n",
      "[\u001b[0;36mDEBUG\u001b[0m | src.tasks.clm_training.fabric.base]: \u001b[0;36mvalidate_after_k_steps value: 1000, type: <class 'int'>\u001b[0m\n",
      "2025-06-16 13:55:49 - src.tasks.clm_training.fabric.base - \u001b[0;36mDEBUG\u001b[0m - \u001b[0;36mstep_count value: 98, type: <class 'int'>\u001b[0m\n",
      "[\u001b[0;36mDEBUG\u001b[0m | src.tasks.clm_training.fabric.base]: \u001b[0;36mstep_count value: 98, type: <class 'int'>\u001b[0m\n",
      "2025-06-16 13:55:49 - src.tasks.clm_training.fabric.base - \u001b[0;36mDEBUG\u001b[0m - \u001b[0;36mValidation condition check - epochFinished: False, trainingFinished: False\u001b[0m\n",
      "[\u001b[0;36mDEBUG\u001b[0m | src.tasks.clm_training.fabric.base]: \u001b[0;36mValidation condition check - epochFinished: False, trainingFinished: False\u001b[0m\n",
      "2025-06-16 13:55:49 - src.tasks.clm_training.fabric.base - \u001b[0;36mDEBUG\u001b[0m - \u001b[0;36mAbout to check: step_count % validate_after_k_steps == 0 -> 98 % 1000 == 0\u001b[0m\n",
      "[\u001b[0;36mDEBUG\u001b[0m | src.tasks.clm_training.fabric.base]: \u001b[0;36mAbout to check: step_count % validate_after_k_steps == 0 -> 98 % 1000 == 0\u001b[0m\n",
      "2025-06-16 13:55:49 - src.tasks.clm_training.fabric.base - \u001b[0;36mDEBUG\u001b[0m - \u001b[0;36miter 196 step 98: loss 5.6562, iter time: 421.41ms remaining time: \u001b[0m\n",
      "[\u001b[0;36mDEBUG\u001b[0m | src.tasks.clm_training.fabric.base]: \u001b[0;36miter 196 step 98: loss 5.6562, iter time: 421.41ms remaining time: \u001b[0m\n",
      "  4%|\u001b[34m█▋                                      \u001b[0m| 196/4682 [01:23<31:46,  2.35it/s]\u001b[0m2025-06-16 13:55:49 - src.tasks.clm_training.fabric.base - \u001b[0;36mDEBUG\u001b[0m - \u001b[0;36mgradient_accumulation_steps raw value: 2, type: <class 'int'>\u001b[0m\n",
      "[\u001b[0;36mDEBUG\u001b[0m | src.tasks.clm_training.fabric.base]: \u001b[0;36mgradient_accumulation_steps raw value: 2, type: <class 'int'>\u001b[0m\n",
      "2025-06-16 13:55:49 - src.tasks.clm_training.fabric.base - \u001b[0;36mDEBUG\u001b[0m - \u001b[0;36mgradient_accumulation_steps converted: 2, type: <class 'int'>\u001b[0m\n",
      "[\u001b[0;36mDEBUG\u001b[0m | src.tasks.clm_training.fabric.base]: \u001b[0;36mgradient_accumulation_steps converted: 2, type: <class 'int'>\u001b[0m\n",
      "2025-06-16 13:55:49 - src.tasks.clm_training.fabric.base - \u001b[0;36mDEBUG\u001b[0m - \u001b[0;36miter_num value: 196, type: <class 'int'>\u001b[0m\n",
      "[\u001b[0;36mDEBUG\u001b[0m | src.tasks.clm_training.fabric.base]: \u001b[0;36miter_num value: 196, type: <class 'int'>\u001b[0m\n",
      "2025-06-16 13:55:49 - src.tasks.clm_training.fabric.base - \u001b[0;36mDEBUG\u001b[0m - \u001b[0;36mAbout to check: (iter_num + 1) % gradient_accumulation_steps != 0 -> (196 + 1) % 2 != 0\u001b[0m\n",
      "[\u001b[0;36mDEBUG\u001b[0m | src.tasks.clm_training.fabric.base]: \u001b[0;36mAbout to check: (iter_num + 1) % gradient_accumulation_steps != 0 -> (196 + 1) % 2 != 0\u001b[0m\n",
      "2025-06-16 13:55:49 - src.tasks.clm_training.fabric.base - \u001b[0;36mDEBUG\u001b[0m - \u001b[0;36miter 197 step 98: loss 5.7188, iter time: 122.55ms remaining time: \u001b[0m\n",
      "[\u001b[0;36mDEBUG\u001b[0m | src.tasks.clm_training.fabric.base]: \u001b[0;36miter 197 step 98: loss 5.7188, iter time: 122.55ms remaining time: \u001b[0m\n",
      "  4%|\u001b[34m█▋                                      \u001b[0m| 197/4682 [01:23<31:34,  2.37it/s]\u001b[0m2025-06-16 13:55:49 - src.tasks.clm_training.fabric.base - \u001b[0;36mDEBUG\u001b[0m - \u001b[0;36mgradient_accumulation_steps raw value: 2, type: <class 'int'>\u001b[0m\n",
      "[\u001b[0;36mDEBUG\u001b[0m | src.tasks.clm_training.fabric.base]: \u001b[0;36mgradient_accumulation_steps raw value: 2, type: <class 'int'>\u001b[0m\n",
      "2025-06-16 13:55:49 - src.tasks.clm_training.fabric.base - \u001b[0;36mDEBUG\u001b[0m - \u001b[0;36mgradient_accumulation_steps converted: 2, type: <class 'int'>\u001b[0m\n",
      "[\u001b[0;36mDEBUG\u001b[0m | src.tasks.clm_training.fabric.base]: \u001b[0;36mgradient_accumulation_steps converted: 2, type: <class 'int'>\u001b[0m\n",
      "2025-06-16 13:55:49 - src.tasks.clm_training.fabric.base - \u001b[0;36mDEBUG\u001b[0m - \u001b[0;36miter_num value: 197, type: <class 'int'>\u001b[0m\n",
      "[\u001b[0;36mDEBUG\u001b[0m | src.tasks.clm_training.fabric.base]: \u001b[0;36miter_num value: 197, type: <class 'int'>\u001b[0m\n",
      "2025-06-16 13:55:49 - src.tasks.clm_training.fabric.base - \u001b[0;36mDEBUG\u001b[0m - \u001b[0;36mAbout to check: (iter_num + 1) % gradient_accumulation_steps != 0 -> (197 + 1) % 2 != 0\u001b[0m\n",
      "[\u001b[0;36mDEBUG\u001b[0m | src.tasks.clm_training.fabric.base]: \u001b[0;36mAbout to check: (iter_num + 1) % gradient_accumulation_steps != 0 -> (197 + 1) % 2 != 0\u001b[0m\n",
      "2025-06-16 13:55:50 - src.tasks.clm_training.fabric.base - \u001b[0;36mDEBUG\u001b[0m - \u001b[0;36mGradient norm before clipping: 22.3750\u001b[0m\n",
      "[\u001b[0;36mDEBUG\u001b[0m | src.tasks.clm_training.fabric.base]: \u001b[0;36mGradient norm before clipping: 22.3750\u001b[0m\n",
      "2025-06-16 13:55:50 - src.tasks.clm_training.fabric.base - \u001b[0;36mDEBUG\u001b[0m - \u001b[0;36mvalidate_after_k_steps value: 1000, type: <class 'int'>\u001b[0m\n",
      "[\u001b[0;36mDEBUG\u001b[0m | src.tasks.clm_training.fabric.base]: \u001b[0;36mvalidate_after_k_steps value: 1000, type: <class 'int'>\u001b[0m\n",
      "2025-06-16 13:55:50 - src.tasks.clm_training.fabric.base - \u001b[0;36mDEBUG\u001b[0m - \u001b[0;36mstep_count value: 99, type: <class 'int'>\u001b[0m\n",
      "[\u001b[0;36mDEBUG\u001b[0m | src.tasks.clm_training.fabric.base]: \u001b[0;36mstep_count value: 99, type: <class 'int'>\u001b[0m\n",
      "2025-06-16 13:55:50 - src.tasks.clm_training.fabric.base - \u001b[0;36mDEBUG\u001b[0m - \u001b[0;36mValidation condition check - epochFinished: False, trainingFinished: False\u001b[0m\n",
      "[\u001b[0;36mDEBUG\u001b[0m | src.tasks.clm_training.fabric.base]: \u001b[0;36mValidation condition check - epochFinished: False, trainingFinished: False\u001b[0m\n",
      "2025-06-16 13:55:50 - src.tasks.clm_training.fabric.base - \u001b[0;36mDEBUG\u001b[0m - \u001b[0;36mAbout to check: step_count % validate_after_k_steps == 0 -> 99 % 1000 == 0\u001b[0m\n",
      "[\u001b[0;36mDEBUG\u001b[0m | src.tasks.clm_training.fabric.base]: \u001b[0;36mAbout to check: step_count % validate_after_k_steps == 0 -> 99 % 1000 == 0\u001b[0m\n",
      "2025-06-16 13:55:50 - src.tasks.clm_training.fabric.base - \u001b[0;36mDEBUG\u001b[0m - \u001b[0;36miter 198 step 99: loss 5.4375, iter time: 421.07ms remaining time: \u001b[0m\n",
      "[\u001b[0;36mDEBUG\u001b[0m | src.tasks.clm_training.fabric.base]: \u001b[0;36miter 198 step 99: loss 5.4375, iter time: 421.07ms remaining time: \u001b[0m\n",
      "  4%|\u001b[34m█▋                                      \u001b[0m| 198/4682 [01:24<31:45,  2.35it/s]\u001b[0m2025-06-16 13:55:50 - src.tasks.clm_training.fabric.base - \u001b[0;36mDEBUG\u001b[0m - \u001b[0;36mgradient_accumulation_steps raw value: 2, type: <class 'int'>\u001b[0m\n",
      "[\u001b[0;36mDEBUG\u001b[0m | src.tasks.clm_training.fabric.base]: \u001b[0;36mgradient_accumulation_steps raw value: 2, type: <class 'int'>\u001b[0m\n",
      "2025-06-16 13:55:50 - src.tasks.clm_training.fabric.base - \u001b[0;36mDEBUG\u001b[0m - \u001b[0;36mgradient_accumulation_steps converted: 2, type: <class 'int'>\u001b[0m\n",
      "[\u001b[0;36mDEBUG\u001b[0m | src.tasks.clm_training.fabric.base]: \u001b[0;36mgradient_accumulation_steps converted: 2, type: <class 'int'>\u001b[0m\n",
      "2025-06-16 13:55:50 - src.tasks.clm_training.fabric.base - \u001b[0;36mDEBUG\u001b[0m - \u001b[0;36miter_num value: 198, type: <class 'int'>\u001b[0m\n",
      "[\u001b[0;36mDEBUG\u001b[0m | src.tasks.clm_training.fabric.base]: \u001b[0;36miter_num value: 198, type: <class 'int'>\u001b[0m\n",
      "2025-06-16 13:55:50 - src.tasks.clm_training.fabric.base - \u001b[0;36mDEBUG\u001b[0m - \u001b[0;36mAbout to check: (iter_num + 1) % gradient_accumulation_steps != 0 -> (198 + 1) % 2 != 0\u001b[0m\n",
      "[\u001b[0;36mDEBUG\u001b[0m | src.tasks.clm_training.fabric.base]: \u001b[0;36mAbout to check: (iter_num + 1) % gradient_accumulation_steps != 0 -> (198 + 1) % 2 != 0\u001b[0m\n",
      "2025-06-16 13:55:50 - src.tasks.clm_training.fabric.base - \u001b[0;36mDEBUG\u001b[0m - \u001b[0;36miter 199 step 99: loss 5.7188, iter time: 120.92ms remaining time: \u001b[0m\n",
      "[\u001b[0;36mDEBUG\u001b[0m | src.tasks.clm_training.fabric.base]: \u001b[0;36miter 199 step 99: loss 5.7188, iter time: 120.92ms remaining time: \u001b[0m\n",
      "  4%|\u001b[34m█▋                                      \u001b[0m| 199/4682 [01:24<31:31,  2.37it/s]\u001b[0m2025-06-16 13:55:50 - src.tasks.clm_training.fabric.base - \u001b[0;36mDEBUG\u001b[0m - \u001b[0;36mgradient_accumulation_steps raw value: 2, type: <class 'int'>\u001b[0m\n",
      "[\u001b[0;36mDEBUG\u001b[0m | src.tasks.clm_training.fabric.base]: \u001b[0;36mgradient_accumulation_steps raw value: 2, type: <class 'int'>\u001b[0m\n",
      "2025-06-16 13:55:50 - src.tasks.clm_training.fabric.base - \u001b[0;36mDEBUG\u001b[0m - \u001b[0;36mgradient_accumulation_steps converted: 2, type: <class 'int'>\u001b[0m\n",
      "[\u001b[0;36mDEBUG\u001b[0m | src.tasks.clm_training.fabric.base]: \u001b[0;36mgradient_accumulation_steps converted: 2, type: <class 'int'>\u001b[0m\n",
      "2025-06-16 13:55:50 - src.tasks.clm_training.fabric.base - \u001b[0;36mDEBUG\u001b[0m - \u001b[0;36miter_num value: 199, type: <class 'int'>\u001b[0m\n",
      "[\u001b[0;36mDEBUG\u001b[0m | src.tasks.clm_training.fabric.base]: \u001b[0;36miter_num value: 199, type: <class 'int'>\u001b[0m\n",
      "2025-06-16 13:55:50 - src.tasks.clm_training.fabric.base - \u001b[0;36mDEBUG\u001b[0m - \u001b[0;36mAbout to check: (iter_num + 1) % gradient_accumulation_steps != 0 -> (199 + 1) % 2 != 0\u001b[0m\n",
      "[\u001b[0;36mDEBUG\u001b[0m | src.tasks.clm_training.fabric.base]: \u001b[0;36mAbout to check: (iter_num + 1) % gradient_accumulation_steps != 0 -> (199 + 1) % 2 != 0\u001b[0m\n",
      "2025-06-16 13:55:50 - src.tasks.clm_training.fabric.base - \u001b[0;36mDEBUG\u001b[0m - \u001b[0;36mGradient norm before clipping: 24.8750\u001b[0m\n",
      "[\u001b[0;36mDEBUG\u001b[0m | src.tasks.clm_training.fabric.base]: \u001b[0;36mGradient norm before clipping: 24.8750\u001b[0m\n",
      "2025-06-16 13:55:50 - src.tasks.clm_training.fabric.base - \u001b[0;36mDEBUG\u001b[0m - \u001b[0;36mvalidate_after_k_steps value: 1000, type: <class 'int'>\u001b[0m\n",
      "[\u001b[0;36mDEBUG\u001b[0m | src.tasks.clm_training.fabric.base]: \u001b[0;36mvalidate_after_k_steps value: 1000, type: <class 'int'>\u001b[0m\n",
      "2025-06-16 13:55:50 - src.tasks.clm_training.fabric.base - \u001b[0;36mDEBUG\u001b[0m - \u001b[0;36mstep_count value: 100, type: <class 'int'>\u001b[0m\n",
      "[\u001b[0;36mDEBUG\u001b[0m | src.tasks.clm_training.fabric.base]: \u001b[0;36mstep_count value: 100, type: <class 'int'>\u001b[0m\n",
      "2025-06-16 13:55:50 - src.tasks.clm_training.fabric.base - \u001b[0;36mDEBUG\u001b[0m - \u001b[0;36mValidation condition check - epochFinished: False, trainingFinished: False\u001b[0m\n",
      "[\u001b[0;36mDEBUG\u001b[0m | src.tasks.clm_training.fabric.base]: \u001b[0;36mValidation condition check - epochFinished: False, trainingFinished: False\u001b[0m\n",
      "2025-06-16 13:55:50 - src.tasks.clm_training.fabric.base - \u001b[0;36mDEBUG\u001b[0m - \u001b[0;36mAbout to check: step_count % validate_after_k_steps == 0 -> 100 % 1000 == 0\u001b[0m\n",
      "[\u001b[0;36mDEBUG\u001b[0m | src.tasks.clm_training.fabric.base]: \u001b[0;36mAbout to check: step_count % validate_after_k_steps == 0 -> 100 % 1000 == 0\u001b[0m\n",
      "2025-06-16 13:55:50 - src.tasks.clm_training.fabric.base - \u001b[0;36mDEBUG\u001b[0m - \u001b[0;36miter 200 step 100: loss 5.7812, iter time: 421.21ms remaining time: \u001b[0m\n",
      "[\u001b[0;36mDEBUG\u001b[0m | src.tasks.clm_training.fabric.base]: \u001b[0;36miter 200 step 100: loss 5.7812, iter time: 421.21ms remaining time: \u001b[0m\n",
      "  4%|\u001b[34m█▋                                      \u001b[0m| 200/4682 [01:24<31:43,  2.35it/s]\u001b[0m2025-06-16 13:55:50 - src.tasks.clm_training.fabric.base - \u001b[0;36mDEBUG\u001b[0m - \u001b[0;36mgradient_accumulation_steps raw value: 2, type: <class 'int'>\u001b[0m\n",
      "[\u001b[0;36mDEBUG\u001b[0m | src.tasks.clm_training.fabric.base]: \u001b[0;36mgradient_accumulation_steps raw value: 2, type: <class 'int'>\u001b[0m\n",
      "2025-06-16 13:55:50 - src.tasks.clm_training.fabric.base - \u001b[0;36mDEBUG\u001b[0m - \u001b[0;36mgradient_accumulation_steps converted: 2, type: <class 'int'>\u001b[0m\n",
      "[\u001b[0;36mDEBUG\u001b[0m | src.tasks.clm_training.fabric.base]: \u001b[0;36mgradient_accumulation_steps converted: 2, type: <class 'int'>\u001b[0m\n",
      "2025-06-16 13:55:50 - src.tasks.clm_training.fabric.base - \u001b[0;36mDEBUG\u001b[0m - \u001b[0;36miter_num value: 200, type: <class 'int'>\u001b[0m\n",
      "[\u001b[0;36mDEBUG\u001b[0m | src.tasks.clm_training.fabric.base]: \u001b[0;36miter_num value: 200, type: <class 'int'>\u001b[0m\n",
      "2025-06-16 13:55:50 - src.tasks.clm_training.fabric.base - \u001b[0;36mDEBUG\u001b[0m - \u001b[0;36mAbout to check: (iter_num + 1) % gradient_accumulation_steps != 0 -> (200 + 1) % 2 != 0\u001b[0m\n",
      "[\u001b[0;36mDEBUG\u001b[0m | src.tasks.clm_training.fabric.base]: \u001b[0;36mAbout to check: (iter_num + 1) % gradient_accumulation_steps != 0 -> (200 + 1) % 2 != 0\u001b[0m\n",
      "2025-06-16 13:55:51 - src.tasks.clm_training.fabric.base - \u001b[0;36mDEBUG\u001b[0m - \u001b[0;36miter 201 step 100: loss 6.1562, iter time: 122.40ms remaining time: \u001b[0m\n",
      "[\u001b[0;36mDEBUG\u001b[0m | src.tasks.clm_training.fabric.base]: \u001b[0;36miter 201 step 100: loss 6.1562, iter time: 122.40ms remaining time: \u001b[0m\n",
      "  4%|\u001b[34m█▋                                      \u001b[0m| 201/4682 [01:25<31:32,  2.37it/s]\u001b[0m2025-06-16 13:55:51 - src.tasks.clm_training.fabric.base - \u001b[0;36mDEBUG\u001b[0m - \u001b[0;36mgradient_accumulation_steps raw value: 2, type: <class 'int'>\u001b[0m\n",
      "[\u001b[0;36mDEBUG\u001b[0m | src.tasks.clm_training.fabric.base]: \u001b[0;36mgradient_accumulation_steps raw value: 2, type: <class 'int'>\u001b[0m\n",
      "2025-06-16 13:55:51 - src.tasks.clm_training.fabric.base - \u001b[0;36mDEBUG\u001b[0m - \u001b[0;36mgradient_accumulation_steps converted: 2, type: <class 'int'>\u001b[0m\n",
      "[\u001b[0;36mDEBUG\u001b[0m | src.tasks.clm_training.fabric.base]: \u001b[0;36mgradient_accumulation_steps converted: 2, type: <class 'int'>\u001b[0m\n",
      "2025-06-16 13:55:51 - src.tasks.clm_training.fabric.base - \u001b[0;36mDEBUG\u001b[0m - \u001b[0;36miter_num value: 201, type: <class 'int'>\u001b[0m\n",
      "[\u001b[0;36mDEBUG\u001b[0m | src.tasks.clm_training.fabric.base]: \u001b[0;36miter_num value: 201, type: <class 'int'>\u001b[0m\n",
      "2025-06-16 13:55:51 - src.tasks.clm_training.fabric.base - \u001b[0;36mDEBUG\u001b[0m - \u001b[0;36mAbout to check: (iter_num + 1) % gradient_accumulation_steps != 0 -> (201 + 1) % 2 != 0\u001b[0m\n",
      "[\u001b[0;36mDEBUG\u001b[0m | src.tasks.clm_training.fabric.base]: \u001b[0;36mAbout to check: (iter_num + 1) % gradient_accumulation_steps != 0 -> (201 + 1) % 2 != 0\u001b[0m\n",
      "2025-06-16 13:55:51 - src.tasks.clm_training.fabric.base - \u001b[0;36mDEBUG\u001b[0m - \u001b[0;36mGradient norm before clipping: 21.6250\u001b[0m\n",
      "[\u001b[0;36mDEBUG\u001b[0m | src.tasks.clm_training.fabric.base]: \u001b[0;36mGradient norm before clipping: 21.6250\u001b[0m\n",
      "2025-06-16 13:55:51 - src.tasks.clm_training.fabric.base - \u001b[0;36mDEBUG\u001b[0m - \u001b[0;36mvalidate_after_k_steps value: 1000, type: <class 'int'>\u001b[0m\n",
      "[\u001b[0;36mDEBUG\u001b[0m | src.tasks.clm_training.fabric.base]: \u001b[0;36mvalidate_after_k_steps value: 1000, type: <class 'int'>\u001b[0m\n",
      "2025-06-16 13:55:51 - src.tasks.clm_training.fabric.base - \u001b[0;36mDEBUG\u001b[0m - \u001b[0;36mstep_count value: 101, type: <class 'int'>\u001b[0m\n",
      "[\u001b[0;36mDEBUG\u001b[0m | src.tasks.clm_training.fabric.base]: \u001b[0;36mstep_count value: 101, type: <class 'int'>\u001b[0m\n",
      "2025-06-16 13:55:51 - src.tasks.clm_training.fabric.base - \u001b[0;36mDEBUG\u001b[0m - \u001b[0;36mValidation condition check - epochFinished: False, trainingFinished: False\u001b[0m\n",
      "[\u001b[0;36mDEBUG\u001b[0m | src.tasks.clm_training.fabric.base]: \u001b[0;36mValidation condition check - epochFinished: False, trainingFinished: False\u001b[0m\n",
      "2025-06-16 13:55:51 - src.tasks.clm_training.fabric.base - \u001b[0;36mDEBUG\u001b[0m - \u001b[0;36mAbout to check: step_count % validate_after_k_steps == 0 -> 101 % 1000 == 0\u001b[0m\n",
      "[\u001b[0;36mDEBUG\u001b[0m | src.tasks.clm_training.fabric.base]: \u001b[0;36mAbout to check: step_count % validate_after_k_steps == 0 -> 101 % 1000 == 0\u001b[0m\n",
      "2025-06-16 13:55:51 - src.tasks.clm_training.fabric.base - \u001b[0;36mDEBUG\u001b[0m - \u001b[0;36miter 202 step 101: loss 5.5625, iter time: 422.16ms remaining time: \u001b[0m\n",
      "[\u001b[0;36mDEBUG\u001b[0m | src.tasks.clm_training.fabric.base]: \u001b[0;36miter 202 step 101: loss 5.5625, iter time: 422.16ms remaining time: \u001b[0m\n",
      "  4%|\u001b[34m█▋                                      \u001b[0m| 202/4682 [01:25<31:45,  2.35it/s]\u001b[0m2025-06-16 13:55:51 - src.tasks.clm_training.fabric.base - \u001b[0;36mDEBUG\u001b[0m - \u001b[0;36mgradient_accumulation_steps raw value: 2, type: <class 'int'>\u001b[0m\n",
      "[\u001b[0;36mDEBUG\u001b[0m | src.tasks.clm_training.fabric.base]: \u001b[0;36mgradient_accumulation_steps raw value: 2, type: <class 'int'>\u001b[0m\n",
      "2025-06-16 13:55:51 - src.tasks.clm_training.fabric.base - \u001b[0;36mDEBUG\u001b[0m - \u001b[0;36mgradient_accumulation_steps converted: 2, type: <class 'int'>\u001b[0m\n",
      "[\u001b[0;36mDEBUG\u001b[0m | src.tasks.clm_training.fabric.base]: \u001b[0;36mgradient_accumulation_steps converted: 2, type: <class 'int'>\u001b[0m\n",
      "2025-06-16 13:55:51 - src.tasks.clm_training.fabric.base - \u001b[0;36mDEBUG\u001b[0m - \u001b[0;36miter_num value: 202, type: <class 'int'>\u001b[0m\n",
      "[\u001b[0;36mDEBUG\u001b[0m | src.tasks.clm_training.fabric.base]: \u001b[0;36miter_num value: 202, type: <class 'int'>\u001b[0m\n",
      "2025-06-16 13:55:51 - src.tasks.clm_training.fabric.base - \u001b[0;36mDEBUG\u001b[0m - \u001b[0;36mAbout to check: (iter_num + 1) % gradient_accumulation_steps != 0 -> (202 + 1) % 2 != 0\u001b[0m\n",
      "[\u001b[0;36mDEBUG\u001b[0m | src.tasks.clm_training.fabric.base]: \u001b[0;36mAbout to check: (iter_num + 1) % gradient_accumulation_steps != 0 -> (202 + 1) % 2 != 0\u001b[0m\n",
      "2025-06-16 13:55:52 - src.tasks.clm_training.fabric.base - \u001b[0;36mDEBUG\u001b[0m - \u001b[0;36miter 203 step 101: loss 5.9062, iter time: 122.56ms remaining time: \u001b[0m\n",
      "[\u001b[0;36mDEBUG\u001b[0m | src.tasks.clm_training.fabric.base]: \u001b[0;36miter 203 step 101: loss 5.9062, iter time: 122.56ms remaining time: \u001b[0m\n",
      "  4%|\u001b[34m█▋                                      \u001b[0m| 203/4682 [01:26<31:32,  2.37it/s]\u001b[0m2025-06-16 13:55:52 - src.tasks.clm_training.fabric.base - \u001b[0;36mDEBUG\u001b[0m - \u001b[0;36mgradient_accumulation_steps raw value: 2, type: <class 'int'>\u001b[0m\n",
      "[\u001b[0;36mDEBUG\u001b[0m | src.tasks.clm_training.fabric.base]: \u001b[0;36mgradient_accumulation_steps raw value: 2, type: <class 'int'>\u001b[0m\n",
      "2025-06-16 13:55:52 - src.tasks.clm_training.fabric.base - \u001b[0;36mDEBUG\u001b[0m - \u001b[0;36mgradient_accumulation_steps converted: 2, type: <class 'int'>\u001b[0m\n",
      "[\u001b[0;36mDEBUG\u001b[0m | src.tasks.clm_training.fabric.base]: \u001b[0;36mgradient_accumulation_steps converted: 2, type: <class 'int'>\u001b[0m\n",
      "2025-06-16 13:55:52 - src.tasks.clm_training.fabric.base - \u001b[0;36mDEBUG\u001b[0m - \u001b[0;36miter_num value: 203, type: <class 'int'>\u001b[0m\n",
      "[\u001b[0;36mDEBUG\u001b[0m | src.tasks.clm_training.fabric.base]: \u001b[0;36miter_num value: 203, type: <class 'int'>\u001b[0m\n",
      "2025-06-16 13:55:52 - src.tasks.clm_training.fabric.base - \u001b[0;36mDEBUG\u001b[0m - \u001b[0;36mAbout to check: (iter_num + 1) % gradient_accumulation_steps != 0 -> (203 + 1) % 2 != 0\u001b[0m\n",
      "[\u001b[0;36mDEBUG\u001b[0m | src.tasks.clm_training.fabric.base]: \u001b[0;36mAbout to check: (iter_num + 1) % gradient_accumulation_steps != 0 -> (203 + 1) % 2 != 0\u001b[0m\n",
      "2025-06-16 13:55:52 - src.tasks.clm_training.fabric.base - \u001b[0;36mDEBUG\u001b[0m - \u001b[0;36mGradient norm before clipping: 23.6250\u001b[0m\n",
      "[\u001b[0;36mDEBUG\u001b[0m | src.tasks.clm_training.fabric.base]: \u001b[0;36mGradient norm before clipping: 23.6250\u001b[0m\n",
      "2025-06-16 13:55:52 - src.tasks.clm_training.fabric.base - \u001b[0;36mDEBUG\u001b[0m - \u001b[0;36mvalidate_after_k_steps value: 1000, type: <class 'int'>\u001b[0m\n",
      "[\u001b[0;36mDEBUG\u001b[0m | src.tasks.clm_training.fabric.base]: \u001b[0;36mvalidate_after_k_steps value: 1000, type: <class 'int'>\u001b[0m\n",
      "2025-06-16 13:55:52 - src.tasks.clm_training.fabric.base - \u001b[0;36mDEBUG\u001b[0m - \u001b[0;36mstep_count value: 102, type: <class 'int'>\u001b[0m\n",
      "[\u001b[0;36mDEBUG\u001b[0m | src.tasks.clm_training.fabric.base]: \u001b[0;36mstep_count value: 102, type: <class 'int'>\u001b[0m\n",
      "2025-06-16 13:55:52 - src.tasks.clm_training.fabric.base - \u001b[0;36mDEBUG\u001b[0m - \u001b[0;36mValidation condition check - epochFinished: False, trainingFinished: False\u001b[0m\n",
      "[\u001b[0;36mDEBUG\u001b[0m | src.tasks.clm_training.fabric.base]: \u001b[0;36mValidation condition check - epochFinished: False, trainingFinished: False\u001b[0m\n",
      "2025-06-16 13:55:52 - src.tasks.clm_training.fabric.base - \u001b[0;36mDEBUG\u001b[0m - \u001b[0;36mAbout to check: step_count % validate_after_k_steps == 0 -> 102 % 1000 == 0\u001b[0m\n",
      "[\u001b[0;36mDEBUG\u001b[0m | src.tasks.clm_training.fabric.base]: \u001b[0;36mAbout to check: step_count % validate_after_k_steps == 0 -> 102 % 1000 == 0\u001b[0m\n",
      "2025-06-16 13:55:52 - src.tasks.clm_training.fabric.base - \u001b[0;36mDEBUG\u001b[0m - \u001b[0;36miter 204 step 102: loss 5.6562, iter time: 421.08ms remaining time: \u001b[0m\n",
      "[\u001b[0;36mDEBUG\u001b[0m | src.tasks.clm_training.fabric.base]: \u001b[0;36miter 204 step 102: loss 5.6562, iter time: 421.08ms remaining time: \u001b[0m\n",
      "  4%|\u001b[34m█▋                                      \u001b[0m| 204/4682 [01:26<31:43,  2.35it/s]\u001b[0m2025-06-16 13:55:52 - src.tasks.clm_training.fabric.base - \u001b[0;36mDEBUG\u001b[0m - \u001b[0;36mgradient_accumulation_steps raw value: 2, type: <class 'int'>\u001b[0m\n",
      "[\u001b[0;36mDEBUG\u001b[0m | src.tasks.clm_training.fabric.base]: \u001b[0;36mgradient_accumulation_steps raw value: 2, type: <class 'int'>\u001b[0m\n",
      "2025-06-16 13:55:52 - src.tasks.clm_training.fabric.base - \u001b[0;36mDEBUG\u001b[0m - \u001b[0;36mgradient_accumulation_steps converted: 2, type: <class 'int'>\u001b[0m\n",
      "[\u001b[0;36mDEBUG\u001b[0m | src.tasks.clm_training.fabric.base]: \u001b[0;36mgradient_accumulation_steps converted: 2, type: <class 'int'>\u001b[0m\n",
      "2025-06-16 13:55:52 - src.tasks.clm_training.fabric.base - \u001b[0;36mDEBUG\u001b[0m - \u001b[0;36miter_num value: 204, type: <class 'int'>\u001b[0m\n",
      "[\u001b[0;36mDEBUG\u001b[0m | src.tasks.clm_training.fabric.base]: \u001b[0;36miter_num value: 204, type: <class 'int'>\u001b[0m\n",
      "2025-06-16 13:55:52 - src.tasks.clm_training.fabric.base - \u001b[0;36mDEBUG\u001b[0m - \u001b[0;36mAbout to check: (iter_num + 1) % gradient_accumulation_steps != 0 -> (204 + 1) % 2 != 0\u001b[0m\n",
      "[\u001b[0;36mDEBUG\u001b[0m | src.tasks.clm_training.fabric.base]: \u001b[0;36mAbout to check: (iter_num + 1) % gradient_accumulation_steps != 0 -> (204 + 1) % 2 != 0\u001b[0m\n",
      "2025-06-16 13:55:53 - src.tasks.clm_training.fabric.base - \u001b[0;36mDEBUG\u001b[0m - \u001b[0;36miter 205 step 102: loss 5.6562, iter time: 122.36ms remaining time: \u001b[0m\n",
      "[\u001b[0;36mDEBUG\u001b[0m | src.tasks.clm_training.fabric.base]: \u001b[0;36miter 205 step 102: loss 5.6562, iter time: 122.36ms remaining time: \u001b[0m\n",
      "  4%|\u001b[34m█▊                                      \u001b[0m| 205/4682 [01:26<31:30,  2.37it/s]\u001b[0m2025-06-16 13:55:53 - src.tasks.clm_training.fabric.base - \u001b[0;36mDEBUG\u001b[0m - \u001b[0;36mgradient_accumulation_steps raw value: 2, type: <class 'int'>\u001b[0m\n",
      "[\u001b[0;36mDEBUG\u001b[0m | src.tasks.clm_training.fabric.base]: \u001b[0;36mgradient_accumulation_steps raw value: 2, type: <class 'int'>\u001b[0m\n",
      "2025-06-16 13:55:53 - src.tasks.clm_training.fabric.base - \u001b[0;36mDEBUG\u001b[0m - \u001b[0;36mgradient_accumulation_steps converted: 2, type: <class 'int'>\u001b[0m\n",
      "[\u001b[0;36mDEBUG\u001b[0m | src.tasks.clm_training.fabric.base]: \u001b[0;36mgradient_accumulation_steps converted: 2, type: <class 'int'>\u001b[0m\n",
      "2025-06-16 13:55:53 - src.tasks.clm_training.fabric.base - \u001b[0;36mDEBUG\u001b[0m - \u001b[0;36miter_num value: 205, type: <class 'int'>\u001b[0m\n",
      "[\u001b[0;36mDEBUG\u001b[0m | src.tasks.clm_training.fabric.base]: \u001b[0;36miter_num value: 205, type: <class 'int'>\u001b[0m\n",
      "2025-06-16 13:55:53 - src.tasks.clm_training.fabric.base - \u001b[0;36mDEBUG\u001b[0m - \u001b[0;36mAbout to check: (iter_num + 1) % gradient_accumulation_steps != 0 -> (205 + 1) % 2 != 0\u001b[0m\n",
      "[\u001b[0;36mDEBUG\u001b[0m | src.tasks.clm_training.fabric.base]: \u001b[0;36mAbout to check: (iter_num + 1) % gradient_accumulation_steps != 0 -> (205 + 1) % 2 != 0\u001b[0m\n",
      "2025-06-16 13:55:53 - src.tasks.clm_training.fabric.base - \u001b[0;36mDEBUG\u001b[0m - \u001b[0;36mGradient norm before clipping: 22.0000\u001b[0m\n",
      "[\u001b[0;36mDEBUG\u001b[0m | src.tasks.clm_training.fabric.base]: \u001b[0;36mGradient norm before clipping: 22.0000\u001b[0m\n",
      "2025-06-16 13:55:53 - src.tasks.clm_training.fabric.base - \u001b[0;36mDEBUG\u001b[0m - \u001b[0;36mvalidate_after_k_steps value: 1000, type: <class 'int'>\u001b[0m\n",
      "[\u001b[0;36mDEBUG\u001b[0m | src.tasks.clm_training.fabric.base]: \u001b[0;36mvalidate_after_k_steps value: 1000, type: <class 'int'>\u001b[0m\n",
      "2025-06-16 13:55:53 - src.tasks.clm_training.fabric.base - \u001b[0;36mDEBUG\u001b[0m - \u001b[0;36mstep_count value: 103, type: <class 'int'>\u001b[0m\n",
      "[\u001b[0;36mDEBUG\u001b[0m | src.tasks.clm_training.fabric.base]: \u001b[0;36mstep_count value: 103, type: <class 'int'>\u001b[0m\n",
      "2025-06-16 13:55:53 - src.tasks.clm_training.fabric.base - \u001b[0;36mDEBUG\u001b[0m - \u001b[0;36mValidation condition check - epochFinished: False, trainingFinished: False\u001b[0m\n",
      "[\u001b[0;36mDEBUG\u001b[0m | src.tasks.clm_training.fabric.base]: \u001b[0;36mValidation condition check - epochFinished: False, trainingFinished: False\u001b[0m\n",
      "2025-06-16 13:55:53 - src.tasks.clm_training.fabric.base - \u001b[0;36mDEBUG\u001b[0m - \u001b[0;36mAbout to check: step_count % validate_after_k_steps == 0 -> 103 % 1000 == 0\u001b[0m\n",
      "[\u001b[0;36mDEBUG\u001b[0m | src.tasks.clm_training.fabric.base]: \u001b[0;36mAbout to check: step_count % validate_after_k_steps == 0 -> 103 % 1000 == 0\u001b[0m\n",
      "2025-06-16 13:55:53 - src.tasks.clm_training.fabric.base - \u001b[0;36mDEBUG\u001b[0m - \u001b[0;36miter 206 step 103: loss 6.3750, iter time: 420.73ms remaining time: \u001b[0m\n",
      "[\u001b[0;36mDEBUG\u001b[0m | src.tasks.clm_training.fabric.base]: \u001b[0;36miter 206 step 103: loss 6.3750, iter time: 420.73ms remaining time: \u001b[0m\n",
      "  4%|\u001b[34m█▊                                      \u001b[0m| 206/4682 [01:27<31:41,  2.35it/s]\u001b[0m2025-06-16 13:55:53 - src.tasks.clm_training.fabric.base - \u001b[0;36mDEBUG\u001b[0m - \u001b[0;36mgradient_accumulation_steps raw value: 2, type: <class 'int'>\u001b[0m\n",
      "[\u001b[0;36mDEBUG\u001b[0m | src.tasks.clm_training.fabric.base]: \u001b[0;36mgradient_accumulation_steps raw value: 2, type: <class 'int'>\u001b[0m\n",
      "2025-06-16 13:55:53 - src.tasks.clm_training.fabric.base - \u001b[0;36mDEBUG\u001b[0m - \u001b[0;36mgradient_accumulation_steps converted: 2, type: <class 'int'>\u001b[0m\n",
      "[\u001b[0;36mDEBUG\u001b[0m | src.tasks.clm_training.fabric.base]: \u001b[0;36mgradient_accumulation_steps converted: 2, type: <class 'int'>\u001b[0m\n",
      "2025-06-16 13:55:53 - src.tasks.clm_training.fabric.base - \u001b[0;36mDEBUG\u001b[0m - \u001b[0;36miter_num value: 206, type: <class 'int'>\u001b[0m\n",
      "[\u001b[0;36mDEBUG\u001b[0m | src.tasks.clm_training.fabric.base]: \u001b[0;36miter_num value: 206, type: <class 'int'>\u001b[0m\n",
      "2025-06-16 13:55:53 - src.tasks.clm_training.fabric.base - \u001b[0;36mDEBUG\u001b[0m - \u001b[0;36mAbout to check: (iter_num + 1) % gradient_accumulation_steps != 0 -> (206 + 1) % 2 != 0\u001b[0m\n",
      "[\u001b[0;36mDEBUG\u001b[0m | src.tasks.clm_training.fabric.base]: \u001b[0;36mAbout to check: (iter_num + 1) % gradient_accumulation_steps != 0 -> (206 + 1) % 2 != 0\u001b[0m\n",
      "2025-06-16 13:55:53 - src.tasks.clm_training.fabric.base - \u001b[0;36mDEBUG\u001b[0m - \u001b[0;36miter 207 step 103: loss 5.4062, iter time: 122.75ms remaining time: \u001b[0m\n",
      "[\u001b[0;36mDEBUG\u001b[0m | src.tasks.clm_training.fabric.base]: \u001b[0;36miter 207 step 103: loss 5.4062, iter time: 122.75ms remaining time: \u001b[0m\n",
      "  4%|\u001b[34m█▊                                      \u001b[0m| 207/4682 [01:27<31:30,  2.37it/s]\u001b[0m2025-06-16 13:55:53 - src.tasks.clm_training.fabric.base - \u001b[0;36mDEBUG\u001b[0m - \u001b[0;36mgradient_accumulation_steps raw value: 2, type: <class 'int'>\u001b[0m\n",
      "[\u001b[0;36mDEBUG\u001b[0m | src.tasks.clm_training.fabric.base]: \u001b[0;36mgradient_accumulation_steps raw value: 2, type: <class 'int'>\u001b[0m\n",
      "2025-06-16 13:55:53 - src.tasks.clm_training.fabric.base - \u001b[0;36mDEBUG\u001b[0m - \u001b[0;36mgradient_accumulation_steps converted: 2, type: <class 'int'>\u001b[0m\n",
      "[\u001b[0;36mDEBUG\u001b[0m | src.tasks.clm_training.fabric.base]: \u001b[0;36mgradient_accumulation_steps converted: 2, type: <class 'int'>\u001b[0m\n",
      "2025-06-16 13:55:53 - src.tasks.clm_training.fabric.base - \u001b[0;36mDEBUG\u001b[0m - \u001b[0;36miter_num value: 207, type: <class 'int'>\u001b[0m\n",
      "[\u001b[0;36mDEBUG\u001b[0m | src.tasks.clm_training.fabric.base]: \u001b[0;36miter_num value: 207, type: <class 'int'>\u001b[0m\n",
      "2025-06-16 13:55:53 - src.tasks.clm_training.fabric.base - \u001b[0;36mDEBUG\u001b[0m - \u001b[0;36mAbout to check: (iter_num + 1) % gradient_accumulation_steps != 0 -> (207 + 1) % 2 != 0\u001b[0m\n",
      "[\u001b[0;36mDEBUG\u001b[0m | src.tasks.clm_training.fabric.base]: \u001b[0;36mAbout to check: (iter_num + 1) % gradient_accumulation_steps != 0 -> (207 + 1) % 2 != 0\u001b[0m\n",
      "2025-06-16 13:55:54 - src.tasks.clm_training.fabric.base - \u001b[0;36mDEBUG\u001b[0m - \u001b[0;36mGradient norm before clipping: 22.8750\u001b[0m\n",
      "[\u001b[0;36mDEBUG\u001b[0m | src.tasks.clm_training.fabric.base]: \u001b[0;36mGradient norm before clipping: 22.8750\u001b[0m\n",
      "2025-06-16 13:55:54 - src.tasks.clm_training.fabric.base - \u001b[0;36mDEBUG\u001b[0m - \u001b[0;36mvalidate_after_k_steps value: 1000, type: <class 'int'>\u001b[0m\n",
      "[\u001b[0;36mDEBUG\u001b[0m | src.tasks.clm_training.fabric.base]: \u001b[0;36mvalidate_after_k_steps value: 1000, type: <class 'int'>\u001b[0m\n",
      "2025-06-16 13:55:54 - src.tasks.clm_training.fabric.base - \u001b[0;36mDEBUG\u001b[0m - \u001b[0;36mstep_count value: 104, type: <class 'int'>\u001b[0m\n",
      "[\u001b[0;36mDEBUG\u001b[0m | src.tasks.clm_training.fabric.base]: \u001b[0;36mstep_count value: 104, type: <class 'int'>\u001b[0m\n",
      "2025-06-16 13:55:54 - src.tasks.clm_training.fabric.base - \u001b[0;36mDEBUG\u001b[0m - \u001b[0;36mValidation condition check - epochFinished: False, trainingFinished: False\u001b[0m\n",
      "[\u001b[0;36mDEBUG\u001b[0m | src.tasks.clm_training.fabric.base]: \u001b[0;36mValidation condition check - epochFinished: False, trainingFinished: False\u001b[0m\n",
      "2025-06-16 13:55:54 - src.tasks.clm_training.fabric.base - \u001b[0;36mDEBUG\u001b[0m - \u001b[0;36mAbout to check: step_count % validate_after_k_steps == 0 -> 104 % 1000 == 0\u001b[0m\n",
      "[\u001b[0;36mDEBUG\u001b[0m | src.tasks.clm_training.fabric.base]: \u001b[0;36mAbout to check: step_count % validate_after_k_steps == 0 -> 104 % 1000 == 0\u001b[0m\n",
      "2025-06-16 13:55:54 - src.tasks.clm_training.fabric.base - \u001b[0;36mDEBUG\u001b[0m - \u001b[0;36miter 208 step 104: loss 5.9062, iter time: 420.75ms remaining time: \u001b[0m\n",
      "[\u001b[0;36mDEBUG\u001b[0m | src.tasks.clm_training.fabric.base]: \u001b[0;36miter 208 step 104: loss 5.9062, iter time: 420.75ms remaining time: \u001b[0m\n",
      "  4%|\u001b[34m█▊                                      \u001b[0m| 208/4682 [01:28<31:41,  2.35it/s]\u001b[0m2025-06-16 13:55:54 - src.tasks.clm_training.fabric.base - \u001b[0;36mDEBUG\u001b[0m - \u001b[0;36mgradient_accumulation_steps raw value: 2, type: <class 'int'>\u001b[0m\n",
      "[\u001b[0;36mDEBUG\u001b[0m | src.tasks.clm_training.fabric.base]: \u001b[0;36mgradient_accumulation_steps raw value: 2, type: <class 'int'>\u001b[0m\n",
      "2025-06-16 13:55:54 - src.tasks.clm_training.fabric.base - \u001b[0;36mDEBUG\u001b[0m - \u001b[0;36mgradient_accumulation_steps converted: 2, type: <class 'int'>\u001b[0m\n",
      "[\u001b[0;36mDEBUG\u001b[0m | src.tasks.clm_training.fabric.base]: \u001b[0;36mgradient_accumulation_steps converted: 2, type: <class 'int'>\u001b[0m\n",
      "2025-06-16 13:55:54 - src.tasks.clm_training.fabric.base - \u001b[0;36mDEBUG\u001b[0m - \u001b[0;36miter_num value: 208, type: <class 'int'>\u001b[0m\n",
      "[\u001b[0;36mDEBUG\u001b[0m | src.tasks.clm_training.fabric.base]: \u001b[0;36miter_num value: 208, type: <class 'int'>\u001b[0m\n",
      "2025-06-16 13:55:54 - src.tasks.clm_training.fabric.base - \u001b[0;36mDEBUG\u001b[0m - \u001b[0;36mAbout to check: (iter_num + 1) % gradient_accumulation_steps != 0 -> (208 + 1) % 2 != 0\u001b[0m\n",
      "[\u001b[0;36mDEBUG\u001b[0m | src.tasks.clm_training.fabric.base]: \u001b[0;36mAbout to check: (iter_num + 1) % gradient_accumulation_steps != 0 -> (208 + 1) % 2 != 0\u001b[0m\n",
      "2025-06-16 13:55:54 - src.tasks.clm_training.fabric.base - \u001b[0;36mDEBUG\u001b[0m - \u001b[0;36miter 209 step 104: loss 6.0000, iter time: 123.00ms remaining time: \u001b[0m\n",
      "[\u001b[0;36mDEBUG\u001b[0m | src.tasks.clm_training.fabric.base]: \u001b[0;36miter 209 step 104: loss 6.0000, iter time: 123.00ms remaining time: \u001b[0m\n",
      "  4%|\u001b[34m█▊                                      \u001b[0m| 209/4682 [01:28<31:30,  2.37it/s]\u001b[0m2025-06-16 13:55:54 - src.tasks.clm_training.fabric.base - \u001b[0;36mDEBUG\u001b[0m - \u001b[0;36mgradient_accumulation_steps raw value: 2, type: <class 'int'>\u001b[0m\n",
      "[\u001b[0;36mDEBUG\u001b[0m | src.tasks.clm_training.fabric.base]: \u001b[0;36mgradient_accumulation_steps raw value: 2, type: <class 'int'>\u001b[0m\n",
      "2025-06-16 13:55:54 - src.tasks.clm_training.fabric.base - \u001b[0;36mDEBUG\u001b[0m - \u001b[0;36mgradient_accumulation_steps converted: 2, type: <class 'int'>\u001b[0m\n",
      "[\u001b[0;36mDEBUG\u001b[0m | src.tasks.clm_training.fabric.base]: \u001b[0;36mgradient_accumulation_steps converted: 2, type: <class 'int'>\u001b[0m\n",
      "2025-06-16 13:55:54 - src.tasks.clm_training.fabric.base - \u001b[0;36mDEBUG\u001b[0m - \u001b[0;36miter_num value: 209, type: <class 'int'>\u001b[0m\n",
      "[\u001b[0;36mDEBUG\u001b[0m | src.tasks.clm_training.fabric.base]: \u001b[0;36miter_num value: 209, type: <class 'int'>\u001b[0m\n",
      "2025-06-16 13:55:54 - src.tasks.clm_training.fabric.base - \u001b[0;36mDEBUG\u001b[0m - \u001b[0;36mAbout to check: (iter_num + 1) % gradient_accumulation_steps != 0 -> (209 + 1) % 2 != 0\u001b[0m\n",
      "[\u001b[0;36mDEBUG\u001b[0m | src.tasks.clm_training.fabric.base]: \u001b[0;36mAbout to check: (iter_num + 1) % gradient_accumulation_steps != 0 -> (209 + 1) % 2 != 0\u001b[0m\n",
      "2025-06-16 13:55:55 - src.tasks.clm_training.fabric.base - \u001b[0;36mDEBUG\u001b[0m - \u001b[0;36mGradient norm before clipping: 26.5000\u001b[0m\n",
      "[\u001b[0;36mDEBUG\u001b[0m | src.tasks.clm_training.fabric.base]: \u001b[0;36mGradient norm before clipping: 26.5000\u001b[0m\n",
      "2025-06-16 13:55:55 - src.tasks.clm_training.fabric.base - \u001b[0;36mDEBUG\u001b[0m - \u001b[0;36mvalidate_after_k_steps value: 1000, type: <class 'int'>\u001b[0m\n",
      "[\u001b[0;36mDEBUG\u001b[0m | src.tasks.clm_training.fabric.base]: \u001b[0;36mvalidate_after_k_steps value: 1000, type: <class 'int'>\u001b[0m\n",
      "2025-06-16 13:55:55 - src.tasks.clm_training.fabric.base - \u001b[0;36mDEBUG\u001b[0m - \u001b[0;36mstep_count value: 105, type: <class 'int'>\u001b[0m\n",
      "[\u001b[0;36mDEBUG\u001b[0m | src.tasks.clm_training.fabric.base]: \u001b[0;36mstep_count value: 105, type: <class 'int'>\u001b[0m\n",
      "2025-06-16 13:55:55 - src.tasks.clm_training.fabric.base - \u001b[0;36mDEBUG\u001b[0m - \u001b[0;36mValidation condition check - epochFinished: False, trainingFinished: False\u001b[0m\n",
      "[\u001b[0;36mDEBUG\u001b[0m | src.tasks.clm_training.fabric.base]: \u001b[0;36mValidation condition check - epochFinished: False, trainingFinished: False\u001b[0m\n",
      "2025-06-16 13:55:55 - src.tasks.clm_training.fabric.base - \u001b[0;36mDEBUG\u001b[0m - \u001b[0;36mAbout to check: step_count % validate_after_k_steps == 0 -> 105 % 1000 == 0\u001b[0m\n",
      "[\u001b[0;36mDEBUG\u001b[0m | src.tasks.clm_training.fabric.base]: \u001b[0;36mAbout to check: step_count % validate_after_k_steps == 0 -> 105 % 1000 == 0\u001b[0m\n",
      "2025-06-16 13:55:55 - src.tasks.clm_training.fabric.base - \u001b[0;36mDEBUG\u001b[0m - \u001b[0;36miter 210 step 105: loss 5.7812, iter time: 421.02ms remaining time: \u001b[0m\n",
      "[\u001b[0;36mDEBUG\u001b[0m | src.tasks.clm_training.fabric.base]: \u001b[0;36miter 210 step 105: loss 5.7812, iter time: 421.02ms remaining time: \u001b[0m\n",
      "  4%|\u001b[34m█▊                                      \u001b[0m| 210/4682 [01:29<31:41,  2.35it/s]\u001b[0m2025-06-16 13:55:55 - src.tasks.clm_training.fabric.base - \u001b[0;36mDEBUG\u001b[0m - \u001b[0;36mgradient_accumulation_steps raw value: 2, type: <class 'int'>\u001b[0m\n",
      "[\u001b[0;36mDEBUG\u001b[0m | src.tasks.clm_training.fabric.base]: \u001b[0;36mgradient_accumulation_steps raw value: 2, type: <class 'int'>\u001b[0m\n",
      "2025-06-16 13:55:55 - src.tasks.clm_training.fabric.base - \u001b[0;36mDEBUG\u001b[0m - \u001b[0;36mgradient_accumulation_steps converted: 2, type: <class 'int'>\u001b[0m\n",
      "[\u001b[0;36mDEBUG\u001b[0m | src.tasks.clm_training.fabric.base]: \u001b[0;36mgradient_accumulation_steps converted: 2, type: <class 'int'>\u001b[0m\n",
      "2025-06-16 13:55:55 - src.tasks.clm_training.fabric.base - \u001b[0;36mDEBUG\u001b[0m - \u001b[0;36miter_num value: 210, type: <class 'int'>\u001b[0m\n",
      "[\u001b[0;36mDEBUG\u001b[0m | src.tasks.clm_training.fabric.base]: \u001b[0;36miter_num value: 210, type: <class 'int'>\u001b[0m\n",
      "2025-06-16 13:55:55 - src.tasks.clm_training.fabric.base - \u001b[0;36mDEBUG\u001b[0m - \u001b[0;36mAbout to check: (iter_num + 1) % gradient_accumulation_steps != 0 -> (210 + 1) % 2 != 0\u001b[0m\n",
      "[\u001b[0;36mDEBUG\u001b[0m | src.tasks.clm_training.fabric.base]: \u001b[0;36mAbout to check: (iter_num + 1) % gradient_accumulation_steps != 0 -> (210 + 1) % 2 != 0\u001b[0m\n",
      "2025-06-16 13:55:55 - src.tasks.clm_training.fabric.base - \u001b[0;36mDEBUG\u001b[0m - \u001b[0;36miter 211 step 105: loss 5.9375, iter time: 122.57ms remaining time: \u001b[0m\n",
      "[\u001b[0;36mDEBUG\u001b[0m | src.tasks.clm_training.fabric.base]: \u001b[0;36miter 211 step 105: loss 5.9375, iter time: 122.57ms remaining time: \u001b[0m\n",
      "  5%|\u001b[34m█▊                                      \u001b[0m| 211/4682 [01:29<31:29,  2.37it/s]\u001b[0m2025-06-16 13:55:55 - src.tasks.clm_training.fabric.base - \u001b[0;36mDEBUG\u001b[0m - \u001b[0;36mgradient_accumulation_steps raw value: 2, type: <class 'int'>\u001b[0m\n",
      "[\u001b[0;36mDEBUG\u001b[0m | src.tasks.clm_training.fabric.base]: \u001b[0;36mgradient_accumulation_steps raw value: 2, type: <class 'int'>\u001b[0m\n",
      "2025-06-16 13:55:55 - src.tasks.clm_training.fabric.base - \u001b[0;36mDEBUG\u001b[0m - \u001b[0;36mgradient_accumulation_steps converted: 2, type: <class 'int'>\u001b[0m\n",
      "[\u001b[0;36mDEBUG\u001b[0m | src.tasks.clm_training.fabric.base]: \u001b[0;36mgradient_accumulation_steps converted: 2, type: <class 'int'>\u001b[0m\n",
      "2025-06-16 13:55:55 - src.tasks.clm_training.fabric.base - \u001b[0;36mDEBUG\u001b[0m - \u001b[0;36miter_num value: 211, type: <class 'int'>\u001b[0m\n",
      "[\u001b[0;36mDEBUG\u001b[0m | src.tasks.clm_training.fabric.base]: \u001b[0;36miter_num value: 211, type: <class 'int'>\u001b[0m\n",
      "2025-06-16 13:55:55 - src.tasks.clm_training.fabric.base - \u001b[0;36mDEBUG\u001b[0m - \u001b[0;36mAbout to check: (iter_num + 1) % gradient_accumulation_steps != 0 -> (211 + 1) % 2 != 0\u001b[0m\n",
      "[\u001b[0;36mDEBUG\u001b[0m | src.tasks.clm_training.fabric.base]: \u001b[0;36mAbout to check: (iter_num + 1) % gradient_accumulation_steps != 0 -> (211 + 1) % 2 != 0\u001b[0m\n",
      "2025-06-16 13:55:56 - src.tasks.clm_training.fabric.base - \u001b[0;36mDEBUG\u001b[0m - \u001b[0;36mGradient norm before clipping: 25.5000\u001b[0m\n",
      "[\u001b[0;36mDEBUG\u001b[0m | src.tasks.clm_training.fabric.base]: \u001b[0;36mGradient norm before clipping: 25.5000\u001b[0m\n",
      "2025-06-16 13:55:56 - src.tasks.clm_training.fabric.base - \u001b[0;36mDEBUG\u001b[0m - \u001b[0;36mvalidate_after_k_steps value: 1000, type: <class 'int'>\u001b[0m\n",
      "[\u001b[0;36mDEBUG\u001b[0m | src.tasks.clm_training.fabric.base]: \u001b[0;36mvalidate_after_k_steps value: 1000, type: <class 'int'>\u001b[0m\n",
      "2025-06-16 13:55:56 - src.tasks.clm_training.fabric.base - \u001b[0;36mDEBUG\u001b[0m - \u001b[0;36mstep_count value: 106, type: <class 'int'>\u001b[0m\n",
      "[\u001b[0;36mDEBUG\u001b[0m | src.tasks.clm_training.fabric.base]: \u001b[0;36mstep_count value: 106, type: <class 'int'>\u001b[0m\n",
      "2025-06-16 13:55:56 - src.tasks.clm_training.fabric.base - \u001b[0;36mDEBUG\u001b[0m - \u001b[0;36mValidation condition check - epochFinished: False, trainingFinished: False\u001b[0m\n",
      "[\u001b[0;36mDEBUG\u001b[0m | src.tasks.clm_training.fabric.base]: \u001b[0;36mValidation condition check - epochFinished: False, trainingFinished: False\u001b[0m\n",
      "2025-06-16 13:55:56 - src.tasks.clm_training.fabric.base - \u001b[0;36mDEBUG\u001b[0m - \u001b[0;36mAbout to check: step_count % validate_after_k_steps == 0 -> 106 % 1000 == 0\u001b[0m\n",
      "[\u001b[0;36mDEBUG\u001b[0m | src.tasks.clm_training.fabric.base]: \u001b[0;36mAbout to check: step_count % validate_after_k_steps == 0 -> 106 % 1000 == 0\u001b[0m\n",
      "2025-06-16 13:55:56 - src.tasks.clm_training.fabric.base - \u001b[0;36mDEBUG\u001b[0m - \u001b[0;36miter 212 step 106: loss 5.1250, iter time: 421.07ms remaining time: \u001b[0m\n",
      "[\u001b[0;36mDEBUG\u001b[0m | src.tasks.clm_training.fabric.base]: \u001b[0;36miter 212 step 106: loss 5.1250, iter time: 421.07ms remaining time: \u001b[0m\n",
      "  5%|\u001b[34m█▊                                      \u001b[0m| 212/4682 [01:29<31:40,  2.35it/s]\u001b[0m2025-06-16 13:55:56 - src.tasks.clm_training.fabric.base - \u001b[0;36mDEBUG\u001b[0m - \u001b[0;36mgradient_accumulation_steps raw value: 2, type: <class 'int'>\u001b[0m\n",
      "[\u001b[0;36mDEBUG\u001b[0m | src.tasks.clm_training.fabric.base]: \u001b[0;36mgradient_accumulation_steps raw value: 2, type: <class 'int'>\u001b[0m\n",
      "2025-06-16 13:55:56 - src.tasks.clm_training.fabric.base - \u001b[0;36mDEBUG\u001b[0m - \u001b[0;36mgradient_accumulation_steps converted: 2, type: <class 'int'>\u001b[0m\n",
      "[\u001b[0;36mDEBUG\u001b[0m | src.tasks.clm_training.fabric.base]: \u001b[0;36mgradient_accumulation_steps converted: 2, type: <class 'int'>\u001b[0m\n",
      "2025-06-16 13:55:56 - src.tasks.clm_training.fabric.base - \u001b[0;36mDEBUG\u001b[0m - \u001b[0;36miter_num value: 212, type: <class 'int'>\u001b[0m\n",
      "[\u001b[0;36mDEBUG\u001b[0m | src.tasks.clm_training.fabric.base]: \u001b[0;36miter_num value: 212, type: <class 'int'>\u001b[0m\n",
      "2025-06-16 13:55:56 - src.tasks.clm_training.fabric.base - \u001b[0;36mDEBUG\u001b[0m - \u001b[0;36mAbout to check: (iter_num + 1) % gradient_accumulation_steps != 0 -> (212 + 1) % 2 != 0\u001b[0m\n",
      "[\u001b[0;36mDEBUG\u001b[0m | src.tasks.clm_training.fabric.base]: \u001b[0;36mAbout to check: (iter_num + 1) % gradient_accumulation_steps != 0 -> (212 + 1) % 2 != 0\u001b[0m\n",
      "2025-06-16 13:55:56 - src.tasks.clm_training.fabric.base - \u001b[0;36mDEBUG\u001b[0m - \u001b[0;36miter 213 step 106: loss 5.7188, iter time: 122.56ms remaining time: \u001b[0m\n",
      "[\u001b[0;36mDEBUG\u001b[0m | src.tasks.clm_training.fabric.base]: \u001b[0;36miter 213 step 106: loss 5.7188, iter time: 122.56ms remaining time: \u001b[0m\n",
      "  5%|\u001b[34m█▊                                      \u001b[0m| 213/4682 [01:30<31:28,  2.37it/s]\u001b[0m2025-06-16 13:55:56 - src.tasks.clm_training.fabric.base - \u001b[0;36mDEBUG\u001b[0m - \u001b[0;36mgradient_accumulation_steps raw value: 2, type: <class 'int'>\u001b[0m\n",
      "[\u001b[0;36mDEBUG\u001b[0m | src.tasks.clm_training.fabric.base]: \u001b[0;36mgradient_accumulation_steps raw value: 2, type: <class 'int'>\u001b[0m\n",
      "2025-06-16 13:55:56 - src.tasks.clm_training.fabric.base - \u001b[0;36mDEBUG\u001b[0m - \u001b[0;36mgradient_accumulation_steps converted: 2, type: <class 'int'>\u001b[0m\n",
      "[\u001b[0;36mDEBUG\u001b[0m | src.tasks.clm_training.fabric.base]: \u001b[0;36mgradient_accumulation_steps converted: 2, type: <class 'int'>\u001b[0m\n",
      "2025-06-16 13:55:56 - src.tasks.clm_training.fabric.base - \u001b[0;36mDEBUG\u001b[0m - \u001b[0;36miter_num value: 213, type: <class 'int'>\u001b[0m\n",
      "[\u001b[0;36mDEBUG\u001b[0m | src.tasks.clm_training.fabric.base]: \u001b[0;36miter_num value: 213, type: <class 'int'>\u001b[0m\n",
      "2025-06-16 13:55:56 - src.tasks.clm_training.fabric.base - \u001b[0;36mDEBUG\u001b[0m - \u001b[0;36mAbout to check: (iter_num + 1) % gradient_accumulation_steps != 0 -> (213 + 1) % 2 != 0\u001b[0m\n",
      "[\u001b[0;36mDEBUG\u001b[0m | src.tasks.clm_training.fabric.base]: \u001b[0;36mAbout to check: (iter_num + 1) % gradient_accumulation_steps != 0 -> (213 + 1) % 2 != 0\u001b[0m\n",
      "2025-06-16 13:55:56 - src.tasks.clm_training.fabric.base - \u001b[0;36mDEBUG\u001b[0m - \u001b[0;36mGradient norm before clipping: 24.7500\u001b[0m\n",
      "[\u001b[0;36mDEBUG\u001b[0m | src.tasks.clm_training.fabric.base]: \u001b[0;36mGradient norm before clipping: 24.7500\u001b[0m\n",
      "2025-06-16 13:55:56 - src.tasks.clm_training.fabric.base - \u001b[0;36mDEBUG\u001b[0m - \u001b[0;36mvalidate_after_k_steps value: 1000, type: <class 'int'>\u001b[0m\n",
      "[\u001b[0;36mDEBUG\u001b[0m | src.tasks.clm_training.fabric.base]: \u001b[0;36mvalidate_after_k_steps value: 1000, type: <class 'int'>\u001b[0m\n",
      "2025-06-16 13:55:56 - src.tasks.clm_training.fabric.base - \u001b[0;36mDEBUG\u001b[0m - \u001b[0;36mstep_count value: 107, type: <class 'int'>\u001b[0m\n",
      "[\u001b[0;36mDEBUG\u001b[0m | src.tasks.clm_training.fabric.base]: \u001b[0;36mstep_count value: 107, type: <class 'int'>\u001b[0m\n",
      "2025-06-16 13:55:56 - src.tasks.clm_training.fabric.base - \u001b[0;36mDEBUG\u001b[0m - \u001b[0;36mValidation condition check - epochFinished: False, trainingFinished: False\u001b[0m\n",
      "[\u001b[0;36mDEBUG\u001b[0m | src.tasks.clm_training.fabric.base]: \u001b[0;36mValidation condition check - epochFinished: False, trainingFinished: False\u001b[0m\n",
      "2025-06-16 13:55:56 - src.tasks.clm_training.fabric.base - \u001b[0;36mDEBUG\u001b[0m - \u001b[0;36mAbout to check: step_count % validate_after_k_steps == 0 -> 107 % 1000 == 0\u001b[0m\n",
      "[\u001b[0;36mDEBUG\u001b[0m | src.tasks.clm_training.fabric.base]: \u001b[0;36mAbout to check: step_count % validate_after_k_steps == 0 -> 107 % 1000 == 0\u001b[0m\n",
      "2025-06-16 13:55:56 - src.tasks.clm_training.fabric.base - \u001b[0;36mDEBUG\u001b[0m - \u001b[0;36miter 214 step 107: loss 5.6562, iter time: 421.84ms remaining time: \u001b[0m\n",
      "[\u001b[0;36mDEBUG\u001b[0m | src.tasks.clm_training.fabric.base]: \u001b[0;36miter 214 step 107: loss 5.6562, iter time: 421.84ms remaining time: \u001b[0m\n",
      "  5%|\u001b[34m█▊                                      \u001b[0m| 214/4682 [01:30<31:40,  2.35it/s]\u001b[0m2025-06-16 13:55:56 - src.tasks.clm_training.fabric.base - \u001b[0;36mDEBUG\u001b[0m - \u001b[0;36mgradient_accumulation_steps raw value: 2, type: <class 'int'>\u001b[0m\n",
      "[\u001b[0;36mDEBUG\u001b[0m | src.tasks.clm_training.fabric.base]: \u001b[0;36mgradient_accumulation_steps raw value: 2, type: <class 'int'>\u001b[0m\n",
      "2025-06-16 13:55:56 - src.tasks.clm_training.fabric.base - \u001b[0;36mDEBUG\u001b[0m - \u001b[0;36mgradient_accumulation_steps converted: 2, type: <class 'int'>\u001b[0m\n",
      "[\u001b[0;36mDEBUG\u001b[0m | src.tasks.clm_training.fabric.base]: \u001b[0;36mgradient_accumulation_steps converted: 2, type: <class 'int'>\u001b[0m\n",
      "2025-06-16 13:55:56 - src.tasks.clm_training.fabric.base - \u001b[0;36mDEBUG\u001b[0m - \u001b[0;36miter_num value: 214, type: <class 'int'>\u001b[0m\n",
      "[\u001b[0;36mDEBUG\u001b[0m | src.tasks.clm_training.fabric.base]: \u001b[0;36miter_num value: 214, type: <class 'int'>\u001b[0m\n",
      "2025-06-16 13:55:56 - src.tasks.clm_training.fabric.base - \u001b[0;36mDEBUG\u001b[0m - \u001b[0;36mAbout to check: (iter_num + 1) % gradient_accumulation_steps != 0 -> (214 + 1) % 2 != 0\u001b[0m\n",
      "[\u001b[0;36mDEBUG\u001b[0m | src.tasks.clm_training.fabric.base]: \u001b[0;36mAbout to check: (iter_num + 1) % gradient_accumulation_steps != 0 -> (214 + 1) % 2 != 0\u001b[0m\n",
      "2025-06-16 13:55:57 - src.tasks.clm_training.fabric.base - \u001b[0;36mDEBUG\u001b[0m - \u001b[0;36miter 215 step 107: loss 5.5312, iter time: 122.69ms remaining time: \u001b[0m\n",
      "[\u001b[0;36mDEBUG\u001b[0m | src.tasks.clm_training.fabric.base]: \u001b[0;36miter 215 step 107: loss 5.5312, iter time: 122.69ms remaining time: \u001b[0m\n",
      "  5%|\u001b[34m█▊                                      \u001b[0m| 215/4682 [01:31<31:29,  2.36it/s]\u001b[0m2025-06-16 13:55:57 - src.tasks.clm_training.fabric.base - \u001b[0;36mDEBUG\u001b[0m - \u001b[0;36mgradient_accumulation_steps raw value: 2, type: <class 'int'>\u001b[0m\n",
      "[\u001b[0;36mDEBUG\u001b[0m | src.tasks.clm_training.fabric.base]: \u001b[0;36mgradient_accumulation_steps raw value: 2, type: <class 'int'>\u001b[0m\n",
      "2025-06-16 13:55:57 - src.tasks.clm_training.fabric.base - \u001b[0;36mDEBUG\u001b[0m - \u001b[0;36mgradient_accumulation_steps converted: 2, type: <class 'int'>\u001b[0m\n",
      "[\u001b[0;36mDEBUG\u001b[0m | src.tasks.clm_training.fabric.base]: \u001b[0;36mgradient_accumulation_steps converted: 2, type: <class 'int'>\u001b[0m\n",
      "2025-06-16 13:55:57 - src.tasks.clm_training.fabric.base - \u001b[0;36mDEBUG\u001b[0m - \u001b[0;36miter_num value: 215, type: <class 'int'>\u001b[0m\n",
      "[\u001b[0;36mDEBUG\u001b[0m | src.tasks.clm_training.fabric.base]: \u001b[0;36miter_num value: 215, type: <class 'int'>\u001b[0m\n",
      "2025-06-16 13:55:57 - src.tasks.clm_training.fabric.base - \u001b[0;36mDEBUG\u001b[0m - \u001b[0;36mAbout to check: (iter_num + 1) % gradient_accumulation_steps != 0 -> (215 + 1) % 2 != 0\u001b[0m\n",
      "[\u001b[0;36mDEBUG\u001b[0m | src.tasks.clm_training.fabric.base]: \u001b[0;36mAbout to check: (iter_num + 1) % gradient_accumulation_steps != 0 -> (215 + 1) % 2 != 0\u001b[0m\n",
      "2025-06-16 13:55:57 - src.tasks.clm_training.fabric.base - \u001b[0;36mDEBUG\u001b[0m - \u001b[0;36mGradient norm before clipping: 21.5000\u001b[0m\n",
      "[\u001b[0;36mDEBUG\u001b[0m | src.tasks.clm_training.fabric.base]: \u001b[0;36mGradient norm before clipping: 21.5000\u001b[0m\n",
      "2025-06-16 13:55:57 - src.tasks.clm_training.fabric.base - \u001b[0;36mDEBUG\u001b[0m - \u001b[0;36mvalidate_after_k_steps value: 1000, type: <class 'int'>\u001b[0m\n",
      "[\u001b[0;36mDEBUG\u001b[0m | src.tasks.clm_training.fabric.base]: \u001b[0;36mvalidate_after_k_steps value: 1000, type: <class 'int'>\u001b[0m\n",
      "2025-06-16 13:55:57 - src.tasks.clm_training.fabric.base - \u001b[0;36mDEBUG\u001b[0m - \u001b[0;36mstep_count value: 108, type: <class 'int'>\u001b[0m\n",
      "[\u001b[0;36mDEBUG\u001b[0m | src.tasks.clm_training.fabric.base]: \u001b[0;36mstep_count value: 108, type: <class 'int'>\u001b[0m\n",
      "2025-06-16 13:55:57 - src.tasks.clm_training.fabric.base - \u001b[0;36mDEBUG\u001b[0m - \u001b[0;36mValidation condition check - epochFinished: False, trainingFinished: False\u001b[0m\n",
      "[\u001b[0;36mDEBUG\u001b[0m | src.tasks.clm_training.fabric.base]: \u001b[0;36mValidation condition check - epochFinished: False, trainingFinished: False\u001b[0m\n",
      "2025-06-16 13:55:57 - src.tasks.clm_training.fabric.base - \u001b[0;36mDEBUG\u001b[0m - \u001b[0;36mAbout to check: step_count % validate_after_k_steps == 0 -> 108 % 1000 == 0\u001b[0m\n",
      "[\u001b[0;36mDEBUG\u001b[0m | src.tasks.clm_training.fabric.base]: \u001b[0;36mAbout to check: step_count % validate_after_k_steps == 0 -> 108 % 1000 == 0\u001b[0m\n",
      "2025-06-16 13:55:57 - src.tasks.clm_training.fabric.base - \u001b[0;36mDEBUG\u001b[0m - \u001b[0;36miter 216 step 108: loss 5.6250, iter time: 421.40ms remaining time: \u001b[0m\n",
      "[\u001b[0;36mDEBUG\u001b[0m | src.tasks.clm_training.fabric.base]: \u001b[0;36miter 216 step 108: loss 5.6250, iter time: 421.40ms remaining time: \u001b[0m\n",
      "  5%|\u001b[34m█▊                                      \u001b[0m| 216/4682 [01:31<31:40,  2.35it/s]\u001b[0m2025-06-16 13:55:57 - src.tasks.clm_training.fabric.base - \u001b[0;36mDEBUG\u001b[0m - \u001b[0;36mgradient_accumulation_steps raw value: 2, type: <class 'int'>\u001b[0m\n",
      "[\u001b[0;36mDEBUG\u001b[0m | src.tasks.clm_training.fabric.base]: \u001b[0;36mgradient_accumulation_steps raw value: 2, type: <class 'int'>\u001b[0m\n",
      "2025-06-16 13:55:57 - src.tasks.clm_training.fabric.base - \u001b[0;36mDEBUG\u001b[0m - \u001b[0;36mgradient_accumulation_steps converted: 2, type: <class 'int'>\u001b[0m\n",
      "[\u001b[0;36mDEBUG\u001b[0m | src.tasks.clm_training.fabric.base]: \u001b[0;36mgradient_accumulation_steps converted: 2, type: <class 'int'>\u001b[0m\n",
      "2025-06-16 13:55:57 - src.tasks.clm_training.fabric.base - \u001b[0;36mDEBUG\u001b[0m - \u001b[0;36miter_num value: 216, type: <class 'int'>\u001b[0m\n",
      "[\u001b[0;36mDEBUG\u001b[0m | src.tasks.clm_training.fabric.base]: \u001b[0;36miter_num value: 216, type: <class 'int'>\u001b[0m\n",
      "2025-06-16 13:55:57 - src.tasks.clm_training.fabric.base - \u001b[0;36mDEBUG\u001b[0m - \u001b[0;36mAbout to check: (iter_num + 1) % gradient_accumulation_steps != 0 -> (216 + 1) % 2 != 0\u001b[0m\n",
      "[\u001b[0;36mDEBUG\u001b[0m | src.tasks.clm_training.fabric.base]: \u001b[0;36mAbout to check: (iter_num + 1) % gradient_accumulation_steps != 0 -> (216 + 1) % 2 != 0\u001b[0m\n",
      "2025-06-16 13:55:58 - src.tasks.clm_training.fabric.base - \u001b[0;36mDEBUG\u001b[0m - \u001b[0;36miter 217 step 108: loss 5.6875, iter time: 122.67ms remaining time: \u001b[0m\n",
      "[\u001b[0;36mDEBUG\u001b[0m | src.tasks.clm_training.fabric.base]: \u001b[0;36miter 217 step 108: loss 5.6875, iter time: 122.67ms remaining time: \u001b[0m\n",
      "  5%|\u001b[34m█▊                                      \u001b[0m| 217/4682 [01:32<31:27,  2.37it/s]\u001b[0m2025-06-16 13:55:58 - src.tasks.clm_training.fabric.base - \u001b[0;36mDEBUG\u001b[0m - \u001b[0;36mgradient_accumulation_steps raw value: 2, type: <class 'int'>\u001b[0m\n",
      "[\u001b[0;36mDEBUG\u001b[0m | src.tasks.clm_training.fabric.base]: \u001b[0;36mgradient_accumulation_steps raw value: 2, type: <class 'int'>\u001b[0m\n",
      "2025-06-16 13:55:58 - src.tasks.clm_training.fabric.base - \u001b[0;36mDEBUG\u001b[0m - \u001b[0;36mgradient_accumulation_steps converted: 2, type: <class 'int'>\u001b[0m\n",
      "[\u001b[0;36mDEBUG\u001b[0m | src.tasks.clm_training.fabric.base]: \u001b[0;36mgradient_accumulation_steps converted: 2, type: <class 'int'>\u001b[0m\n",
      "2025-06-16 13:55:58 - src.tasks.clm_training.fabric.base - \u001b[0;36mDEBUG\u001b[0m - \u001b[0;36miter_num value: 217, type: <class 'int'>\u001b[0m\n",
      "[\u001b[0;36mDEBUG\u001b[0m | src.tasks.clm_training.fabric.base]: \u001b[0;36miter_num value: 217, type: <class 'int'>\u001b[0m\n",
      "2025-06-16 13:55:58 - src.tasks.clm_training.fabric.base - \u001b[0;36mDEBUG\u001b[0m - \u001b[0;36mAbout to check: (iter_num + 1) % gradient_accumulation_steps != 0 -> (217 + 1) % 2 != 0\u001b[0m\n",
      "[\u001b[0;36mDEBUG\u001b[0m | src.tasks.clm_training.fabric.base]: \u001b[0;36mAbout to check: (iter_num + 1) % gradient_accumulation_steps != 0 -> (217 + 1) % 2 != 0\u001b[0m\n",
      "2025-06-16 13:55:58 - src.tasks.clm_training.fabric.base - \u001b[0;36mDEBUG\u001b[0m - \u001b[0;36mGradient norm before clipping: 24.5000\u001b[0m\n",
      "[\u001b[0;36mDEBUG\u001b[0m | src.tasks.clm_training.fabric.base]: \u001b[0;36mGradient norm before clipping: 24.5000\u001b[0m\n",
      "2025-06-16 13:55:58 - src.tasks.clm_training.fabric.base - \u001b[0;36mDEBUG\u001b[0m - \u001b[0;36mvalidate_after_k_steps value: 1000, type: <class 'int'>\u001b[0m\n",
      "[\u001b[0;36mDEBUG\u001b[0m | src.tasks.clm_training.fabric.base]: \u001b[0;36mvalidate_after_k_steps value: 1000, type: <class 'int'>\u001b[0m\n",
      "2025-06-16 13:55:58 - src.tasks.clm_training.fabric.base - \u001b[0;36mDEBUG\u001b[0m - \u001b[0;36mstep_count value: 109, type: <class 'int'>\u001b[0m\n",
      "[\u001b[0;36mDEBUG\u001b[0m | src.tasks.clm_training.fabric.base]: \u001b[0;36mstep_count value: 109, type: <class 'int'>\u001b[0m\n",
      "2025-06-16 13:55:58 - src.tasks.clm_training.fabric.base - \u001b[0;36mDEBUG\u001b[0m - \u001b[0;36mValidation condition check - epochFinished: False, trainingFinished: False\u001b[0m\n",
      "[\u001b[0;36mDEBUG\u001b[0m | src.tasks.clm_training.fabric.base]: \u001b[0;36mValidation condition check - epochFinished: False, trainingFinished: False\u001b[0m\n",
      "2025-06-16 13:55:58 - src.tasks.clm_training.fabric.base - \u001b[0;36mDEBUG\u001b[0m - \u001b[0;36mAbout to check: step_count % validate_after_k_steps == 0 -> 109 % 1000 == 0\u001b[0m\n",
      "[\u001b[0;36mDEBUG\u001b[0m | src.tasks.clm_training.fabric.base]: \u001b[0;36mAbout to check: step_count % validate_after_k_steps == 0 -> 109 % 1000 == 0\u001b[0m\n",
      "2025-06-16 13:55:58 - src.tasks.clm_training.fabric.base - \u001b[0;36mDEBUG\u001b[0m - \u001b[0;36miter 218 step 109: loss 5.4688, iter time: 421.51ms remaining time: \u001b[0m\n",
      "[\u001b[0;36mDEBUG\u001b[0m | src.tasks.clm_training.fabric.base]: \u001b[0;36miter 218 step 109: loss 5.4688, iter time: 421.51ms remaining time: \u001b[0m\n",
      "  5%|\u001b[34m█▊                                      \u001b[0m| 218/4682 [01:32<31:38,  2.35it/s]\u001b[0m2025-06-16 13:55:58 - src.tasks.clm_training.fabric.base - \u001b[0;36mDEBUG\u001b[0m - \u001b[0;36mgradient_accumulation_steps raw value: 2, type: <class 'int'>\u001b[0m\n",
      "[\u001b[0;36mDEBUG\u001b[0m | src.tasks.clm_training.fabric.base]: \u001b[0;36mgradient_accumulation_steps raw value: 2, type: <class 'int'>\u001b[0m\n",
      "2025-06-16 13:55:58 - src.tasks.clm_training.fabric.base - \u001b[0;36mDEBUG\u001b[0m - \u001b[0;36mgradient_accumulation_steps converted: 2, type: <class 'int'>\u001b[0m\n",
      "[\u001b[0;36mDEBUG\u001b[0m | src.tasks.clm_training.fabric.base]: \u001b[0;36mgradient_accumulation_steps converted: 2, type: <class 'int'>\u001b[0m\n",
      "2025-06-16 13:55:58 - src.tasks.clm_training.fabric.base - \u001b[0;36mDEBUG\u001b[0m - \u001b[0;36miter_num value: 218, type: <class 'int'>\u001b[0m\n",
      "[\u001b[0;36mDEBUG\u001b[0m | src.tasks.clm_training.fabric.base]: \u001b[0;36miter_num value: 218, type: <class 'int'>\u001b[0m\n",
      "2025-06-16 13:55:58 - src.tasks.clm_training.fabric.base - \u001b[0;36mDEBUG\u001b[0m - \u001b[0;36mAbout to check: (iter_num + 1) % gradient_accumulation_steps != 0 -> (218 + 1) % 2 != 0\u001b[0m\n",
      "[\u001b[0;36mDEBUG\u001b[0m | src.tasks.clm_training.fabric.base]: \u001b[0;36mAbout to check: (iter_num + 1) % gradient_accumulation_steps != 0 -> (218 + 1) % 2 != 0\u001b[0m\n",
      "2025-06-16 13:55:58 - src.tasks.clm_training.fabric.base - \u001b[0;36mDEBUG\u001b[0m - \u001b[0;36miter 219 step 109: loss 5.8125, iter time: 122.82ms remaining time: \u001b[0m\n",
      "[\u001b[0;36mDEBUG\u001b[0m | src.tasks.clm_training.fabric.base]: \u001b[0;36miter 219 step 109: loss 5.8125, iter time: 122.82ms remaining time: \u001b[0m\n",
      "  5%|\u001b[34m█▊                                      \u001b[0m| 219/4682 [01:32<31:26,  2.37it/s]\u001b[0m2025-06-16 13:55:58 - src.tasks.clm_training.fabric.base - \u001b[0;36mDEBUG\u001b[0m - \u001b[0;36mgradient_accumulation_steps raw value: 2, type: <class 'int'>\u001b[0m\n",
      "[\u001b[0;36mDEBUG\u001b[0m | src.tasks.clm_training.fabric.base]: \u001b[0;36mgradient_accumulation_steps raw value: 2, type: <class 'int'>\u001b[0m\n",
      "2025-06-16 13:55:58 - src.tasks.clm_training.fabric.base - \u001b[0;36mDEBUG\u001b[0m - \u001b[0;36mgradient_accumulation_steps converted: 2, type: <class 'int'>\u001b[0m\n",
      "[\u001b[0;36mDEBUG\u001b[0m | src.tasks.clm_training.fabric.base]: \u001b[0;36mgradient_accumulation_steps converted: 2, type: <class 'int'>\u001b[0m\n",
      "2025-06-16 13:55:58 - src.tasks.clm_training.fabric.base - \u001b[0;36mDEBUG\u001b[0m - \u001b[0;36miter_num value: 219, type: <class 'int'>\u001b[0m\n",
      "[\u001b[0;36mDEBUG\u001b[0m | src.tasks.clm_training.fabric.base]: \u001b[0;36miter_num value: 219, type: <class 'int'>\u001b[0m\n",
      "2025-06-16 13:55:58 - src.tasks.clm_training.fabric.base - \u001b[0;36mDEBUG\u001b[0m - \u001b[0;36mAbout to check: (iter_num + 1) % gradient_accumulation_steps != 0 -> (219 + 1) % 2 != 0\u001b[0m\n",
      "[\u001b[0;36mDEBUG\u001b[0m | src.tasks.clm_training.fabric.base]: \u001b[0;36mAbout to check: (iter_num + 1) % gradient_accumulation_steps != 0 -> (219 + 1) % 2 != 0\u001b[0m\n",
      "2025-06-16 13:55:59 - src.tasks.clm_training.fabric.base - \u001b[0;36mDEBUG\u001b[0m - \u001b[0;36mGradient norm before clipping: 23.1250\u001b[0m\n",
      "[\u001b[0;36mDEBUG\u001b[0m | src.tasks.clm_training.fabric.base]: \u001b[0;36mGradient norm before clipping: 23.1250\u001b[0m\n",
      "2025-06-16 13:55:59 - src.tasks.clm_training.fabric.base - \u001b[0;36mDEBUG\u001b[0m - \u001b[0;36mvalidate_after_k_steps value: 1000, type: <class 'int'>\u001b[0m\n",
      "[\u001b[0;36mDEBUG\u001b[0m | src.tasks.clm_training.fabric.base]: \u001b[0;36mvalidate_after_k_steps value: 1000, type: <class 'int'>\u001b[0m\n",
      "2025-06-16 13:55:59 - src.tasks.clm_training.fabric.base - \u001b[0;36mDEBUG\u001b[0m - \u001b[0;36mstep_count value: 110, type: <class 'int'>\u001b[0m\n",
      "[\u001b[0;36mDEBUG\u001b[0m | src.tasks.clm_training.fabric.base]: \u001b[0;36mstep_count value: 110, type: <class 'int'>\u001b[0m\n",
      "2025-06-16 13:55:59 - src.tasks.clm_training.fabric.base - \u001b[0;36mDEBUG\u001b[0m - \u001b[0;36mValidation condition check - epochFinished: False, trainingFinished: False\u001b[0m\n",
      "[\u001b[0;36mDEBUG\u001b[0m | src.tasks.clm_training.fabric.base]: \u001b[0;36mValidation condition check - epochFinished: False, trainingFinished: False\u001b[0m\n",
      "2025-06-16 13:55:59 - src.tasks.clm_training.fabric.base - \u001b[0;36mDEBUG\u001b[0m - \u001b[0;36mAbout to check: step_count % validate_after_k_steps == 0 -> 110 % 1000 == 0\u001b[0m\n",
      "[\u001b[0;36mDEBUG\u001b[0m | src.tasks.clm_training.fabric.base]: \u001b[0;36mAbout to check: step_count % validate_after_k_steps == 0 -> 110 % 1000 == 0\u001b[0m\n",
      "2025-06-16 13:55:59 - src.tasks.clm_training.fabric.base - \u001b[0;36mDEBUG\u001b[0m - \u001b[0;36miter 220 step 110: loss 5.7188, iter time: 420.85ms remaining time: \u001b[0m\n",
      "[\u001b[0;36mDEBUG\u001b[0m | src.tasks.clm_training.fabric.base]: \u001b[0;36miter 220 step 110: loss 5.7188, iter time: 420.85ms remaining time: \u001b[0m\n",
      "  5%|\u001b[34m█▉                                      \u001b[0m| 220/4682 [01:33<31:37,  2.35it/s]\u001b[0m2025-06-16 13:55:59 - src.tasks.clm_training.fabric.base - \u001b[0;36mDEBUG\u001b[0m - \u001b[0;36mgradient_accumulation_steps raw value: 2, type: <class 'int'>\u001b[0m\n",
      "[\u001b[0;36mDEBUG\u001b[0m | src.tasks.clm_training.fabric.base]: \u001b[0;36mgradient_accumulation_steps raw value: 2, type: <class 'int'>\u001b[0m\n",
      "2025-06-16 13:55:59 - src.tasks.clm_training.fabric.base - \u001b[0;36mDEBUG\u001b[0m - \u001b[0;36mgradient_accumulation_steps converted: 2, type: <class 'int'>\u001b[0m\n",
      "[\u001b[0;36mDEBUG\u001b[0m | src.tasks.clm_training.fabric.base]: \u001b[0;36mgradient_accumulation_steps converted: 2, type: <class 'int'>\u001b[0m\n",
      "2025-06-16 13:55:59 - src.tasks.clm_training.fabric.base - \u001b[0;36mDEBUG\u001b[0m - \u001b[0;36miter_num value: 220, type: <class 'int'>\u001b[0m\n",
      "[\u001b[0;36mDEBUG\u001b[0m | src.tasks.clm_training.fabric.base]: \u001b[0;36miter_num value: 220, type: <class 'int'>\u001b[0m\n",
      "2025-06-16 13:55:59 - src.tasks.clm_training.fabric.base - \u001b[0;36mDEBUG\u001b[0m - \u001b[0;36mAbout to check: (iter_num + 1) % gradient_accumulation_steps != 0 -> (220 + 1) % 2 != 0\u001b[0m\n",
      "[\u001b[0;36mDEBUG\u001b[0m | src.tasks.clm_training.fabric.base]: \u001b[0;36mAbout to check: (iter_num + 1) % gradient_accumulation_steps != 0 -> (220 + 1) % 2 != 0\u001b[0m\n",
      "2025-06-16 13:55:59 - src.tasks.clm_training.fabric.base - \u001b[0;36mDEBUG\u001b[0m - \u001b[0;36miter 221 step 110: loss 5.5625, iter time: 122.78ms remaining time: \u001b[0m\n",
      "[\u001b[0;36mDEBUG\u001b[0m | src.tasks.clm_training.fabric.base]: \u001b[0;36miter 221 step 110: loss 5.5625, iter time: 122.78ms remaining time: \u001b[0m\n",
      "  5%|\u001b[34m█▉                                      \u001b[0m| 221/4682 [01:33<31:26,  2.37it/s]\u001b[0m2025-06-16 13:55:59 - src.tasks.clm_training.fabric.base - \u001b[0;36mDEBUG\u001b[0m - \u001b[0;36mgradient_accumulation_steps raw value: 2, type: <class 'int'>\u001b[0m\n",
      "[\u001b[0;36mDEBUG\u001b[0m | src.tasks.clm_training.fabric.base]: \u001b[0;36mgradient_accumulation_steps raw value: 2, type: <class 'int'>\u001b[0m\n",
      "2025-06-16 13:55:59 - src.tasks.clm_training.fabric.base - \u001b[0;36mDEBUG\u001b[0m - \u001b[0;36mgradient_accumulation_steps converted: 2, type: <class 'int'>\u001b[0m\n",
      "[\u001b[0;36mDEBUG\u001b[0m | src.tasks.clm_training.fabric.base]: \u001b[0;36mgradient_accumulation_steps converted: 2, type: <class 'int'>\u001b[0m\n",
      "2025-06-16 13:55:59 - src.tasks.clm_training.fabric.base - \u001b[0;36mDEBUG\u001b[0m - \u001b[0;36miter_num value: 221, type: <class 'int'>\u001b[0m\n",
      "[\u001b[0;36mDEBUG\u001b[0m | src.tasks.clm_training.fabric.base]: \u001b[0;36miter_num value: 221, type: <class 'int'>\u001b[0m\n",
      "2025-06-16 13:55:59 - src.tasks.clm_training.fabric.base - \u001b[0;36mDEBUG\u001b[0m - \u001b[0;36mAbout to check: (iter_num + 1) % gradient_accumulation_steps != 0 -> (221 + 1) % 2 != 0\u001b[0m\n",
      "[\u001b[0;36mDEBUG\u001b[0m | src.tasks.clm_training.fabric.base]: \u001b[0;36mAbout to check: (iter_num + 1) % gradient_accumulation_steps != 0 -> (221 + 1) % 2 != 0\u001b[0m\n",
      "2025-06-16 13:56:00 - src.tasks.clm_training.fabric.base - \u001b[0;36mDEBUG\u001b[0m - \u001b[0;36mGradient norm before clipping: 25.1250\u001b[0m\n",
      "[\u001b[0;36mDEBUG\u001b[0m | src.tasks.clm_training.fabric.base]: \u001b[0;36mGradient norm before clipping: 25.1250\u001b[0m\n",
      "2025-06-16 13:56:00 - src.tasks.clm_training.fabric.base - \u001b[0;36mDEBUG\u001b[0m - \u001b[0;36mvalidate_after_k_steps value: 1000, type: <class 'int'>\u001b[0m\n",
      "[\u001b[0;36mDEBUG\u001b[0m | src.tasks.clm_training.fabric.base]: \u001b[0;36mvalidate_after_k_steps value: 1000, type: <class 'int'>\u001b[0m\n",
      "2025-06-16 13:56:00 - src.tasks.clm_training.fabric.base - \u001b[0;36mDEBUG\u001b[0m - \u001b[0;36mstep_count value: 111, type: <class 'int'>\u001b[0m\n",
      "[\u001b[0;36mDEBUG\u001b[0m | src.tasks.clm_training.fabric.base]: \u001b[0;36mstep_count value: 111, type: <class 'int'>\u001b[0m\n",
      "2025-06-16 13:56:00 - src.tasks.clm_training.fabric.base - \u001b[0;36mDEBUG\u001b[0m - \u001b[0;36mValidation condition check - epochFinished: False, trainingFinished: False\u001b[0m\n",
      "[\u001b[0;36mDEBUG\u001b[0m | src.tasks.clm_training.fabric.base]: \u001b[0;36mValidation condition check - epochFinished: False, trainingFinished: False\u001b[0m\n",
      "2025-06-16 13:56:00 - src.tasks.clm_training.fabric.base - \u001b[0;36mDEBUG\u001b[0m - \u001b[0;36mAbout to check: step_count % validate_after_k_steps == 0 -> 111 % 1000 == 0\u001b[0m\n",
      "[\u001b[0;36mDEBUG\u001b[0m | src.tasks.clm_training.fabric.base]: \u001b[0;36mAbout to check: step_count % validate_after_k_steps == 0 -> 111 % 1000 == 0\u001b[0m\n",
      "2025-06-16 13:56:00 - src.tasks.clm_training.fabric.base - \u001b[0;36mDEBUG\u001b[0m - \u001b[0;36miter 222 step 111: loss 5.9688, iter time: 421.88ms remaining time: \u001b[0m\n",
      "[\u001b[0;36mDEBUG\u001b[0m | src.tasks.clm_training.fabric.base]: \u001b[0;36miter 222 step 111: loss 5.9688, iter time: 421.88ms remaining time: \u001b[0m\n",
      "  5%|\u001b[34m█▉                                      \u001b[0m| 222/4682 [01:34<31:37,  2.35it/s]\u001b[0m2025-06-16 13:56:00 - src.tasks.clm_training.fabric.base - \u001b[0;36mDEBUG\u001b[0m - \u001b[0;36mgradient_accumulation_steps raw value: 2, type: <class 'int'>\u001b[0m\n",
      "[\u001b[0;36mDEBUG\u001b[0m | src.tasks.clm_training.fabric.base]: \u001b[0;36mgradient_accumulation_steps raw value: 2, type: <class 'int'>\u001b[0m\n",
      "2025-06-16 13:56:00 - src.tasks.clm_training.fabric.base - \u001b[0;36mDEBUG\u001b[0m - \u001b[0;36mgradient_accumulation_steps converted: 2, type: <class 'int'>\u001b[0m\n",
      "[\u001b[0;36mDEBUG\u001b[0m | src.tasks.clm_training.fabric.base]: \u001b[0;36mgradient_accumulation_steps converted: 2, type: <class 'int'>\u001b[0m\n",
      "2025-06-16 13:56:00 - src.tasks.clm_training.fabric.base - \u001b[0;36mDEBUG\u001b[0m - \u001b[0;36miter_num value: 222, type: <class 'int'>\u001b[0m\n",
      "[\u001b[0;36mDEBUG\u001b[0m | src.tasks.clm_training.fabric.base]: \u001b[0;36miter_num value: 222, type: <class 'int'>\u001b[0m\n",
      "2025-06-16 13:56:00 - src.tasks.clm_training.fabric.base - \u001b[0;36mDEBUG\u001b[0m - \u001b[0;36mAbout to check: (iter_num + 1) % gradient_accumulation_steps != 0 -> (222 + 1) % 2 != 0\u001b[0m\n",
      "[\u001b[0;36mDEBUG\u001b[0m | src.tasks.clm_training.fabric.base]: \u001b[0;36mAbout to check: (iter_num + 1) % gradient_accumulation_steps != 0 -> (222 + 1) % 2 != 0\u001b[0m\n",
      "^C\n",
      "  5%|\u001b[34m█▉                                      \u001b[0m| 222/4682 [01:34<31:41,  2.35it/s]\u001b[0m\n",
      "Traceback (most recent call last):\n",
      "  File \"/workspace/src/main.py\", line 72, in <module>\n",
      "    execute_task(args.config)\n",
      "  File \"/workspace/src/main.py\", line 31, in execute_task\n",
      "    task_module.execute(config)\n",
      "  File \"/workspace/src/tasks/clm_training/__init__.py\", line 7, in execute\n",
      "    return orchestrator.execute()\n",
      "           ^^^^^^^^^^^^^^^^^^^^^^\n",
      "  File \"/workspace/src/tasks/clm_training/orchestrator.py\", line 235, in execute\n",
      "    self.fsdp(dataset)\n",
      "  File \"/workspace/src/tasks/clm_training/orchestrator.py\", line 93, in fsdp\n",
      "    trainer.setup()\n",
      "  File \"/workspace/src/tasks/clm_training/fabric/base.py\", line 114, in setup\n",
      "    fabric.launch(self._pipeline)\n",
      "  File \"/usr/local/lib/python3.12/dist-packages/lightning/fabric/fabric.py\", line 837, in launch\n",
      "    return self._wrap_and_launch(function, self, *args, **kwargs)\n",
      "           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n",
      "  File \"/usr/local/lib/python3.12/dist-packages/lightning/fabric/fabric.py\", line 923, in _wrap_and_launch\n",
      "    return to_run(*args, **kwargs)\n",
      "           ^^^^^^^^^^^^^^^^^^^^^^^\n",
      "  File \"/usr/local/lib/python3.12/dist-packages/lightning/fabric/fabric.py\", line 928, in _wrap_with_setup\n",
      "    return to_run(*args, **kwargs)\n",
      "           ^^^^^^^^^^^^^^^^^^^^^^^\n",
      "  File \"/workspace/src/tasks/clm_training/fabric/base.py\", line 601, in _pipeline\n",
      "    self._train(fabric)\n",
      "  File \"/workspace/src/tasks/clm_training/fabric/base.py\", line 418, in _train\n",
      "    self._train_logs(fabric, loss)\n",
      "  File \"/workspace/src/tasks/clm_training/fabric/base.py\", line 223, in _train_logs\n",
      "    f\"iter {self.state['iter_num']} step {self.state['step_count']}: loss {loss.item():.4f}, iter time:\"\n",
      "                                                                           ^^^^^^^^^^^\n",
      "KeyboardInterrupt\n"
     ]
    }
   ],
   "source": [
    "import os\n",
    "os.chdir(\"/workspace\")\n",
    "\n",
    "\n",
    "# 1. Run tokenization\n",
    "!python src/main.py --config tutorials/configs/tokenization_tutorial.yaml\n",
    "\n",
    "# 2. Run CLM training\n",
    "print(\"Running CLM training...\")\n",
    "!python src/main.py --config tutorials/configs/clm_training_tutorial.yaml"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# ============================================================================="
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Conclusion\n",
    "\n",
    "In this tutorial, we've covered the basics of CLM training using the Continual Pretraining Framework. We've learned how to:\n",
    "\n",
    "1. Load a tokenized dataset\n",
    "2. Configure CLM training parameters\n",
    "3. Select an appropriate distributed training strategy\n",
    "4. Train a model using the ContinualOrchestrator\n",
    "5. Monitor training progress and optimize training\n",
    "\n",
    "The Continual Pretraining Framework provides a flexible and efficient way to train causal language models, with support for various distributed training strategies and optimization techniques.\n",
    "\n",
    "For more advanced usage, refer to the framework documentation and experiment with different configurations to find what works best for your specific use case."
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
