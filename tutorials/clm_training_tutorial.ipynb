{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Causal Language Model (CLM) Training Tutorial\n",
    "\n",
    "This tutorial demonstrates how to train a causal language model using the Continual Pretraining Framework. We'll cover the following topics:\n",
    "\n",
    "1. Understanding CLM training concepts\n",
    "2. Setting up the training configuration\n",
    "3. Loading a tokenized dataset\n",
    "4. Selecting a training strategy\n",
    "5. Training a model using the ContinualOrchestrator\n",
    "6. Monitoring training progress\n",
    "7. Evaluating the trained model\n",
    "8. Best practices and optimization tips\n",
    "\n",
    "This tutorial assumes you have already completed the tokenization tutorial and have a tokenized dataset available."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 1. Understanding CLM Training Concepts\n",
    "\n",
    "Causal Language Model (CLM) training is a fundamental technique for training large language models. In CLM training, the model learns to predict the next token in a sequence given all previous tokens. This is also known as autoregressive language modeling.\n",
    "\n",
    "Key concepts in CLM training include:\n",
    "\n",
    "- **Autoregressive Prediction**: The model predicts one token at a time, with each prediction conditioned on all previous tokens.\n",
    "- **Causal Attention Mask**: Ensures that the model can only attend to previous tokens in the sequence, not future ones.\n",
    "- **Next Token Prediction Loss**: The training objective is to minimize the negative log-likelihood of predicting the correct next token.\n",
    "- **Distributed Training**: Large models often require training across multiple GPUs or nodes using strategies like FSDP, DDP, or DeepSpeed.\n",
    "- **Gradient Accumulation**: Accumulating gradients across multiple batches to simulate larger batch sizes.\n",
    "- **Learning Rate Scheduling**: Adjusting the learning rate during training to improve convergence.\n",
    "\n",
    "The Continual Pretraining Framework provides a comprehensive implementation for CLM training with various distributed training strategies, making it easy to train large language models efficiently."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 2. Setting Up the Environment\n",
    "\n",
    "First, let's import the necessary modules and set up our environment:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "import torch\n",
    "import yaml\n",
    "from box import Box\n",
    "from datasets import Dataset, DatasetDict\n",
    "from pathlib import Path\n",
    "\n",
    "# Import the CLM training components\n",
    "from src.tasks.clm_training import execute\n",
    "from src.tasks.clm_training.orchestrator import ContinualOrchestrator\n",
    "from src.config.config_loader import ConfigValidator\n",
    "\n",
    "# Check if CUDA is available\n",
    "print(f\"CUDA available: {torch.cuda.is_available()}\")\n",
    "if torch.cuda.is_available():\n",
    "    print(f\"CUDA device count: {torch.cuda.device_count()}\")\n",
    "    print(f\"CUDA device name: {torch.cuda.get_device_name(0)}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 3. Creating a Sample Dataset\n",
    "\n",
    "For this tutorial, we'll use a small sample dataset. In a real-world scenario, you would typically use the tokenized dataset from the tokenization step. Let's first create a small sample dataset for demonstration purposes:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Create a sample dataset directory\n",
    "sample_dataset_dir = Path(\"sample_tokenized_dataset\")\n",
    "sample_dataset_dir.mkdir(exist_ok=True)\n",
    "\n",
    "# Create a small sample tokenized dataset\n",
    "def create_sample_tokenized_dataset():\n",
    "    # Sample tokenized data (input_ids, attention_mask, and labels)\n",
    "    # This simulates what would come from the tokenization step\n",
    "    train_data = {\n",
    "        \"input_ids\": [\n",
    "            [101, 2023, 2003, 1037, 4937, 2361, 1012, 102] + [0] * 8,  # \"This is a sample text.\" padded\n",
    "            [101, 2023, 2003, 1037, 2828, 2361, 1012, 102] + [0] * 8   # \"This is a short text.\" padded\n",
    "        ],\n",
    "        \"attention_mask\": [\n",
    "            [1, 1, 1, 1, 1, 1, 1, 1] + [0] * 8,  # Mask for the first sequence\n",
    "            [1, 1, 1, 1, 1, 1, 1, 1] + [0] * 8   # Mask for the second sequence\n",
    "        ],\n",
    "        \"labels\": [\n",
    "            [101, 2023, 2003, 1037, 4937, 2361, 1012, 102] + [0] * 8,  # Same as input_ids for CLM\n",
    "            [101, 2023, 2003, 1037, 2828, 2361, 1012, 102] + [0] * 8   # Same as input_ids for CLM\n",
    "        ]\n",
    "    }\n",
    "    \n",
    "    valid_data = {\n",
    "        \"input_ids\": [\n",
    "            [101, 2023, 2003, 1037, 3231, 2361, 1012, 102] + [0] * 8,  # \"This is a test text.\" padded\n",
    "        ],\n",
    "        \"attention_mask\": [\n",
    "            [1, 1, 1, 1, 1, 1, 1, 1] + [0] * 8,  # Mask for the sequence\n",
    "        ],\n",
    "        \"labels\": [\n",
    "            [101, 2023, 2003, 1037, 3231, 2361, 1012, 102] + [0] * 8,  # Same as input_ids for CLM\n",
    "        ]\n",
    "    }\n",
    "    \n",
    "    # Create datasets\n",
    "    train_dataset = Dataset.from_dict(train_data)\n",
    "    valid_dataset = Dataset.from_dict(valid_data)\n",
    "    \n",
    "    # Create a DatasetDict\n",
    "    dataset_dict = DatasetDict({\n",
    "        \"train\": train_dataset,\n",
    "        \"valid\": valid_dataset\n",
    "    })\n",
    "    \n",
    "    # Save the dataset\n",
    "    dataset_dict.save_to_disk(sample_dataset_dir)\n",
    "    \n",
    "    return dataset_dict\n",
    "\n",
    "# Create and display the sample dataset\n",
    "sample_dataset = create_sample_tokenized_dataset()\n",
    "print(\"Sample dataset created:\")\n",
    "print(f\"Train split: {len(sample_dataset['train'])} examples\")\n",
    "print(f\"Validation split: {len(sample_dataset['valid'])} examples\")\n",
    "print(\"\\nSample from train split:\")\n",
    "print(sample_dataset['train'][0])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 4. Loading a Tokenized Dataset\n",
    "\n",
    "In a real-world scenario, you would load the tokenized dataset created in the tokenization step. Let's see how to load a tokenized dataset from disk:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from datasets import load_from_disk\n",
    "\n",
    "# Load the sample dataset we just created\n",
    "loaded_dataset = load_from_disk(str(sample_dataset_dir))\n",
    "print(f\"Loaded dataset splits: {list(loaded_dataset.keys())}\")\n",
    "print(f\"Number of examples in train split: {len(loaded_dataset['train'])}\")\n",
    "print(f\"Number of examples in validation split: {len(loaded_dataset['valid'])}\")\n",
    "\n",
    "# Display the first example from the train split\n",
    "print(\"\\nFirst example from train split:\")\n",
    "print(loaded_dataset['train'][0])\n",
    "\n",
    "# Verify that the dataset has the required columns for CLM training\n",
    "required_columns = [\"input_ids\", \"attention_mask\", \"labels\"]\n",
    "all_columns_present = all(col in loaded_dataset['train'].column_names for col in required_columns)\n",
    "print(f\"\\nAll required columns present: {all_columns_present}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 5. Setting Up the Training Configuration\n",
    "\n",
    "For CLM training, we need to create a configuration that specifies the model, training parameters, and distributed training strategy. The Continual Pretraining Framework uses YAML configuration files, but we can also create a configuration programmatically:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Create a configuration for CLM training\n",
    "def create_clm_training_config(dataset_path, output_dir=\"output\", model_name=\"meta-llama/Llama-3.2-1B\"):\n",
    "    \"\"\"\n",
    "    Create a configuration for CLM training.\n",
    "    \n",
    "    Args:\n",
    "        dataset_path: Path to the tokenized dataset\n",
    "        output_dir: Directory to save model checkpoints and logs\n",
    "        model_name: Name or path of the pretrained model to use\n",
    "        \n",
    "    Returns:\n",
    "        Box: Configuration object\n",
    "    \"\"\"\n",
    "    config = {\n",
    "        \"task\": \"pretraining\",\n",
    "        \"experiment_name\": \"tutorial_clm_training\",\n",
    "        \"verbose_level\": 4,\n",
    "        \n",
    "        # Dataset configuration\n",
    "        \"dataset\": {\n",
    "            \"source\": \"local\",\n",
    "            \"nameOrPath\": dataset_path\n",
    "        },\n",
    "        \n",
    "        # Output directory\n",
    "        \"output_dir\": output_dir,\n",
    "        \n",
    "        # Model configuration\n",
    "        \"model_name\": model_name,\n",
    "        \"precision\": \"bf16-true\" if torch.cuda.is_available() else \"32-true\",\n",
    "        \n",
    "        # Training parameters\n",
    "        \"number_epochs\": 1,\n",
    "        \"batch_size\": 2,\n",
    "        \n",
    "        # Validation parameters\n",
    "        \"validate_on_end\": True,\n",
    "        \"validate_after_epoch\": True,\n",
    "        \"validate_after_k_steps\": None,\n",
    "        \n",
    "        # Gradient parameters\n",
    "        \"gradient_accumulation\": True,\n",
    "        \"gradient_accumulation_steps\": 2,\n",
    "        \"grad_clip\": 1.0,\n",
    "        \n",
    "        # Optimizer parameters\n",
    "        \"lr\": 2e-5,\n",
    "        \"lr_decay\": True,\n",
    "        \"weight_decay\": 0.01,\n",
    "        \"beta1\": 0.9,\n",
    "        \"beta2\": 0.95,\n",
    "        \n",
    "        # Scheduler parameters\n",
    "        \"lr_scheduler\": \"warmup_linear\",\n",
    "        \"warmup_proportion\": 0.06,\n",
    "        \n",
    "        # Distributed training strategy\n",
    "        \"parallelization_strategy\": \"none\",  # For single GPU, use \"none\"\n",
    "        \"num_workers\": 1,\n",
    "        \"gradient_checkpointing\": False,\n",
    "    }\n",
    "    \n",
    "    # Convert to Box object for dot notation access\n",
    "    return Box(config)\n",
    "\n",
    "# Create the configuration\n",
    "clm_config = create_clm_training_config(\n",
    "    dataset_path=str(sample_dataset_dir),\n",
    "    output_dir=\"clm_training_output\",\n",
    "    model_name=\"gpt2\"  # Using a small model for demonstration\n",
    ")\n",
    "\n",
    "# Print the configuration\n",
    "print(\"CLM Training Configuration:\")\n",
    "for key, value in clm_config.items():\n",
    "    if isinstance(value, dict):\n",
    "        print(f\"{key}:\")\n",
    "        for k, v in value.items():\n",
    "            print(f\"  {k}: {v}\")\n",
    "    else:\n",
    "        print(f\"{key}: {value}\")\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 6. Understanding Distributed Training Strategies\n",
    "\n",
    "The Continual Pretraining Framework supports various distributed training strategies:\n",
    "\n",
    "1. **FSDP (Fully Sharded Data Parallel)**: Shards model parameters, gradients, and optimizer states across GPUs, enabling training of very large models.\n",
    "2. **DDP (Distributed Data Parallel)**: Replicates the model on each GPU and synchronizes gradients, suitable for medium-sized models.\n",
    "3. **DeepSpeed**: Implements ZeRO optimization for efficient large model training with memory optimizations.\n",
    "4. **DP (Data Parallel)**: Simple data parallelism for single-node multi-GPU setups.\n",
    "5. **None**: Single GPU or CPU training.\n",
    "\n",
    "Let's see how to configure each strategy:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Example configurations for different distributed training strategies\n",
    "\n",
    "# 1. FSDP Configuration\n",
    "def create_fsdp_config():\n",
    "    config = clm_config.copy()\n",
    "    config.parallelization_strategy = \"fsdp\"\n",
    "    config.auto_wrap_policy = \"gpt2\"  # or \"llama\" for LLaMA models\n",
    "    config.sharding_strategy = \"FULL_SHARD\"\n",
    "    config.state_dict_type = \"sharded\"\n",
    "    config.limit_all_gathers = True\n",
    "    config.cpu_offload = False\n",
    "    config.gradient_checkpointing = True\n",
    "    return config\n",
    "\n",
    "# 2. DDP Configuration\n",
    "def create_ddp_config():\n",
    "    config = clm_config.copy()\n",
    "    config.parallelization_strategy = \"ddp\"\n",
    "    config.find_unused_parameters = False\n",
    "    config.process_group_backend = \"nccl\"\n",
    "    config.static_graph = True\n",
    "    return config\n",
    "\n",
    "# 3. DeepSpeed Configuration\n",
    "def create_deepspeed_config():\n",
    "    config = clm_config.copy()\n",
    "    config.parallelization_strategy = \"deep_speed\"\n",
    "    config.zero_stage = 2\n",
    "    config.offload_optimizer = False\n",
    "    config.offload_parameters = False\n",
    "    return config\n",
    "\n",
    "# 4. DP Configuration\n",
    "def create_dp_config():\n",
    "    config = clm_config.copy()\n",
    "    config.parallelization_strategy = \"dp\"\n",
    "    return config\n",
    "\n",
    "# Print the FSDP configuration as an example\n",
    "if torch.cuda.device_count() > 1:\n",
    "    print(\"Multi-GPU setup detected. FSDP configuration example:\")\n",
    "    fsdp_config = create_fsdp_config()\n",
    "    for key, value in fsdp_config.items():\n",
    "        if key not in clm_config or fsdp_config[key] != clm_config[key]:\n",
    "            print(f\"{key}: {value}\")\n",
    "else:\n",
    "    print(\"Single GPU or CPU setup detected. Using 'none' strategy for training.\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 7. Training a Model with ContinualOrchestrator\n",
    "\n",
    "Now that we have our dataset and configuration, let's train a model using the ContinualOrchestrator. For this tutorial, we'll use a small model and our sample dataset:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Create output directory\n",
    "os.makedirs(\"clm_training_output\", exist_ok=True)\n",
    "\n",
    "# Initialize the orchestrator\n",
    "orchestrator = ContinualOrchestrator(clm_config)\n",
    "\n",
    "# Load the dataset\n",
    "dataset = load_from_disk(str(sample_dataset_dir))\n",
    "\n",
    "# Train the model (this is a simplified version for demonstration)\n",
    "# In a real scenario, you would use the execute function\n",
    "try:\n",
    "    # This is a simplified training loop for demonstration\n",
    "    # It won't actually run the full training due to the small dataset and setup\n",
    "    print(\"Setting up training environment...\")\n",
    "    orchestrator.setup_environment()\n",
    "    \n",
    "    print(\"Loading dataset...\")\n",
    "    orchestrator.load_dataset(dataset)\n",
    "    \n",
    "    print(\"Setting up training...\")\n",
    "    orchestrator.setup()\n",
    "    \n",
    "    print(\"\\\\nTraining would start here in a real scenario.\")\n",
    "    print(\"For a complete training run, use the execute function with your configuration.\")\n",
    "except Exception as e:\n",
    "    print(f\"Error during training setup: {str(e)}\")\n",
    "    print(\"\\\\nNote: This is expected in a notebook environment without proper GPU setup.\")\n",
    "    print(\"In a real training scenario, you would use the execute function with your configuration file.\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 8. Running Training with the Execute Function\n",
    "\n",
    "In a real-world scenario, you would typically use the `execute` function to run the entire training pipeline. Here's how you would do it:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Save the configuration to a YAML file\n",
    "config_path = \"clm_training_config.yaml\"\n",
    "with open(config_path, \"w\") as f:\n",
    "    yaml.dump(clm_config.to_dict(), f)\n",
    "\n",
    "print(f\"Configuration saved to {config_path}\")\n",
    "\n",
    "# In a real scenario, you would run:\n",
    "# execute(config_path)\n",
    "print(\"To run training with this configuration, use:\")\n",
    "print(f\"from src.tasks.clm_training import execute\")\n",
    "print(f\"execute('{config_path}')\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 9. Monitoring Training Progress\n",
    "\n",
    "The Continual Pretraining Framework provides several ways to monitor training progress:\n",
    "\n",
    "1. **Console Logging**: Training metrics are logged to the console.\n",
    "2. **Weights & Biases (WandB)**: Optional integration for experiment tracking.\n",
    "3. **Checkpoints**: Regular model checkpoints are saved to the output directory.\n",
    "\n",
    "Let's see how to configure logging and monitor training:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Example of configuring WandB logging\n",
    "def configure_wandb_logging(config):\n",
    "    \"\"\"Configure Weights & Biases logging\"\"\"\n",
    "    config_with_wandb = config.copy()\n",
    "    config_with_wandb.logging_config = \"wandb\"\n",
    "    config_with_wandb.wandb_project = \"clm_training_tutorial\"\n",
    "    config_with_wandb.wandb_entity = \"your_wandb_entity\"  # Replace with your WandB entity\n",
    "    config_with_wandb.log_model = False\n",
    "    config_with_wandb.log_iter_interval = 10\n",
    "    return config_with_wandb\n",
    "\n",
    "# Example of how to load and inspect a checkpoint\n",
    "def load_checkpoint(checkpoint_path):\n",
    "    \"\"\"Load a checkpoint and print its contents\"\"\"\n",
    "    if os.path.exists(checkpoint_path):\n",
    "        checkpoint = torch.load(checkpoint_path, map_location=\"cpu\")\n",
    "        print(f\"Checkpoint keys: {list(checkpoint.keys())}\")\n",
    "        if \"step_count\" in checkpoint:\n",
    "            print(f\"Training step: {checkpoint['step_count']}\")\n",
    "        if \"current_epoch\" in checkpoint:\n",
    "            print(f\"Current epoch: {checkpoint['current_epoch']}\")\n",
    "        return checkpoint\n",
    "    else:\n",
    "        print(f\"Checkpoint not found at {checkpoint_path}\")\n",
    "        return None\n",
    "\n",
    "# Example checkpoint path\n",
    "checkpoint_path = \"clm_training_output/checkpoint.pt\"\n",
    "print(f\"In a real training run, checkpoints would be saved to: {checkpoint_path}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 10. Best Practices and Optimization Tips\n",
    "\n",
    "Here are some best practices and optimization tips for CLM training:\n",
    "\n",
    "### Model Selection and Hardware Requirements\n",
    "\n",
    "- **Model Size**: Choose a model size appropriate for your hardware. Larger models require more memory and compute.\n",
    "- **GPU Memory**: As a rule of thumb, you need at least 8GB of GPU memory for small models (125M parameters), 16GB for medium models (1B parameters), and 40GB+ for large models (7B+ parameters).\n",
    "- **Distributed Training**: For models larger than 1B parameters, consider using distributed training strategies like FSDP or DeepSpeed.\n",
    "\n",
    "### Training Optimization\n",
    "\n",
    "- **Gradient Accumulation**: Use gradient accumulation to simulate larger batch sizes on limited hardware.\n",
    "- **Gradient Checkpointing**: Enable gradient checkpointing to reduce memory usage at the cost of increased computation time.\n",
    "- **Mixed Precision Training**: Use mixed precision training (bf16 or fp16) to reduce memory usage and speed up training.\n",
    "- **Learning Rate Scheduling**: Use a learning rate scheduler with warmup to improve training stability.\n",
    "\n",
    "### Dataset Preparation\n",
    "\n",
    "- **Dataset Size**: Larger datasets generally lead to better models, but also require more training time.\n",
    "- **Dataset Quality**: High-quality, diverse data is crucial for good model performance.\n",
    "- **Validation Split**: Always include a validation split to monitor training progress and prevent overfitting.\n",
    "\n",
    "### Monitoring and Debugging\n",
    "\n",
    "- **Regular Validation**: Validate the model regularly to catch issues early.\n",
    "- **Gradient Norms**: Monitor gradient norms to detect exploding or vanishing gradients.\n",
    "- **Learning Rate**: Start with a small learning rate and gradually increase it if training is stable.\n",
    "- **Memory Usage**: Monitor GPU memory usage to detect memory leaks or inefficient memory usage."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 11. Integration with Tokenization\n",
    "\n",
    "The CLM training task is designed to work seamlessly with the tokenized dataset produced by the tokenization task. Here's how to integrate the two tasks:\n",
    "\n",
    "1. **Run the Tokenization Task**: First, run the tokenization task to prepare your dataset.\n",
    "2. **Configure CLM Training**: Set up your CLM training configuration to use the tokenized dataset.\n",
    "3. **Run CLM Training**: Execute the CLM training task using the tokenized dataset.\n",
    "\n",
    "Example workflow:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Example workflow integrating tokenization and CLM training\n",
    "\n",
    "# 1. Tokenization configuration\n",
    "tokenization_config = {\n",
    "    \"task\": \"tokenization\",\n",
    "    \"tokenizer_name\": \"gpt2\",\n",
    "    \"context_length\": 1024,\n",
    "    \"overlap\": 0,\n",
    "    \"batch_size\": 1000,\n",
    "    \"num_proc\": 4,\n",
    "    \"dataset\": {\n",
    "        \"source\": \"local\",\n",
    "        \"nameOrPath\": \"path/to/raw/dataset\"\n",
    "    },\n",
    "    \"output_dir\": \"tokenized_dataset\"\n",
    "}\n",
    "\n",
    "# 2. CLM training configuration using the tokenized dataset\n",
    "clm_training_config = {\n",
    "    \"task\": \"pretraining\",\n",
    "    \"model_name\": \"gpt2\",\n",
    "    \"dataset\": {\n",
    "        \"source\": \"local\",\n",
    "        \"nameOrPath\": \"tokenized_dataset\"  # Output from tokenization task\n",
    "    },\n",
    "    \"output_dir\": \"trained_model\",\n",
    "    # Other training parameters...\n",
    "}\n",
    "\n",
    "# In a real scenario, you would run:\n",
    "# 1. Execute tokenization\n",
    "# from src.tasks.tokenization import execute as tokenize\n",
    "# tokenize(\"path/to/tokenization_config.yaml\")\n",
    "\n",
    "# 2. Execute CLM training\n",
    "# from src.tasks.clm_training import execute as train\n",
    "# train(\"path/to/clm_training_config.yaml\")\n",
    "\n",
    "print(\"This example shows how to integrate tokenization and CLM training tasks.\")\n",
    "print(\"In a real scenario, you would save these configurations to YAML files and run the execute functions.\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 12. Conclusion\n",
    "\n",
    "In this tutorial, we've covered the basics of CLM training using the Continual Pretraining Framework. We've learned how to:\n",
    "\n",
    "1. Load a tokenized dataset\n",
    "2. Configure CLM training parameters\n",
    "3. Select an appropriate distributed training strategy\n",
    "4. Train a model using the ContinualOrchestrator\n",
    "5. Monitor training progress and optimize training\n",
    "\n",
    "The Continual Pretraining Framework provides a flexible and efficient way to train causal language models, with support for various distributed training strategies and optimization techniques.\n",
    "\n",
    "For more advanced usage, refer to the framework documentation and experiment with different configurations to find what works best for your specific use case."
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.12"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
