{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Tokenization Tutorial\n",
    "\n",
    "This tutorial demonstrates how to use the tokenization task in the Continual Pretraining Framework. Tokenization is a crucial step in the NLP pipeline that converts raw text into numerical tokens that can be processed by language models.\n",
    "\n",
    "## What is Tokenization?\n",
    "\n",
    "Tokenization is the process of breaking down text into smaller units called tokens. In the context of language models, these tokens are typically words, subwords, or characters that are then converted into numerical IDs using a vocabulary. These numerical representations are what the model actually processes during training and inference.\n",
    "\n",
    "## Why is Tokenization Important?\n",
    "\n",
    "- **Model Input Preparation**: Language models don't understand raw text; they need numerical inputs.\n",
    "- **Vocabulary Management**: Tokenization helps manage the size of the vocabulary the model needs to learn.\n",
    "- **Context Length Control**: It helps in managing the context length (sequence length) that will be fed to the model.\n",
    "- **Performance Optimization**: Proper tokenization can significantly improve training efficiency and model performance.\n",
    "\n",
    "In this tutorial, we'll walk through the process of tokenizing a dataset using the Continual Pretraining Framework's tokenization task."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Setup\n",
    "\n",
    "First, let's import the necessary modules and set up our environment."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Import necessary libraries\n",
    "import os\n",
    "import sys\n",
    "from box import Box\n",
    "from datasets import load_dataset, Dataset\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "from transformers import AutoTokenizer\n",
    "\n",
    "# Add the project root to the Python path to import modules from src\n",
    "project_root = os.path.abspath(os.path.join(os.getcwd(), '..'))\n",
    "if project_root not in sys.path:\n",
    "    sys.path.append(project_root)\n",
    "\n",
    "# Import framework modules\n",
    "from src.tasks.tokenization import execute\n",
    "from src.tasks.tokenization.orchestrator import TokenizationOrchestrator\n",
    "from src.tasks.tokenization.tokenizer import CausalLMTokenizer\n",
    "from src.tasks.tokenization.tokenizer.config import TokenizerConfig\n",
    "from src.utils.logging import VerboseLevel, get_logger\n",
    "\n",
    "# Set up logging\n",
    "logger = get_logger(__name__, VerboseLevel.INFO)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 1. Understanding the Tokenization Configuration\n",
    "\n",
    "The tokenization process in the framework is controlled by a configuration object. Let's explore the key parameters:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Let's examine the TokenizerConfig class\n",
    "help(TokenizerConfig)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The main parameters for tokenization are:\n",
    "\n",
    "- **context_length**: Maximum sequence length (in tokens) for the model input\n",
    "- **overlap**: Number of tokens to overlap between sequences when processing long texts\n",
    "- **tokenizer_name**: Name or path of the HuggingFace tokenizer to use\n",
    "- **batch_size**: Number of examples to process at once\n",
    "- **num_proc**: Number of processes for parallel processing\n",
    "- **show_progress**: Whether to display progress bars\n",
    "- **verbose_level**: Logging verbosity level\n",
    "\n",
    "Now, let's create a configuration for our tokenization task:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Create a tokenizer configuration\n",
    "tokenizer_config = TokenizerConfig(\n",
    "    context_length=512,  # Maximum sequence length\n",
    "    overlap=128,         # Overlap between sequences\n",
    "    tokenizer_name=\"gpt2\",  # Using GPT-2 tokenizer\n",
    "    batch_size=32,       # Process 32 examples at once\n",
    "    num_proc=2,          # Use 2 processes\n",
    "    show_progress=True,  # Show progress bars\n",
    "    verbose_level=VerboseLevel.INFO  # Set logging level\n",
    ")\n",
    "\n",
    "print(f\"Tokenizer configuration:\\n- Context length: {tokenizer_config.context_length}\")\n",
    "print(f\"- Overlap: {tokenizer_config.overlap}\")\n",
    "print(f\"- Tokenizer: {tokenizer_config.tokenizer_name}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 2. Preparing a Sample Dataset\n",
    "\n",
    "For this tutorial, we'll create a small sample dataset. In a real-world scenario, you might load a dataset from a file or use the Hugging Face datasets library."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Create a simple sample dataset\n",
    "sample_texts = [\n",
    "    \"The quick brown fox jumps over the lazy dog. This sentence is often used as a pangram because it contains all the letters of the English alphabet.\",\n",
    "    \"Natural language processing (NLP) is a subfield of linguistics, computer science, and artificial intelligence concerned with the interactions between computers and human language.\",\n",
    "    \"Tokenization is the process of breaking down text into smaller units called tokens. These tokens can be words, subwords, or characters.\",\n",
    "    \"Large language models like GPT-3 and GPT-4 have billions of parameters and are trained on massive datasets of text from the internet.\",\n",
    "    \"The Continual Pretraining Framework provides tools for efficient tokenization, training, and deployment of language models.\"\n",
    "]\n",
    "\n",
    "# Convert to a Hugging Face Dataset\n",
    "sample_dataset = Dataset.from_dict({\"text\": sample_texts})\n",
    "print(f\"Sample dataset size: {len(sample_dataset)} examples\")\n",
    "print(\"\\nSample example:\")\n",
    "print(sample_dataset[0])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 3. Exploring the Tokenizer\n",
    "\n",
    "Before we tokenize the entire dataset, let's explore how the tokenizer works on a single example:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Initialize the tokenizer directly\n",
    "tokenizer = AutoTokenizer.from_pretrained(tokenizer_config.tokenizer_name)\n",
    "tokenizer.pad_token = tokenizer.eos_token  # Set padding token\n",
    "\n",
    "# Tokenize a single example\n",
    "example_text = sample_texts[0]\n",
    "encoded = tokenizer(example_text, return_tensors=\"pt\")\n",
    "\n",
    "print(f\"Original text: {example_text}\")\n",
    "print(f\"\\nTokenized to {len(encoded['input_ids'][0])} tokens\")\n",
    "print(f\"Input IDs: {encoded['input_ids'][0][:10].tolist()}...\")\n",
    "\n",
    "# Decode back to text\n",
    "decoded = tokenizer.decode(encoded['input_ids'][0])\n",
    "print(f\"\\nDecoded text: {decoded}\")\n",
    "\n",
    "# Visualize token to text mapping\n",
    "tokens = tokenizer.convert_ids_to_tokens(encoded['input_ids'][0])\n",
    "print(\"\\nFirst 20 tokens:\")\n",
    "for i, token in enumerate(tokens[:20]):\n",
    "    print(f\"{i}: {token}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 4. Using the CausalLMTokenizer\n",
    "\n",
    "Now, let's use the framework's `CausalLMTokenizer` to tokenize our sample dataset:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Initialize the CausalLMTokenizer with our configuration\n",
    "causal_tokenizer = CausalLMTokenizer(tokenizer_config)\n",
    "\n",
    "# Tokenize the sample dataset\n",
    "tokenized_dataset = causal_tokenizer.tokenize(sample_dataset)\n",
    "\n",
    "# Examine the tokenized dataset\n",
    "print(f\"Tokenized dataset features: {tokenized_dataset.features}\")\n",
    "print(f\"Number of examples: {len(tokenized_dataset)}\")\n",
    "\n",
    "# Display a tokenized example\n",
    "example = tokenized_dataset[0]\n",
    "print(\"\\nExample tokenized data:\")\n",
    "print(f\"- Input IDs shape: {len(example['input_ids'])}\")\n",
    "print(f\"- Attention mask shape: {len(example['attention_mask'])}\")\n",
    "print(f\"- Labels shape: {len(example['labels'])}\")\n",
    "\n",
    "# Show the first few tokens\n",
    "print(\"\\nFirst 10 input IDs:\", example['input_ids'][:10])\n",
    "print(\"First 10 attention mask values:\", example['attention_mask'][:10])\n",
    "print(\"First 10 labels:\", example['labels'][:10])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 5. Understanding Tokenization with Overlapping\n",
    "\n",
    "The framework supports tokenization with overlapping, which is useful for processing long texts. Let's explore how this works:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Create a long text example\n",
    "long_text = \" \".join([sample_texts[i % len(sample_texts)] for i in range(20)])\n",
    "print(f\"Long text length: {len(long_text)} characters\")\n",
    "\n",
    "# Create a dataset with the long text\n",
    "long_dataset = Dataset.from_dict({\"text\": [long_text]})\n",
    "\n",
    "# Tokenize with different overlap settings\n",
    "results = {}\n",
    "for overlap in [0, 64, 128]:\n",
    "    config = TokenizerConfig(\n",
    "        context_length=256,\n",
    "        overlap=overlap,\n",
    "        tokenizer_name=\"gpt2\",\n",
    "        batch_size=1,\n",
    "        show_progress=True\n",
    "    )\n",
    "    tokenizer = CausalLMTokenizer(config)\n",
    "    tokenized = tokenizer.tokenize(long_dataset)\n",
    "    results[overlap] = {\n",
    "        \"num_examples\": len(tokenized),\n",
    "        \"first_example\": tokenized[0]\n",
    "    }\n",
    "\n",
    "# Compare results\n",
    "print(\"\\nComparison of different overlap settings:\")\n",
    "for overlap, result in results.items():\n",
    "    print(f\"\\nOverlap = {overlap}:\")\n",
    "    print(f\"- Generated {result['num_examples']} examples\")\n",
    "    \n",
    "    # Decode the first few tokens of each example\n",
    "    if result['num_examples'] > 0:\n",
    "        first_example = result['first_example']\n",
    "        decoded = tokenizer._tokenizer.decode(first_example['input_ids'][:20])\n",
    "        print(f\"- First 20 tokens decode to: '{decoded}...'\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 6. Using the TokenizationOrchestrator\n",
    "\n",
    "The framework provides a `TokenizationOrchestrator` that handles the complete tokenization workflow. Let's see how to use it:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Create a temporary directory for output\n",
    "import tempfile\n",
    "output_dir = tempfile.mkdtemp()\n",
    "print(f\"Output directory: {output_dir}\")\n",
    "\n",
    "# Create a configuration for the orchestrator\n",
    "config = Box({\n",
    "    \"tokenizer\": {\n",
    "        \"tokenizer_name\": \"gpt2\",\n",
    "        \"context_length\": 512,\n",
    "        \"overlap\": 128,\n",
    "        \"batch_size\": 32,\n",
    "        \"show_progress\": True\n",
    "    },\n",
    "    \"dataset\": {\n",
    "        \"in_memory\": True,  # We're providing the dataset directly\n",
    "    },\n",
    "    \"output\": {\n",
    "        \"path\": os.path.join(output_dir, \"tokenized_dataset\")\n",
    "    },\n",
    "    \"verbose_level\": VerboseLevel.INFO,\n",
    "    \"task\": \"clm_training\"\n",
    "})\n",
    "\n",
    "# Create the orchestrator\n",
    "orchestrator = TokenizationOrchestrator(config)\n",
    "\n",
    "# We need to provide the dataset since we're not loading it from disk\n",
    "# In a real scenario, the orchestrator would load the dataset based on config\n",
    "orchestrator.load_dataset = lambda: sample_dataset\n",
    "\n",
    "# Execute the tokenization workflow\n",
    "orchestrator.execute()\n",
    "\n",
    "print(f\"\\nTokenized dataset saved to: {config.output.path}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 7. Loading and Verifying the Tokenized Dataset\n",
    "\n",
    "Now, let's load the tokenized dataset from disk and verify its contents:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Load the tokenized dataset from disk\n",
    "from datasets import load_from_disk\n",
    "\n",
    "loaded_dataset = load_from_disk(config.output.path)\n",
    "print(f\"Loaded dataset features: {loaded_dataset.features}\")\n",
    "print(f\"Number of examples: {len(loaded_dataset)}\")\n",
    "\n",
    "# Verify the first example\n",
    "example = loaded_dataset[0]\n",
    "print(\"\\nFirst example:\")\n",
    "print(f\"- Input IDs shape: {len(example['input_ids'])}\")\n",
    "print(f\"- First 10 input IDs: {example['input_ids'][:10]}\")\n",
    "\n",
    "# Decode the first example\n",
    "decoded_text = tokenizer.decode(example['input_ids'])\n",
    "print(f\"\\nDecoded text from first example:\\n{decoded_text[:200]}...\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 8. Analyzing Token Distribution\n",
    "\n",
    "Let's analyze the distribution of tokens in our tokenized dataset:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Collect token statistics\n",
    "token_counts = {}\n",
    "sequence_lengths = []\n",
    "\n",
    "for example in loaded_dataset:\n",
    "    # Count non-padding tokens\n",
    "    non_padding = sum(example['attention_mask'])\n",
    "    sequence_lengths.append(non_padding)\n",
    "    \n",
    "    # Count individual tokens\n",
    "    for token_id in example['input_ids']:\n",
    "        if token_id in token_counts:\n",
    "            token_counts[token_id] += 1\n",
    "        else:\n",
    "            token_counts[token_id] = 1\n",
    "\n",
    "# Plot sequence length distribution\n",
    "plt.figure(figsize=(10, 6))\n",
    "plt.hist(sequence_lengths, bins=20)\n",
    "plt.title('Distribution of Sequence Lengths')\n",
    "plt.xlabel('Number of Tokens')\n",
    "plt.ylabel('Frequency')\n",
    "plt.grid(True, alpha=0.3)\n",
    "plt.show()\n",
    "\n",
    "# Plot most common tokens\n",
    "top_tokens = sorted(token_counts.items(), key=lambda x: x[1], reverse=True)[:20]\n",
    "token_ids, counts = zip(*top_tokens)\n",
    "token_texts = [tokenizer.decode([tid]) for tid in token_ids]\n",
    "\n",
    "plt.figure(figsize=(12, 6))\n",
    "plt.bar(range(len(token_texts)), counts)\n",
    "plt.xticks(range(len(token_texts)), token_texts, rotation=45, ha='right')\n",
    "plt.title('Most Common Tokens')\n",
    "plt.xlabel('Token')\n",
    "plt.ylabel('Count')\n",
    "plt.tight_layout()\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 9. Using the Execute Function\n",
    "\n",
    "The framework provides a simple `execute` function that can be used to run the tokenization task with a configuration. Let's see how to use it:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Create a new output directory\n",
    "new_output_dir = tempfile.mkdtemp()\n",
    "print(f\"New output directory: {new_output_dir}\")\n",
    "\n",
    "# Create a configuration for the execute function\n",
    "execute_config = Box({\n",
    "    \"tokenizer\": {\n",
    "        \"tokenizer_name\": \"gpt2\",\n",
    "        \"context_length\": 256,\n",
    "        \"overlap\": 64,\n",
    "        \"batch_size\": 32,\n",
    "        \"show_progress\": True\n",
    "    },\n",
    "    \"dataset\": {\n",
    "        \"path\": \"wikitext\",  # Using a HuggingFace dataset\n",
    "        \"name\": \"wikitext-2-raw-v1\",\n",
    "        \"split\": \"test\",\n",
    "        \"text_field\": \"text\"\n",
    "    },\n",
    "    \"output\": {\n",
    "        \"path\": os.path.join(new_output_dir, \"wikitext_tokenized\")\n",
    "    },\n",
    "    \"verbose_level\": VerboseLevel.INFO,\n",
    "    \"task\": \"clm_training\"\n",
    "})\n",
    "\n",
    "# Note: In a real scenario, you would run:\n",
    "# from src.tasks.tokenization import execute\n",
    "# execute(execute_config)\n",
    "\n",
    "# For this tutorial, we'll just print the configuration\n",
    "print(\"\\nConfiguration for execute function:\")\n",
    "for key, value in execute_config.items():\n",
    "    if isinstance(value, dict):\n",
    "        print(f\"\\n{key}:\")\n",
    "        for k, v in value.items():\n",
    "            print(f\"  {k}: {v}\")\n",
    "    else:\n",
    "        print(f\"{key}: {value}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 10. Performance Considerations\n",
    "\n",
    "Tokenization can be a performance bottleneck when processing large datasets. Here are some tips for optimizing tokenization performance:"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Fast vs. Slow Tokenizers\n",
    "\n",
    "The framework automatically detects whether a tokenizer is \"fast\" (Rust-based) or \"slow\" (Python-based) and optimizes accordingly:\n",
    "\n",
    "- **Fast tokenizers** use internal Rust parallelism and don't benefit from Python multiprocessing\n",
    "- **Slow tokenizers** benefit from Python multiprocessing with `num_proc > 1`\n",
    "\n",
    "### Batch Size\n",
    "\n",
    "Batch size affects memory usage and processing speed:\n",
    "\n",
    "- Larger batch sizes generally improve throughput but require more memory\n",
    "- The default batch size is 2000, which works well for most cases\n",
    "- For very large documents, you might need to reduce the batch size\n",
    "\n",
    "### Number of Processes\n",
    "\n",
    "The `num_proc` parameter controls parallelism:\n",
    "\n",
    "- For fast tokenizers, `num_proc=None` is optimal (uses internal Rust parallelism)\n",
    "- For slow tokenizers, setting `num_proc` to half the available CPU cores is a good starting point\n",
    "- Setting `num_proc=1` forces single-process mode\n",
    "\n",
    "Let's measure tokenization performance with different configurations:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Create a larger dataset for performance testing\n",
    "large_texts = sample_texts * 100  # Repeat the sample texts 100 times\n",
    "large_dataset = Dataset.from_dict({\"text\": large_texts})\n",
    "print(f\"Large dataset size: {len(large_dataset)} examples\")\n",
    "\n",
    "# Measure tokenization performance with different configurations\n",
    "performance_results = []\n",
    "\n",
    "# Test different batch sizes\n",
    "for batch_size in [10, 100, 1000]:\n",
    "    config = TokenizerConfig(\n",
    "        context_length=512,\n",
    "        overlap=128,\n",
    "        tokenizer_name=\"gpt2\",\n",
    "        batch_size=batch_size,\n",
    "        num_proc=None,  # Let the tokenizer decide\n",
    "        show_progress=True\n",
    "    )\n",
    "    \n",
    "    tokenizer = CausalLMTokenizer(config)\n",
    "    \n",
    "    # Measure tokenization time\n",
    "    import time\n",
    "    start_time = time.time()\n",
    "    tokenized = tokenizer.tokenize(large_dataset)\n",
    "    elapsed_time = time.time() - start_time\n",
    "    \n",
    "    performance_results.append({\n",
    "        \"batch_size\": batch_size,\n",
    "        \"num_proc\": \"auto\",\n",
    "        \"elapsed_time\": elapsed_time,\n",
    "        \"examples_per_second\": len(large_dataset) / elapsed_time\n",
    "    })\n",
    "\n",
    "# Display performance results\n",
    "print(\"\\nPerformance results:\")\n",
    "for result in performance_results:\n",
    "    print(f\"Batch size: {result['batch_size']}, Num proc: {result['num_proc']}\")\n",
    "    print(f\"  Time: {result['elapsed_time']:.2f} seconds\")\n",
    "    print(f\"  Throughput: {result['examples_per_second']:.2f} examples/sec\")\n",
    "\n",
    "# Plot performance results\n",
    "plt.figure(figsize=(10, 6))\n",
    "batch_sizes = [r['batch_size'] for r in performance_results]\n",
    "throughputs = [r['examples_per_second'] for r in performance_results]\n",
    "plt.bar(range(len(batch_sizes)), throughputs)\n",
    "plt.xticks(range(len(batch_sizes)), [f\"Batch={bs}\" for bs in batch_sizes])\n",
    "plt.title('Tokenization Performance by Batch Size')\n",
    "plt.xlabel('Configuration')\n",
    "plt.ylabel('Examples per Second')\n",
    "plt.grid(True, alpha=0.3)\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Conclusion\n",
    "\n",
    "In this tutorial, we've explored the tokenization task in the Continual Pretraining Framework. We've learned how to:\n",
    "\n",
    "1. Configure the tokenizer with appropriate parameters\n",
    "2. Tokenize datasets using the CausalLMTokenizer\n",
    "3. Use the TokenizationOrchestrator for end-to-end tokenization workflow\n",
    "4. Analyze tokenized data and understand token distributions\n",
    "5. Optimize tokenization performance\n",
    "\n",
    "Tokenization is a critical step in the language model training pipeline, and the framework provides efficient tools to handle this task at scale. The tokenized datasets produced by this process can be directly used for causal language model training in the next step of the pipeline."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Next Steps\n",
    "\n",
    "After tokenization, the next steps in the Continual Pretraining Framework are:\n",
    "\n",
    "1. **CLM Training**: Train a causal language model on the tokenized dataset\n",
    "2. **Publishing**: Publish the trained model for use in downstream tasks\n",
    "\n",
    "Check out the other tutorials in this series to learn more about these tasks!"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.10"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
