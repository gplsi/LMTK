{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "7d50166d",
   "metadata": {},
   "source": [
    "# Tokenization Instruction Tutorial\n",
    "\n",
    "This notebook demonstrates how to run the tokenization_instruction task using LMTK.\n",
    "\n",
    "- Loads a YAML config (`tokenization_instruction.yaml`)\n",
    "- Runs tokenization via the framework\n",
    "- Inspects and saves the resulting tokenized dataset\n",
    "\n",
    "**Prerequisites:**\n",
    "- Ensure `tutorials/data/raw_text_data/instruction_data.txt` exists.\n",
    "- All dependencies are installed (see README).\n",
    "- Run from the project root for correct path resolution."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Install dependencies if needed\n",
    "# %pip install pyyaml box datasets huggingface_hub\n",
    "import os\n",
    "import yaml\n",
    "from box import Box\n",
    "\n",
    "config_path = 'tutorials/configs/tokenization_instruction.yaml'\n",
    "assert os.path.exists(config_path), f'Config file not found: {config_path}'\n",
    "with open(config_path, 'r') as f:\n",
    "    config = Box(yaml.safe_load(f))\n",
    "print('Loaded config:')\n",
    "print(config)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Run Tokenization Task\n",
    "This cell runs the tokenization_instruction task using the framework's CLI.\n",
    "\n",
    "**Note:** You can also use the Python API if available."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Run tokenization task\n",
    "import subprocess\n",
    "result = subprocess.run(['python', 'src/main.py', '--config', config_path], capture_output=True, text=True)\n",
    "print(result.stdout)\n",
    "if result.returncode != 0:\n",
    "    print(result.stderr)\n",
    "    raise RuntimeError('Tokenization failed!')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Inspect Tokenized Dataset\n",
    "Check that the dataset was saved as expected."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from datasets import load_from_disk\n",
    "tokenized_path = config.dataset.output_dir\n",
    "assert os.path.exists(tokenized_path), f'Tokenized dataset not found: {tokenized_path}'\n",
    "ds = load_from_disk(tokenized_path)\n",
    "print(ds)\n",
    "# Show a sample\n",
    "print(ds['test'][0] if 'test' in ds else ds[0])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## (Optional) Push to the Hub\n",
    "Uncomment and configure the following if you want to push the tokenized dataset to the Hugging Face Hub.\n",
    "Make sure you are authenticated and have set `repo_id` in the config."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# from datasets import load_dataset\n",
    "# ds.push_to_hub(config.output.repo_id, commit_message=config.output.commit_message)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "name": "python",
   "version": "3.12"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
