"""

"""

import torch
from transformers import AutoModelForCausalLM, AutoModelForMaskedLM, AutoTokenizer, AutoConfig
from box import Box
from huggingface_hub import HfApi
import os
from logging import getLogger

logger = getLogger(__name__)

class FSDPtoHuggingFace:
    def __init__(self, base_model, checkpoint_path, task_type: str | None = None, convert_modifications: dict | None = None):
        self.base_model = base_model
        self.checkpoint_path = checkpoint_path
        # task_type: "mlm" | "clm" | "instruction" (treated as causal)
        self.task_type = (task_type or "").lower().strip() or None
        self.convert_modifications = Box(convert_modifications or {}, box_dots=True)

    def _fix_fsdp_state_dict_keys(self, checkpoint_state_dict, model_state_dict):
        """Fix FSDP checkpoint keys with proper lm_head handling."""
        logger.info("üîß Fixing FSDP checkpoint key structure...")
    
        checkpoint_keys = list(checkpoint_state_dict.keys())
        model_keys = list(model_state_dict.keys())
        
        # Log sample keys if available
        if checkpoint_keys:
            logger.info(f"Sample checkpoint key: {checkpoint_keys[0]}")
        else:
            logger.info("No checkpoint keys found")
            
        if model_keys:
            logger.info(f"Sample model key: {model_keys[0]}")
        else:
            logger.info("No model keys found")
        
        fixed_state_dict = {}
        
        for key, value in checkpoint_state_dict.items():
            new_key = None
            
            # Handle different FSDP key patterns
            if key.startswith("model.model."):
                # Most parameters: model.model.layers.X -> model.layers.X
                new_key = key.replace("model.model.", "model.", 1)
            elif key.startswith("model.embed_tokens."):
                # Embedding tokens: keep as is
                new_key = key
            elif key == "model.lm_head.weight":
                # lm_head: model.lm_head.weight -> lm_head.weight
                new_key = "lm_head.weight"
            else:
                new_key = key
                
            # Check if the new key exists in model
            if new_key in model_state_dict:
                fixed_state_dict[new_key] = value
            else:
                # Special handling for lm_head when it might be tied
                if "lm_head" in key:
                    logger.warning(f"üîó lm_head parameter found but not in model state_dict - likely tied weights")
                    continue  # Skip lm_head if it's tied to embeddings
                else:
                    logger.warning(f"‚ö†Ô∏è  Could not map key: {key} -> tried {new_key}")
        
        logger.info(f"‚úÖ Successfully mapped {len(fixed_state_dict)}/{len(checkpoint_state_dict)} parameters")
        return fixed_state_dict
        

    def _load_fsdp_checkpoint_safely(self, checkpoint_path):
        """Load FSDP checkpoint handling common formats."""
        logger.info(f"Loading FSDP checkpoint from {checkpoint_path}...")
        checkpoint = torch.load(checkpoint_path, map_location="cpu")
        
        # Handle different FSDP checkpoint structures
        if isinstance(checkpoint, dict):
            # Common FSDP checkpoint keys
            if 'model_state_dict' in checkpoint:
                state_dict = checkpoint['model_state_dict']
                logger.info("‚úÖ Found model_state_dict in FSDP checkpoint")
            elif 'state_dict' in checkpoint:
                state_dict = checkpoint['state_dict']  
                logger.info("‚úÖ Found state_dict in FSDP checkpoint")
            elif 'model' in checkpoint:
                state_dict = checkpoint['model']
                logger.info("‚úÖ Found model in FSDP checkpoint")
            elif 'app' in checkpoint and 'model' in checkpoint['app']:
                # Distributed checkpoint format
                state_dict = checkpoint['app']['model']
                logger.info("‚úÖ Found distributed checkpoint format")
            else:
                # Assume whole checkpoint is state dict
                state_dict = checkpoint
                logger.info("‚úÖ Using entire checkpoint as state_dict")
        else:
            state_dict = checkpoint
            logger.info("‚úÖ Checkpoint is direct state_dict")
        
        return state_dict

    def _apply_weight_tying(self, model):
        logger.info("üîó Applying weight tying...")
        model.tie_weights()  # This will tie lm_head.weight to embed_tokens.weight


    def _info(self, model, missing_keys, unexpected_keys):
        """Log information about the conversion results"""
        from src.utils.logging import get_logger
        logger = get_logger(__name__)
        
        # Get model state dict for parameter counting
        model_state_dict = model.state_dict()
        
        # Check for missing keys after weight tying
        remaining_missing = []
        for key in missing_keys:
            if key not in model_state_dict:
                remaining_missing.append(key)

        if remaining_missing:
            logger.warning(f"‚ö†Ô∏è  {len(remaining_missing)} keys still missing after weight tying")
            logger.warning(f"Sample missing: {remaining_missing[:3]}")
        else:
            logger.info("‚úÖ All parameters resolved after weight tying!")
        
        if unexpected_keys:
            logger.warning(f"‚ö†Ô∏è  {len(unexpected_keys)} unexpected keys")
            logger.warning(f"Sample unexpected: {unexpected_keys[:3]}")
        
        # Tensor-level summary
        total_tensors = len(model_state_dict)
        loaded_tensors = total_tensors - len(remaining_missing)
        if total_tensors == 0:
            tensor_loading_pct = 100.0
        else:
            tensor_loading_pct = (loaded_tensors / total_tensors) * 100

        # Element-level summary (scalar parameters)
        total_elems = sum(v.numel() for v in model_state_dict.values())
        loaded_elems = sum(
            v.numel() for k, v in model_state_dict.items() if k not in remaining_missing
        )
        if total_elems == 0:
            elem_loading_pct = 100.0
        else:
            elem_loading_pct = (loaded_elems / total_elems) * 100

        logger.info(
            f"‚úÖ Final result: tensors {loaded_tensors}/{total_tensors} ({tensor_loading_pct:.1f}%), "
            f"elements {loaded_elems}/{total_elems} ({elem_loading_pct:.2f}%)"
        )
        
        if elem_loading_pct < 95:
            raise ValueError(
                f"Only {elem_loading_pct:.2f}% of scalar parameters loaded from FSDP checkpoint"
            )
        
        # Check for weight tying, but handle mock objects in tests
        try:
            if hasattr(model, 'lm_head') and hasattr(model.model, 'embed_tokens'):
                if torch.equal(model.lm_head.weight, model.model.embed_tokens.weight):
                    logger.info("‚úÖ lm_head.weight properly tied to embed_tokens.weight")
                else:
                    logger.warning("‚ö†Ô∏è  Weight tying may not be active")
        except (TypeError, AttributeError):
            # Skip weight tying check in test environment with mock objects
            logger.info("Skipping weight tying check in test environment")

    def _convert_checkpoint(self, model, checkpoint_file):
            original_state_dict = self._load_fsdp_checkpoint_safely(checkpoint_file)
            model_state_dict = model.state_dict()
            fixed_state_dict = self._fix_fsdp_state_dict_keys(original_state_dict, model_state_dict)
            
            
            # Handle both tuple return and None return (in tests)
            result = model.load_state_dict(fixed_state_dict, strict=False)
            if result is None:
                missing_keys, unexpected_keys = [], []
            else:
                try:
                    missing_keys, unexpected_keys = result
                except (ValueError, TypeError):
                    missing_keys, unexpected_keys = [], []
            logger.info(self._info(model, missing_keys, unexpected_keys))
            logger.info("üîó Applying weight tying...")
            model.tie_weights()
            return model

    def execute(self, output_dir):
        config = AutoConfig.from_pretrained(self.base_model)
        # Determine if single or multiple checkpoints
        checkpoint_files = []
        if self.checkpoint_path.endswith(".pth"):
            checkpoint_files = [self.checkpoint_path]
        else:
            for root, dirs, files in os.walk(self.checkpoint_path):
                files = sorted(files)
                for file in files:
                    if file.endswith(".pth"):
                        checkpoint_files.append(os.path.join(root, file))
        
        # Fresh model per checkpoint and save immediately
        summary = []
        total = len(checkpoint_files)
        for idx, checkpoint_file in enumerate(checkpoint_files, 1):
            logger.info(f"[Progress] Converting checkpoint {idx}/{total}: {checkpoint_file}")
            cfg = AutoConfig.from_pretrained(self.base_model)
            # If KAN FFN was used, annotate config to preserve architecture info in HF artifact
            if hasattr(self.convert_modifications, 'replace_ffn'):
                ffn_cfg = self.convert_modifications.replace_ffn
                if isinstance(ffn_cfg, (dict, Box)):
                    cfg._name_or_path = getattr(cfg, '_name_or_path', self.base_model)
                    cfg.lmtk_replace_ffn = {
                        'type': 'kan_chebyshev',
                        'cheb_order': int(getattr(ffn_cfg, 'cheb_order', ffn_cfg.get('cheb_order', 1)))
                    }
            is_decoder = bool(getattr(cfg, "is_decoder", False))
            if self.task_type == "mlm" or (not is_decoder and self.task_type is None):
                model = AutoModelForMaskedLM.from_config(cfg)
            else:
                cfg.is_decoder = True
                model = AutoModelForCausalLM.from_config(cfg)
            try:
                # Load original to optionally extract KAN tensors
                original_state_dict = self._load_fsdp_checkpoint_safely(checkpoint_file)
                kan_keys = [
                    k for k in original_state_dict.keys()
                    if (".intermediate.kan" in k.lower()) or (".mixing." in k.lower()) or ("chebyshev" in k.lower())
                ]
                kan_state = {k: original_state_dict[k] for k in kan_keys}

                model = self._convert_checkpoint(model, checkpoint_file)
                base_name = os.path.splitext(os.path.basename(checkpoint_file))[0]
                final_dir = os.path.join(output_dir, base_name)
                os.makedirs(final_dir, exist_ok=True)
                logger.info(f"[Progress] Saving HuggingFace model {idx}/{total} to {final_dir}")

                # Persist KAN tensors if any and note in config
                if len(kan_state) > 0:
                    try:
                        from safetensors.torch import save_file as save_sft
                        kan_state_file = os.path.join(final_dir, 'kan_ffn_state.safetensors')
                        save_sft(kan_state, kan_state_file)
                    except Exception:
                        kan_state_file = os.path.join(final_dir, 'kan_ffn_state.pt')
                        torch.save(kan_state, kan_state_file)
                    if not hasattr(model.config, 'lmtk_replace_ffn'):
                        model.config.lmtk_replace_ffn = {'type': 'kan_chebyshev'}
                    model.config.lmtk_kan_state_file = os.path.basename(kan_state_file)
                    model.config.lmtk_kan_keys_count = len(kan_state)

                model.save_pretrained(final_dir)
                tokenizer = AutoTokenizer.from_pretrained(self.base_model)
                tokenizer.save_pretrained(final_dir)
                summary.append({'checkpoint': checkpoint_file, 'success': True, 'error': None, 'output_dir': final_dir})
            except Exception as e:
                logger.error(f"Failed to convert {checkpoint_file}: {e}")
                summary.append({'checkpoint': checkpoint_file, 'success': False, 'error': str(e)})
        
        logger.info("All conversions completed.")
        logger.info(f"Summary: {summary}")
        return summary