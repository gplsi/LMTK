# Example configuration for Instruction Tuning with DeepSpeed strategy
# This demonstrates the new compositional schema structure

# Base configuration
task: "instruction"
experiment_name: "llama3-8b-instruction-deepspeed-example"

# Model configuration (from components/model.schema.yaml)
model_name: "meta-llama/Llama-3.2-1B-Instruct"
precision: "bf16-true"
verbose_level: "INFO"
log_every_n_steps: 5

# Data configuration (from components/data.schema.yaml)
dataset:
  source: local
  nameOrPath: "workspace/data/tokenized_outputs/instruction_v5"

# Training arguments (from components/training_args.schema.yaml)
number_epochs: 3
batch_size: 1
validate_after_epoch: true
validate_on_end: true
validate_after_k_steps: 500
save_on_validate: false
save_on_end: true
checkpoint: null
output_dir: "/workspace/output/instruction_training_example"

# Optimizer configuration (from components/optimizer.schema.yaml)
gradient_accumulation: true
gradient_accumulation_steps: 64
grad_clip: 1.0
lr: 5e-6  # Lower learning rate for instruction tuning
lr_decay: true
weight_decay: 0.01
beta1: 0.9
beta2: 0.95

# Scheduler configuration (from components/scheduler.schema.yaml)
lr_scheduler: "warmup_linear"
warmup_proportion: 0.03  # Smaller warmup for instruction tuning

# Instruction-specific configuration
ignore_index: -100
chat_template: null  # Use model's default chat template
system_message: "You are a helpful assistant."

# Parallelization strategy
parallelization_strategy: "fsdp"
parallelization_config:
  # FSDP-specific configuration (from strategy/fsdp.schema.yaml)
  auto_wrap_policy: "gpt2"
  sharding_strategy: "FULL_SHARD"
  state_dict_type: "full"
  limit_all_gathers: true
  cpu_offload: false
  num_workers: 4
  gradient_checkpointing: true


# Logging configuration
logging_config: "wandb"

# Seed for reproducibility
seed: 42
