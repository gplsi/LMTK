# Example configuration for Instruction Tokenization
# Uses data from /data/v5 directory for instruction tuning dataset preparation

task: "tokenization"
experiment_name: "instruction-tokenization-v5-example"
verbose_level: 4

tokenizer:
  tokenizer_name: "meta-llama/Llama-3.2-1B-Instruct"
  context_length: 1024
  task: instruction
  
  # Performance optimization settings
  batch_size: 1000
  num_proc: 4
  show_progress: true
  
  # Instruction tuning specific settings
  mask_prompt: true
  ignore_index: -100
  max_seq_length: 1024
  
  # Padding strategy configuration
  padding_strategy: "dynamic"  # Options: "fixed" (default) or "dynamic"
  
  # Masking strategy configuration
  masking_strategy: "context_aware"  # Options: "context_aware" (default) or "response_only"

dataset:
  source: local
  nameOrPath: "/workspace/data/v5"
  format: files
  file_config:
    format: json

output:
  path: "/workspace/data/tokenized_outputs/instruction_v5"

test_size: 0.1
