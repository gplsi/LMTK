# Example configuration for CLM training with FSDP strategy
# This demonstrates the new compositional schema structure

# Base configuration
task: "clm_training"
experiment_name: "llama3-8b-clm-fsdp-example"

# Model configuration (from components/model.schema.yaml)
model_name: "gpt2"
precision: "bf16-true"
verbose_level: "INFO"
log_every_n_steps: 10

# Data configuration (from components/data.schema.yaml)
dataset:
  source: local
  nameOrPath: "workspace/data/tokenized_outputs/clm_testing"

# Training arguments (from components/training_args.schema.yaml)
number_epochs: 3
batch_size: 2
validate_after_epoch: true
validate_on_end: true
validate_after_k_steps: null
save_on_validate: false
save_on_end: true
checkpoint: null
output_dir: "/workspace/output/clm_training_example"

# Optimizer configuration (from components/optimizer.schema.yaml)
gradient_accumulation: true
gradient_accumulation_steps: 32
grad_clip: 1.0
lr: 2e-5
lr_decay: true
weight_decay: 0.01
beta1: 0.9
beta2: 0.95

# Scheduler configuration (from components/scheduler.schema.yaml)
lr_scheduler: "warmup_linear"
warmup_proportion: 0.06

# Parallelization strategy
parallelization_strategy: "fsdp"
parallelization_config:
  # FSDP-specific configuration (from strategy/fsdp.schema.yaml)
  auto_wrap_policy: "llama"
  sharding_strategy: "FULL_SHARD"
  state_dict_type: "full"
  limit_all_gathers: true
  cpu_offload: false
  num_workers: 4
  gradient_checkpointing: true

# Logging configuration
logging_config: "wandb"
# WandB configuration would be handled by training_logging.schema.yaml

# Seed for reproducibility
seed: 42
