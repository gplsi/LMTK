# experiments/continual-pretrain.yaml
task: pretraining
experiment_name: "continual-pretrain-test"
model:
  architecture: transformer
  num_layers: 32
  hidden_size: 4096

distributed:
  strategy: fsdp
  config:
    sharding_strategy: FULL_SHARD
    cpu_offload: true

pretraining_params:
  learning_rate: 0.0001
  batch_size: 1024
  dataset: "redpajama-v2"
