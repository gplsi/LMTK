# Configuration for Tokenization Instruction Task - ALIA Dataset
# This config uses the ALIA dataset configuration for salamandra-2b-instruct
# to process multi-language instruction datasets with chat templates

task: "tokenization_instruction"
experiment_name: "ALIA_instruction_sala_2b_v2"
verbose_level: 4

# Model and tokenizer configuration
model_name: "BSC-LT/salamandra-2b-instruct"
llm_type: "causal"
model_type: "llama"

# Tokenizer configuration
tokenizer:
  tokenizer_name: "BSC-LT/salamandra-2b-instruct"  # Salamandra tokenizer with chat template support
  context_length: 512          # Maximum sequence length for instruction tuning (from train_args.max_seq_length)
  batch_size: 2000             # Batch size for tokenization processing
  num_proc: 1                  # Number of processes for parallel tokenization
  show_progress: true          # Show progress bars during tokenization

# Multi-language configuration
languages: ['ca', 'en', 'es', 'eu', 'gl', 'pt']  # Languages to process

# Data path (ALIA dataset v2)
dataPath: "/workspace/NAS/GPLSI/odesiaChallenge/data/ALIA/v2"  # Base path containing language subdirectories

# Dataset splitting configuration (70/30 split)
train_size: 0.7             # 70% for training, 30% for validation
random_seed: 42             # Seed for reproducible splits

# Output paths (matching ALIA tokenized data structure)
output:
  train_path: "/workspace/NAS/GPLSI/odesiaChallenge/data/ALIA/tokenized_data/ALIA/v2/tokenizedTrain"    # Path for training dataset
  validation_path: "/workspace/NAS/GPLSI/odesiaChallenge/data/ALIA/tokenized_data/ALIA/v2/tokenizedEval"  # Path for validation dataset

# Instruction tuning specific configuration
prompt_style: "tinyllama"   # Prompt style for instruction formatting
mask_prompt: true           # Mask prompt tokens in labels (standard for instruction tuning)
ignore_index: -100          # Index to ignore in loss calculation
temperature: 1e-3           # Temperature for sampling (if used during preprocessing)
top_p: 0.9                  # Top-p for sampling (if used during preprocessing)
max_new_tokens: 20          # Maximum new tokens (if used during preprocessing)
do_sample: true             # Whether to use sampling (if used during preprocessing)
precision: "bf16-true"      # Precision for computation

# Training arguments (passed to tokenizer_dataset_multiTurn)
train_args:
  epochs: 3
  learning_rate: 1e-5
  beta1: 0.9
  beta2: 0.95
  weight_decay: 1e-1
  global_batch_size: 4
  micro_batch_size: 1
  log_interval: 5
  save_interval: 5
  lr_warmup_steps: 100
  max_seq_length: 512         # Maximum sequence length for tokenization
  mask_prompt: true           # Mask prompt tokens in labels
  ignore_index: -100          # Index to ignore in loss calculation

# Evaluation arguments
eval_args:
  micro_batch_size: 1
  log_interval: 5
  interval: 5
  max_iters: 10000

# Training configuration flags
is_training: true           # Flag indicating this is for training
is_evaluate: true           # Flag indicating evaluation is enabled
decay_lr: true              # Whether to decay learning rate
max_step: 100000            # Maximum training steps
grad_clip: 1.0              # Gradient clipping value
log_step_interval: 1        # Logging step interval

# Paths for model checkpoints
checkpoint_path: "/models/ALIA/"
wandb_project: "ALIA_instruction_sala_2b_v2"
