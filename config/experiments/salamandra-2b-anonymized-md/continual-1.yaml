task: "clm_training"   # pretraining, tokenization
experiment_name: "md_anonymized_salamandra_2b_clm_training"
verbose_level: 3

# Training data
dataset:
  source: "local"  # huggingface, local
  nameOrPath: "/workspace/data/tokenized/md-anonymized-salamandra-2b"
  format: "hf"


# Output dir
output_dir: "output/s2b-md"
precision: "bf16-true"

# Model
model_name: "BSC-LT/salamandra-2b"

# Model precision
static_graph: false

# Training arguments
number_epochs: 2
batch_size: 2
gradient_accumulation: true
gradient_accumulation_steps: 24
grad_clip: 1.0

# lr 2e-5
lr: 0.00006
lr_decay: true
lr_scheduler: "cosine"
weight_decay: 0.1
beta1: 0.9
beta2: 0.95
warmup_proportion: 0

# Validation parameters
validate_on_end: true
validate_after_epoch: true
validate_after_k_steps: 750

# Checkpoint saving logic
save_on_validate: true
save_on_end: false

# Gradient parameters
gradient_accumulation: true
gradient_accumulation_steps: 8
grad_clip: 1.0

# Distributed_strategy
parallelization_strategy: "fsdp"

# FSDP specific parameters
# auto_wrap_policy: "llama"
sharding_strategy: "FULL_SHARD"
state_dict_type: "sharded"
limit_all_gathers: true
cpu_offload: false
num_workers: 4
gradient_checkpointing: true

# Logging
logging_config: "wandb"

# Wandb logging specific parameters
wandb_project: "salamandra-2b_fsdp__allen-md-anonymized"
wandb_entity: "gplsi_continual"
log_model: false
log_iter_interval: 10

seed: 42
checkpoint: "/workspace/output/s2b-md/iter-143999-ckpt.pth"