task: "mlm_training"
experiment_name: "wikitext103-roberta-mlm-ddp-attn-chebyshev-o4"
verbose_level: 3

# Model configuration
model_name: "roberta-base"
precision: "bf16-true"
log_every_n_steps: 10

roberta:
  custom: true
  from_scratch: true
  vocab_size: 50265
  hidden_size: 768
  num_hidden_layers: 12
  num_attention_heads: 12
  intermediate_size: 3072
  hidden_dropout_prob: 0.1
  attention_dropout_prob: 0.1
  max_position_embeddings: 514
  layer_norm_eps: 1e-5

modifications:
  # Replace attention with Chebyshev-based variant (keep FFN standard)
  replace_attention:
    mode: "chebyshev"   # "chebyshev" | "orthogonal" | "orthogonal_sparse" | "sparse"
    cheb_order: 4

dataset:
  source: local
  nameOrPath: "/workspace/data/tokenized/wikitext103_roberta_mlm_512_corpus"
  format: dataset

number_epochs: 5
batch_size: 16
num_workers: 6
gradient_checkpointing: false
output_dir: "/workspace/outputs/initial/roberta_mlm_ddp_attn_cheb_o4"

validate_after_epoch: true
validate_on_end: true
validate_after_k_steps: 1000
save_on_validate: true
save_on_end: true

gradient_accumulation: true
gradient_accumulation_steps: 8
grad_clip: 1.0
lr: 0.00015
lr_decay: true
weight_decay: 0.01
beta1: 0.9
beta2: 0.98

lr_scheduler: "warmup_linear"
warmup_proportion: 0.03

mlm_probability: 0.15
ignore_index: -100

parallelization_strategy: "ddp"
find_unused_parameters: true
process_group_backend: "nccl"
static_graph: false

logging_config: "wandb"
wandb_project: "traceable-attention-experiments"
wandb_entity: null

seed: 42


