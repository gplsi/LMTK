task: "clm_training"
experiment_name: "llama3.2-3b-bench-continual"
verbose_level: 3

# Training data
dataset:
  source: "local"
  nameOrPath: "/workspace/data/tokenized/dc6-llama-3b"
  #nameOrPath: "/workspace/data/tokenized/anonymized-va-bench"
  format: "hf"

# Output dir
output_dir: "output/llama3_2-3b-bench/"
precision: "bf16-true"
# Model
model_name: "meta-llama/Llama-3.2-3B"
#"meta-llama/Llama-3.2-3B"

# Model precision
static_graph: false

# Training parameters
number_epochs: 1
batch_size: 6
gradient_accumulation: true
gradient_accumulation_steps: 4
grad_clip: 1.0

# Optimizer parameters
lr: 0.00002      
lr_decay: true
weight_decay: 0.01
beta1: 0.9
beta2: 0.999


# Scheduler parameters
lr_scheduler: "cosine"
warmup_proportion: 0

# Validation and checkpoint saving parameters
validations_per_epoch: 1
validate_after_epoch: false
validate_on_end: false
save_on_end: false
save_on_validate: false


# Distributed_strategy
parallelization_strategy: "fsdp"

# FSDP specific parameters
auto_wrap_policy: "llama"
sharding_strategy: "FULL_SHARD"
state_dict_type: "sharded"
limit_all_gathers: true
cpu_offload: false
num_workers: 4
gradient_checkpointing: true

# Wandb logging specific parameters
logging_config: wandb
wandb_project: "lmtk-benchmark-2"
wandb_entity: "gplsi_continual"
wandb_run_name: "llama3_2-3b-bench-continual-7gpu-prueba-batch6"
log_model: false
log_iter_interval: 10


seed: 42
