# Experiment configuration for tokenizing instruction data using GPT-2
task: "tokenization"
experiment_name: "gpt-2_tokenizationinstruction_multilingual"
verbose_level: 4

tokenizer:
  tokenizer_name: gpt2
  context_length: 1024  # GPT-2's maximum context length
  task: instruction
  batch_size: 2000
  num_proc: 1
  show_progress: true
  mask_prompt: true
  ignore_index: -100
  max_seq_length: 1024

dataset:
  source: local
  nameOrPath: data/v5
  format: files
  file_config:
    format: json

output:
  path: output/tokenized_gpt2_instructions

test_size: 0.1
