# Example configuration for MLM training with DDP strategy
# This demonstrates the new compositional schema structure

# Base configuration
task: "mlm_training"
experiment_name: "RobertaBNE-base-mlm-ddp-example"

# Model configuration (from components/model.schema.yaml)
model_name: "BSC-LT/roberta-base-bne"
precision: "bf16-true"
verbose_level: 3
log_every_n_steps: 10

# Data configuration (from components/data.schema.yaml)
dataset:
  source: local
  nameOrPath: "/workspace/data/tokenized/mlm_turismo"

# Training arguments (from components/training_args.schema.yaml)
number_epochs: 1
batch_size: 4
validate_after_epoch: true
validate_on_end: true
validate_after_k_steps: 1000
save_on_validate: true
save_on_end: true
checkpoint: null
num_workers: 4
gradient_checkpointing: false
output_dir: "/workspace/output/mlm_training_turismo"

# Optimizer configuration (from components/optimizer.schema.yaml)
gradient_accumulation: true
gradient_accumulation_steps: 32
grad_clip: 1.0
lr: 0.00005
lr_decay: true
weight_decay: 0.01
beta1: 0.9
beta2: 0.999

# Scheduler configuration (from components/scheduler.schema.yaml)
lr_scheduler: "warmup_linear"
warmup_proportion: 0.06

# MLM-specific configuration
#mlm_probability: 0.15
ignore_index: -100

# Parallelization strategy
parallelization_strategy: "ddp"
parallelization_config:
  # DDP-specific configuration (from strategy/ddp.schema.yaml)
  find_unused_parameters: false
  gradient_as_bucket_view: true
  static_graph: false

# Logging configuration
logging_config: "wandb"

# Seed for reproducibility
seed: 123
