task: "tokenization"
experiment_name: "news_corpus_val-es"
verbose_level: 4

tokenizer:
  name: "gpt2"  # Using GPT2 as example, replace with your preferred pre-trained tokenizer
  context_length: 512
  overlap: 128
  task: "causal_pretraining"

dataset:
  source: "local"
  nameOrPath: "/workspace/data/raw/corpus-val-es"
  format: "files"
  file_config:
    format: "txt"
    encoding: "utf-8"

output:
  path: "data/tokenized/news_corpus"
