task: "tokenization"
experiment_name: "news_corpus_val-es"
verbose_level: 4

tokenizer:
  name: "meta-llama/Llama-3.2-1B"  # Using GPT2 as example, replace with your preferred pre-trained tokenizer
  context_length: 1024
  overlap: 256
  task: "causal_pretraining"

dataset:
  source: "local"
  nameOrPath: "/workspace/data/raw/corpus"
  format: "files"
  use_txt_as_samples: true
  file_config:
    format: "txt"
    encoding: "utf-8"

output:
  path: "data/tokenized/news_corpus"

test_size: 0