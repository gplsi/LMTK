task: "continual"
experiment_name: "news_corpus_val-es"

verbose_level: 4

# Dataset dir
train_data_dir: "data/tokenized/news_corpus"

# Model
model_name: "openai-community/gpt2"


# Checkpointing
checkpoint: null

# Model precision
precision: "bf16-true"

# Training parameters
number_epochs: 1
micro_batch_size: 1


# Gradient parameters
gradient_accumulation: true
gradient_accumulation_steps: 64
grad_clip: 1.0


# Optimizer parameters
lr: 2e-5      
lr_decay: true
weight_decay: 0.01
beta1: 0.9
beta2: 0.999


# Scheduler parameters
lr_scheduler: "warmup_linear"
warmup_proportion: 0.06   


# Logging
logging_config: "wandb"

# Wandb logging specific parameters
wandb_project: "prueba_continual_framework"
wandb_entity: "gplsi_continual"
log_model: false
log_iter_interval: 10


# Distributed_strategy
parallelization_strategy: "fsdp"

# FSDP specific parameters
auto_wrap_policy: "LlamaDecoderLayer"
sharding_strategy: "FULL_SHARD"
state_dict_type: "full"
limit_all_gathers: true
cpu_offload: false
num_workers: 4
gradient_checkpointing: true

