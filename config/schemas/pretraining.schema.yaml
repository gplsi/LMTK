$schema: "http://json-schema.org/draft-07/schema#"
$id: "pretraining.schema.yaml"
description: "Main schema for training configuration"

allOf:
  - $ref: "base.schema.yaml"
  - type: object
    properties:

      # Data arguments
      dataset:
        type: object
        properties:
          source:
            type: string
            enum: ["huggingface", "local"]
          nameOrPath:
            type: string
            description: "Dataset name for HuggingFace or local path"

      # Model arguments
      model_name:
        type: string
        default: "meta-llama/Meta-Llama-3-8B"
        description: "Name or path of the model to load"
      precision:
        type: string
        default: "bf16-true"
        description: "Precision for the loaded model"

      # Checkpoint arguments
      checkpoint:
        type: string
        default: null
        description: "Path to the checkpoint file for resume"

      # Training arguments
      number_epochs:
        type: integer
        default: 2
        description: "Number of epochs to train"
      micro_batch_size:
        type: integer
        default: 1
        description: "Size of each data batch"

      # Optimizer arguments
      gradient_accumulation:
        type: boolean
        default: true
        description: "If True, use gradient accumulation"

      gradient_accumulation_steps:
        type: integer
        default: 64
        description: "Number of gradient accumulation steps"
      
      grad_clip:
        type: number
        default: 1.0
        description: "Gradient clipping value"
      
      lr:
        type: number
        default: 2e-5
        description: "Learning rate for the entire model"
      
      lr_decay:
        type: boolean
        default: true
        description: "If True, apply learning rate decay"
      

      weight_decay:
        type: number
        default: 0.01
        description: "Weight decay factor"
      
      beta1:
        type: number
        default: 0.9
        description: "Beta1 for the Adam optimizer"

      beta2:  
        type: number
        default: 0.95
        description: "Beta2 for the Adam optimizer"
      
      lr_scheduler:
        type: string
        default: "linear"
        enum: ["fixed", "warmup_linear", "warmup_constant"]
        description: "Learning rate scheduler to use"

      warmup_proportion:
        type: number
        default: 0.06
        description: "Proportion of training steps to warm up the learning rate"
    
      # Output arguments
      output_dir:
        type: string
        default: "/data/training_outputs"
        description: "Path to the output directory"

      # Training strategy arguments
      parallelization_strategy:
        type: string
        enum: ["none", "ddp", "fsdp", "deep_speed", "dp"]
        default: "none"
        description: "Which distributed training strategy to use"

      # Parallelization config - details differ depending on strategy
      parallelization_config:
        type: object
        default: {}
        description: "Configuration object for the selected parallelization strategy"

      # Logging arguments
      logging_config:
        type: string
        enum: ["none", "wandb"]
        default: "none"
        description: "Which logging strategy to use"

      # Seed arguments
      seed:
        type: [integer, 'null']
        default: null
        description: "Seed for reproducibility"

    required:
      - dataset
      - model_name
      - precision
      - number_epochs
      - micro_batch_size
      - gradient_accumulation_steps
      - grad_clip
      - lr
      - lr_decay
      - lr_scheduler
      - warmup_proportion
      - weight_decay
      - beta1
      - beta2
      - parallelization_strategy
      - output_dir




    dependentSchemas:
      fsdp:
        properties:
          parallelization_config:
            allOf:
              - $ref: "pretraining.fsdp.schema.yaml"
        required: ["parallelization_config"]
      ddp:
        properties:
          parallelization_config:
            allOf:
              - $ref: "pretraining.ddp.schema.yaml"
        required: ["parallelization_config"]
      deep_speed:
        properties:
          parallelization_config:
            allOf:
              - $ref: "pretraining.deep_speed.schema.yaml"
        required: ["parallelization_config"]
      dp:
        properties:
          parallelization_config:
            allOf:
              - $ref: "pretraining.dp.schema.yaml"
        required: ["parallelization_config"]
      
      wandb:
        properties:
          logging_config:
            allOf:
              - $ref: "pretraining.logging.schema.yaml"
        required: ["logging_config"]


    # # Conditionally apply the FSDP schema if parallelization_strategy == "fsdp"
    # if:
    #   properties:
    #     parallelization_strategy:
    #       const: "fsdp"
    # then:
    #   properties:
    #     parallelization_config:
    #       # Merge the FSDP schema here
    #       allOf:
    #         - $ref: "pretraining.fsdp.schema.yaml"
    #   required:
    #     - parallelization_config
    # else:
    #   # If not fsdp, no additional constraints on parallelization_config
    #   properties: {}

    # # Conditionally apply DDP schema if parallelization_strategy == "ddp"รง
    # if:
    #   properties:
    #     parallelization_strategy:
    #       const: "ddp"
    # then:
    #   properties:
    #     parallelization_config:
    #       # Merge the DDP schema here
    #       allOf:
    #         - $ref: "pretraining.ddp.schema.yaml"
    #   required:
    #     - parallelization_config
    # else:
    #   # If not ddp, no additional constraints on parallelization_config
    #   properties: {}

    # if:
    #   properties:
    #     logging_config:
    #       const: "wandb"
    # then:
    #   properties:
    #     logging_config:
    #       # Merge the logging schema here
    #       allOf:
    #         - $ref: "pretraining.logging.schema.yaml"
    #   required:
    #     - logging_config
    # else:
    #   # If not wandb, no additional constraints on logging_config
    #   properties: {}