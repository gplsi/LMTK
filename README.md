# LM Continual Pretraining Framework

**A Modular Toolkit for Efficient Language Model Pretraining and Adaptation**  
*Streamlining continual pretraining of foundation language models through scalable pipelines and reproducible configurations*

## ğŸŒŸ Key Features

### ğŸ”§ Core Infrastructure
- **Configuration System** - Type-safe YAML schemas with Pydantic validation
- **Task Orchestration** - Modular task execution via CLI/config mapping
- **Environment Management** - Dockerized training stack with Makefile control

### ğŸ› ï¸ Training Capabilities
- **Resumable Workflows** - Atomic checkpoints with full state serialization
- **Distributed Strategies** - FSDP/DeepSpeed integration profiles
- **Curriculum Learning** - Phase-wise data mixing (`config/curricula`)

### ğŸ“Š Data Processing
- **Multi-format Corpus** - Unified processor for JSONL/Parquet/TXT
- **Streaming Tokenization** - Memory-efficient HF datasets integration
- **Data Health Checks** - Statistical validation pre-training

### ğŸ”¬ Model Monitoring
- **Training Telemetry** - Gradient/activation histograms in `utils/monitoring.py`
- **Early Warning System** - NaN/overflow detection
- **Optimizer Diagnostics** - Learning rate/parameter scale tracking

## ğŸš€ Quick Start

# Build environment
```
make build
```

# Validate configuration
```
make validate CONFIG=config/pretraining.yaml
```
# Run tokenization pipeline
```
make tokenize CONFIG=config/tokenization.yaml
```
# Launch distributed pretraining`
```
make train CONFIG=config/pretraining.yaml
```

## âš™ï¸ Configuration System

**Structured YAML Schemas**:
```
# src/config/base.py
class TaskConfig(BaseModel):
    task_name: str = Field(..., description="Name of task to execute")
    output_dir: Path = Field(..., description="Base output directory")
    
class TokenizationConfig(TaskConfig):
    dataset_path: str
    tokenizer_name: str
    chunk_size: int = 2048
    validation_ratio: float = 0.1
```

Validation flow:
```
CLI Command â†’ Load YAML â†’ Validate Schema â†’ Build Config Object â†’ Execute Task
```

## ğŸ“‚ Project Structure

```
project/
â”œâ”€â”€ config/                 # YAML configuration templates
â”‚   â”œâ”€â”€ curricula/         # Data mixing schedules
â”‚   â”œâ”€â”€ tokenization.yaml  # Tokenizer params
â”‚   â””â”€â”€ pretraining.yaml   # Model/training params
â”œâ”€â”€ docker/                # Container definitions
â”‚   â””â”€â”€ Dockerfile         # CUDA+PyTorch base image
â”œâ”€â”€ Makefile               # Project orchestration
â”œâ”€â”€ requirements/          # Pinned dependencies
â””â”€â”€ src/
    â”œâ”€â”€ config/            # Pydantic schema definitions
    â”œâ”€â”€ tasks/             # Task implementations
    â””â”€â”€ utils/             # Monitoring/checkpointing
```

## ğŸ‹ Docker & Makefile

**Dockerfile Highlights**:
```
FROM nvcr.io/nvidia/pytorch:23.10-py3
COPY requirements.txt .
RUN pip install -r requirements.txt
ENTRYPOINT ["make"]
```

**Makefile Targets**:
```
validate:  # Config schema check
    python -m src.main --validate $(CONFIG)

tokenize:  # Process datasets
    python -m src.main --task tokenize $(CONFIG)

train:     # Launch training
    torchrun --nproc_per_node=$(GPUS) src/main.py --task train $(CONFIG)
```

## ğŸ“ Reproducibility Features

1. **Configuration Hashing** - MD5 checksum of all config files
2. **Environment Snapshot** - `pip freeze` in training logs
3. **Deterministic Seeds** - Full random state preservation

## ğŸ¤ Contributing

1. Implement new config schemas in `src/config/`
2. Add corresponding task modules in `src/tasks/`
3. Include validation tests:

```
def test_tokenization_config():
    with pytest.raises(ValidationError):
        TokenizationConfig(tokenizer_name="invalid/model")
```
