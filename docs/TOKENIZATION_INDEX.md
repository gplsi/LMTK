# Tokenization Documentation Index

## Overview

This directory contains comprehensive documentation for the tokenization system in the continual pretraining framework. The tokenization system has been extensively optimized for performance, parallelism, and ease of use.

## Documentation Files

### ðŸ“Š [TOKENIZATION_PERFORMANCE.md](./TOKENIZATION_PERFORMANCE.md)
**Comprehensive performance optimization guide**
- Intelligent parallelism for fast vs slow tokenizers
- Vectorized label generation optimizations
- Memory management strategies
- Batch processing configurations
- Performance benchmarks and troubleshooting

### ðŸš€ [FAST_VS_SLOW_TOKENIZERS.md](./FAST_VS_SLOW_TOKENIZERS.md)
**Understanding tokenizer types and parallelism strategies**
- Fast tokenizers (Rust-based) vs slow tokenizers (Python-based)
- Automatic parallelism detection and configuration
- Migration guide from manual to automatic configuration
- Performance comparison and recommendations

## Key Features Implemented

### âœ… **Intelligent Parallelism**
- Auto-detects fast vs slow tokenizers
- Fast tokenizers: Uses internal Rust parallelism
- Slow tokenizers: Uses Python multiprocessing with intelligent defaults

### âœ… **Performance Optimizations**
- Vectorized label generation (3-5x faster)
- Optimized batch sizes (6000 default)
- Efficient memory usage with NumPy arrays
- Environment variable optimization

### âœ… **Configuration Flexibility**
- Automatic intelligent defaults
- Manual override capabilities
- Debug and single-process modes
- Comprehensive logging and monitoring

### âœ… **Memory Management**
- Fixed-length sequences for better allocation
- Efficient data types (int64/int8)
- Configurable batch sizes
- Memory usage guidelines

## Quick Start

### For Fast Tokenizers (Recommended)
```yaml
tokenizer:
  tokenizer_name: "meta-llama/Llama-2-7b-hf"
  context_length: 4096
  # Let the system auto-detect optimal settings
```

### For Custom Configuration
```yaml
tokenizer:
  tokenizer_name: "your-tokenizer"
  context_length: 2048
  batch_size: 4000      # Custom batch size
  num_proc: 8           # For slow tokenizers only
  show_progress: true
```

## Performance Improvements Summary

| Optimization | Performance Gain | Description |
|-------------|------------------|-------------|
| Fast Tokenizer Detection | 5-10x | Automatic Rust parallelism |
| Vectorized Labels | 3-5x | NumPy instead of nested loops |
| Optimized Batch Size | 2-3x | Increased from 1000 to 6000 |
| Intelligent Multiprocessing | 2-8x | Auto CPU core detection |
| Memory Optimization | 20-30% | Efficient data types |

## Migration Guide

### From Previous Versions
1. Remove manual `num_proc` settings (auto-detected now)
2. Increase batch sizes or remove for auto-defaults
3. Check logs for tokenizer type warnings
4. Update configuration schema references

### Before vs After
```yaml
# OLD Configuration
tokenizer:
  batch_size: 1000
  num_proc: 4
  
# NEW Configuration (Recommended)
tokenizer:
  # batch_size: auto-detected
  # num_proc: auto-detected based on tokenizer type
```

## Troubleshooting Quick Reference

| Issue | Solution |
|-------|----------|
| Slow performance | Check tokenizer type, increase batch_size |
| Memory errors | Reduce batch_size, reduce num_proc |
| CPU underutilization | Verify TOKENIZERS_PARALLELISM=true |
| Debug needed | Set verbose_level: DEBUG |

## Architecture Overview

```
CausalLMTokenizer
â”œâ”€â”€ _get_optimal_num_proc()     # Intelligent parallelism detection
â”œâ”€â”€ tokenize()                  # Main tokenization with performance monitoring
â”œâ”€â”€ _tokenize_function()        # Vectorized batch processing
â””â”€â”€ Performance Optimizations:
    â”œâ”€â”€ Environment variables (TOKENIZERS_PARALLELISM, RAYON_RS_NUM_THREADS)
    â”œâ”€â”€ Batch size optimization (6000 default)
    â”œâ”€â”€ Writer batch size (40000 for DatasetDict, 10000 for Dataset)
    â””â”€â”€ Memory optimizations (NumPy arrays, efficient data types)
```

## Configuration Schema

```yaml
tokenizer:
  context_length: int                    # Required
  overlap: int                          # Optional
  tokenizer_name: str                   # Optional
  verbose_level: str                    # DEBUG|INFO|WARNING|ERROR
  batch_size: int                       # 1-10000, default: auto-detected
  num_proc: int                         # 1-32, default: auto-detected
  show_progress: bool                   # default: true
```

## Best Practices

1. **Use fast tokenizers** when available
2. **Let auto-detection work** (remove manual num_proc)
3. **Monitor performance logs** for optimization opportunities
4. **Start with defaults** and adjust based on your system
5. **Profile with your data** to find optimal settings

## Future Roadmap

- [ ] Automatic batch size tuning based on available memory
- [ ] GPU-accelerated tokenization for supported models
- [ ] Streaming tokenization for extremely large datasets
- [ ] Advanced memory management for low-resource environments
- [ ] Real-time performance monitoring dashboard

---

For detailed information on any specific aspect, please refer to the individual documentation files linked above.
